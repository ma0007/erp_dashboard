# -*- coding: utf-8 -*-
# v2.5.8: Final fixes including pricing ceiling rounding, chart rewrite/cleanup, and welcome screen redesign.
# v2.5.9: Implemented session state to prevent rerun loops on file load errors.
# v2.5.9-mobile-logo-center-v2: Centered logo in sidebar using base64 data URI and HTML/CSS.

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.font_manager import FontProperties # ä¸­æ–‡å­—ä½“
import io
import os
import numpy as np
from datetime import datetime, timedelta
import pytz # Using pytz for timezone handling
import streamlit.components.v1 as components
import traceback # For detailed error logging if needed
import math # Added for ceiling rounding
import base64 # Added for image encoding

# --- Constants ---
TOP_N_DISPLAY = 150 # Max rows to display in tables (performance)
APP_TIMEZONE_STR = 'Europe/Athens' # Define standard timezone for calculations

# Use current date for versioning
try:
    # Attempt to get current time in the specified timezone for versioning
    APP_VERSION = f"v2.5.9-{datetime.now(pytz.timezone(APP_TIMEZONE_STR)).strftime('%Y%m%d')}"
except Exception:
    # Fallback if timezone setup fails early
    APP_VERSION = "v2.5.9-unknown_date"


# --- Timezone Setup ---
try:
    APP_TIMEZONE = pytz.timezone(APP_TIMEZONE_STR)
except pytz.exceptions.UnknownTimeZoneError:
    st.warning(f"æ— æ³•è¯†åˆ«çš„æ—¶åŒº '{APP_TIMEZONE_STR}'ï¼Œå°†å›é€€åˆ° UTCã€‚")
    APP_TIMEZONE = pytz.utc # Fallback
    APP_TIMEZONE_STR = 'UTC'

# --- Update version string again now that timezone is confirmed ---
APP_VERSION = f"v2.5.9-{datetime.now(APP_TIMEZONE).strftime('%Y%m%d')}"

# --- Matplotlib ä¸­æ–‡æ˜¾ç¤ºè®¾ç½® ---
# Try to find the font in common locations or the script directory
font_paths_to_check = [
    'SimHei.ttf', # Check local directory first
    '/usr/share/fonts/truetype/simhei/SimHei.ttf', # Linux common path
    '/Library/Fonts/SimHei.ttf', # macOS common path
    'C:/Windows/Fonts/simhei.ttf' # Windows common path
]
font_path = None
for p in font_paths_to_check:
    if os.path.exists(p):
        font_path = p
        break

FONT_AVAILABLE = False
chinese_font = None
if font_path:
    try:
        chinese_font = FontProperties(fname=font_path, size=10)
        FONT_AVAILABLE = True
        print(f"æˆåŠŸåŠ è½½å­—ä½“: {font_path}") # Log success
    except Exception as font_err:
        print(f"åŠ è½½å­—ä½“æ–‡ä»¶ '{font_path}' æ—¶å‡ºé”™: {font_err}") # Log error to console
else:
    print(f"è­¦å‘Šï¼šåœ¨å¸¸è§ä½ç½®åŠè„šæœ¬ç›®å½•æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“æ–‡ä»¶ (å¦‚ SimHei.ttf)ã€‚") # Log warning

# Ensure negative signs display correctly even with CJK fonts
plt.rcParams['axes.unicode_minus'] = False

# --- Initialize Session State (For Preventing Rerun Loops) ---
if 'main_load_error' not in st.session_state:
    st.session_state.main_load_error = None
if 'pricing_load_error' not in st.session_state:
    st.session_state.pricing_load_error = None
if 'last_main_file_id' not in st.session_state:
    st.session_state.last_main_file_id = None
if 'last_pricing_file_id' not in st.session_state:
    st.session_state.last_pricing_file_id = None


# ======== Helper Functions ========

# --- Function to encode image to base64 ---
def get_image_as_base64(path):
    """Reads an image file and returns its base64 encoded data URI."""
    try:
        with open(path, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode()
        # Basic image type detection from extension
        image_type = os.path.splitext(path)[1].lower().replace('.', '')
        # Add more types if needed, provide fallback
        supported_types = ['png', 'jpg', 'jpeg', 'gif', 'svg', 'webp']
        if image_type not in supported_types:
            image_type = 'png' # Default fallback if extension is unknown/unsupported
        return f"data:image/{image_type};base64,{encoded_string}"
    except FileNotFoundError:
        print(f"Error: Logo file not found at {path}")
        return None
    except Exception as e:
        print(f"Error encoding image {path}: {e}")
        return None

# --- load_data å‡½æ•° (For Main Analysis) ---
@st.cache_data(ttl=timedelta(minutes=10))
def load_data(uploaded_file_content, uploaded_file_name):
    """
    Loads data from uploaded file content (Excel or CSV) for main analysis.
    Returns (sales_df, stock_df, purchase_df, error_message)
    On success, error_message is None.
    On failure, dataframes are None and error_message contains the error string.
    """
    try:
        file_ext = os.path.splitext(uploaded_file_name)[1].lower()
        file_buffer = io.BytesIO(uploaded_file_content)
        sales_df, stock_df, purchase_df = None, None, None

        if file_ext == '.csv':
            df = pd.read_csv(file_buffer)
            if 'DataType' not in df.columns: raise ValueError("CSVæ–‡ä»¶å¿…é¡»åŒ…å« 'DataType' åˆ—ä»¥åŒºåˆ† 'è®¢å•æ•°æ®', 'åº“å­˜æ•°æ®', 'é‡‡è´­æ•°æ®'.")
            sales_df = df[df['DataType'] == 'è®¢å•æ•°æ®'].copy()
            stock_df = df[df['DataType'] == 'åº“å­˜æ•°æ®'].copy()
            purchase_df = df[df['DataType'] == 'é‡‡è´­æ•°æ®'].copy()
            if sales_df.empty: raise ValueError("åœ¨CSVä¸­æœªæ‰¾åˆ° DataType ä¸º 'è®¢å•æ•°æ®' çš„æ•°æ®.")
            # Handle potentially empty but present stock/purchase data in CSV
            if stock_df.empty and 'åº“å­˜æ•°æ®' in df['DataType'].unique():
                st.warning("è­¦å‘Šï¼šCSVä¸­DataTypeä¸º'åº“å­˜æ•°æ®'çš„éƒ¨åˆ†ä¸ºç©ºã€‚å°†ä½¿ç”¨ç©ºç»“æ„ã€‚")
                stock_df = pd.DataFrame(columns=["äº§å“ID", "å½“å‰åº“å­˜", "äº§å“åç§°", "é‡‡è´­ä»·", "äº§å“åˆ†ç±»"])
            elif not stock_df.empty and 'åº“å­˜æ•°æ®' not in df['DataType'].unique():
                 # Should not happen if logic above is right, but as safety
                 st.warning("è­¦å‘Šï¼šå‘ç°åº“å­˜æ•°æ®ï¼Œä½†æœªæ ‡è®°ä¸º 'åº“å­˜æ•°æ®' DataTypeã€‚")

            if purchase_df.empty and 'é‡‡è´­æ•°æ®' in df['DataType'].unique():
                st.caption("æ³¨æ„ï¼šCSVä¸­DataTypeä¸º'é‡‡è´­æ•°æ®'çš„éƒ¨åˆ†ä¸ºç©ºã€‚å°†ä½¿ç”¨ç©ºç»“æ„ã€‚")
                purchase_df = pd.DataFrame(columns=["é‡‡è´­æ—¥æœŸ", "äº§å“ID", "é‡‡è´­æ•°é‡", "äº§å“åˆ†ç±»"])
            elif not purchase_df.empty and 'é‡‡è´­æ•°æ®' not in df['DataType'].unique():
                 st.warning("è­¦å‘Šï¼šå‘ç°é‡‡è´­æ•°æ®ï¼Œä½†æœªæ ‡è®°ä¸º 'é‡‡è´­æ•°æ®' DataTypeã€‚")

        elif file_ext in ['.xlsx', '.xls']:
            try:
                xls = pd.ExcelFile(file_buffer)
                required_sheets = ["è®¢å•æ•°æ®", "åº“å­˜æ•°æ®"]
                if not all(sheet in xls.sheet_names for sheet in required_sheets):
                    raise ValueError(f"Excelæ–‡ä»¶å¿…é¡»è‡³å°‘åŒ…å«ä»¥ä¸‹å·¥ä½œè¡¨: {', '.join(required_sheets)}")

                sales_df = pd.read_excel(xls, sheet_name="è®¢å•æ•°æ®")
                stock_df = pd.read_excel(xls, sheet_name="åº“å­˜æ•°æ®")

                if "é‡‡è´­æ•°æ®" in xls.sheet_names:
                    purchase_df = pd.read_excel(xls, sheet_name="é‡‡è´­æ•°æ®")
                    if purchase_df.empty: st.caption("æ³¨æ„ï¼š'é‡‡è´­æ•°æ®' å·¥ä½œè¡¨ä¸ºç©ºã€‚å°†ä½¿ç”¨ç©ºç»“æ„ã€‚")
                else:
                    st.warning("è­¦å‘Šï¼šExcelæ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'é‡‡è´­æ•°æ®' å·¥ä½œè¡¨ã€‚é‡‡è´­ç›¸å…³åˆ†æå°†å—é™ã€‚å°†ä½¿ç”¨ç©ºç»“æ„ã€‚")
                    purchase_df = pd.DataFrame(columns=["é‡‡è´­æ—¥æœŸ", "äº§å“ID", "é‡‡è´­æ•°é‡", "äº§å“åˆ†ç±»"]) # Ensure it exists even if sheet missing

            except Exception as e:
                raise ValueError(f"è¯»å–Excelæ–‡ä»¶ç»“æ„æˆ–å†…å®¹æ—¶å‡ºé”™: {e}")
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚è¯·ä¸Šä¼  .csv, .xlsx, æˆ– .xls æ–‡ä»¶ã€‚")

        # Ensure dataframes are not None before proceeding
        if sales_df is None: sales_df = pd.DataFrame() # Should have been caught earlier
        if stock_df is None: stock_df = pd.DataFrame(columns=["äº§å“ID", "å½“å‰åº“å­˜", "äº§å“åç§°", "é‡‡è´­ä»·", "äº§å“åˆ†ç±»"])
        if purchase_df is None: purchase_df = pd.DataFrame(columns=["é‡‡è´­æ—¥æœŸ", "äº§å“ID", "é‡‡è´­æ•°é‡", "äº§å“åˆ†ç±»"])


        # --- Date Conversions ---
        date_cols_map = {'sales': (sales_df, 'è®¢å•æ—¥æœŸ'), 'purchase': (purchase_df, 'é‡‡è´­æ—¥æœŸ')}
        for key, (df_loop, date_col) in date_cols_map.items():
             if isinstance(df_loop, pd.DataFrame) and not df_loop.empty and date_col in df_loop.columns:
                 original_count = len(df_loop)
                 # Coerce errors first, then handle NaT if needed, then convert valid to datetime
                 df_loop[date_col] = pd.to_datetime(df_loop[date_col], errors='coerce')
                 failed_count = df_loop[date_col].isna().sum() # Count NaNs/NaTs directly
                 if failed_count > 0:
                     st.warning(f"è­¦å‘Šï¼šåœ¨ '{key}' æ•°æ®çš„ '{date_col}' åˆ—ä¸­å‘ç° {failed_count} ä¸ªæ— æ•ˆæ—¥æœŸæ ¼å¼ï¼Œè¿™äº›å€¼å·²è¢«ç½®ç©ºã€‚åç»­åˆ†æå¯èƒ½å¿½ç•¥è¿™äº›è¡Œã€‚")
                     df_loop.dropna(subset=[date_col], inplace=True) # Drop rows where date conversion failed

        # --- Column Validation ---
        required_sales_cols = ["è®¢å•æ—¥æœŸ", "äº§å“ID", "è´­ä¹°æ•°é‡", "äº§å“åç§°"]
        required_stock_cols = ["äº§å“ID", "å½“å‰åº“å­˜", "äº§å“åç§°"] # Keep 'é‡‡è´­ä»·' optional here, validate numeric later
        required_purchase_cols = ["é‡‡è´­æ—¥æœŸ", "äº§å“ID", "é‡‡è´­æ•°é‡"] # Optional 'äº§å“åˆ†ç±»'

        if not all(col in sales_df.columns for col in required_sales_cols):
            raise ValueError(f"'è®¢å•æ•°æ®' ç¼ºå°‘å¿…éœ€åˆ—: {', '.join([c for c in required_sales_cols if c not in sales_df.columns])}")
        if not stock_df.empty and not all(col in stock_df.columns for col in required_stock_cols):
             raise ValueError(f"'åº“å­˜æ•°æ®' ç¼ºå°‘å¿…éœ€åˆ—: {', '.join([c for c in required_stock_cols if c not in stock_df.columns])}")
        # Only validate purchase columns if the purchase_df is not empty
        if purchase_df is not None and not purchase_df.empty and not all(col in purchase_df.columns for col in required_purchase_cols):
            raise ValueError(f"'é‡‡è´­æ•°æ®' å·¥ä½œè¡¨å­˜åœ¨ä½†ç¼ºå°‘å¿…éœ€åˆ—: {', '.join([c for c in required_purchase_cols if c not in purchase_df.columns])}")

        # --- Numeric Conversion ---
        num_cols_map = {
            'sales': (sales_df, ["è´­ä¹°æ•°é‡"]),
            'stock': (stock_df, ["å½“å‰åº“å­˜", "é‡‡è´­ä»·"]), #é‡‡è´­ä»· optional but convert if present
            'purchase': (purchase_df, ["é‡‡è´­æ•°é‡"])
        }
        for key, (df_loop, cols) in num_cols_map.items():
             if isinstance(df_loop, pd.DataFrame) and not df_loop.empty:
                 for col in cols:
                     if col in df_loop.columns: # Check if column exists before conversion
                         # Store original dtype to check if conversion actually happens
                         # original_dtype = df_loop[col].dtype
                         initial_na_count = df_loop[col].isna().sum()
                         # Force conversion to numeric, coercing errors to NaN
                         df_loop[col] = pd.to_numeric(df_loop[col], errors='coerce')
                         final_na_count = df_loop[col].isna().sum()
                         newly_failed_count = final_na_count - initial_na_count
                         if newly_failed_count > 0:
                              st.warning(f"è­¦å‘Šï¼šåœ¨ '{key}' æ•°æ®çš„ '{col}' åˆ—ä¸­å‘ç° {newly_failed_count} ä¸ªæ— æ³•è§£æçš„éæ•°å€¼ï¼Œå·²æ›¿æ¢ä¸ºç©ºå€¼ (NaN)ã€‚")
                         # Fill NaN resulting from coercion or original NaNs with 0
                         df_loop[col] = df_loop[col].fillna(0)
                         # Attempt integer conversion only if feasible
                         try:
                              # Check if all non-zero values are whole numbers after filling NaNs
                              non_zero_mask = df_loop[col] != 0
                              if not non_zero_mask.any() or (df_loop.loc[non_zero_mask, col] % 1 == 0).all():
                                   # Convert to largest integer type to avoid overflow
                                   df_loop[col] = df_loop[col].astype(np.int64)
                             # Otherwise, keep as float if there are decimals
                         except Exception:
                              # If int conversion fails for any reason, keep as float
                              pass
                     elif col in ["é‡‡è´­ä»·"] and key == 'stock':
                         # If optional 'é‡‡è´­ä»·' is missing in stock, add it as 0 float
                         df_loop[col] = 0.0
                         st.caption(f"æ³¨æ„ï¼šåº“å­˜æ•°æ®ä¸­æœªæ‰¾åˆ° '{col}' åˆ—ï¼Œå°†å‡è®¾å…¶å€¼ä¸º 0ã€‚")


        # Success: Return dataframes and None for error message
        return sales_df, stock_df, purchase_df, None

    except ValueError as ve:
        # Return None for dataframes and the error message
        return None, None, None, f"âŒ åˆ†ææ•°æ®æ ¼å¼æˆ–å†…å®¹é”™è¯¯: {ve}"
    except Exception as e:
        # Return None for dataframes and the error message
        # Log traceback for debugging if needed (prints to console where streamlit runs)
        # traceback.print_exc()
        return None, None, None, f"âŒ åˆ†ææ•°æ®æ–‡ä»¶è¯»å–æˆ–å¤„ç†å¤±è´¥: {e.__class__.__name__}: {e}"

# --- load_pricing_data å‡½æ•° ---
@st.cache_data(ttl=timedelta(minutes=10))
def load_pricing_data(uploaded_file_content, uploaded_file_name):
    """
    Loads and validates data for the pricing tool.
    Returns (pricing_df, error_message)
    On success, error_message is None.
    On failure, dataframe is None and error_message contains the error string.
    """
    try:
        file_ext = os.path.splitext(uploaded_file_name)[1].lower()
        file_buffer = io.BytesIO(uploaded_file_content)
        df = None

        if file_ext == '.csv': df = pd.read_csv(file_buffer, header=0)
        elif file_ext in ['.xlsx', '.xls']: df = pd.read_excel(file_buffer, sheet_name=0, header=0)
        else: raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚è¯·ä¸Šä¼  .csv, .xlsx, æˆ– .xls æ–‡ä»¶ã€‚")

        if df is None or df.empty: raise ValueError("ä¸Šä¼ çš„å®šä»·æ•°æ®æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–ã€‚")

        # Flexible Column Name Finding
        id_col_found = None; name_col_found = None; price_col_found = None
        if 'äº§å“ID' in df.columns: id_col_found = 'äº§å“ID'
        elif 'å‹å·' in df.columns: id_col_found = 'å‹å·'

        if 'äº§å“åç§°' in df.columns: name_col_found = 'äº§å“åç§°'
        elif 'å“å' in df.columns: name_col_found = 'å“å'

        # Handle potential duplicate 'é‡‡è´­ä»·' (common in exports)
        if 'é‡‡è´­ä»·.1' in df.columns:
            price_col_found = 'é‡‡è´­ä»·.1';
            st.caption("æ£€æµ‹åˆ°é‡å¤çš„'é‡‡è´­ä»·'åˆ—ï¼Œå·²ä¼˜å…ˆä½¿ç”¨ç¬¬äºŒåˆ— ('é‡‡è´­ä»·.1')ã€‚")
        elif 'é‡‡è´­ä»·' in df.columns:
            price_col_found = 'é‡‡è´­ä»·'

        # Validate essential columns were found
        missing_essential_list = []
        if not id_col_found: missing_essential_list.append('äº§å“ID/å‹å·')
        if not name_col_found: missing_essential_list.append('äº§å“åç§°/å“å')
        if not price_col_found: missing_essential_list.append('é‡‡è´­ä»·')
        if missing_essential_list:
            raise ValueError(f"å®šä»·æ•°æ®æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {', '.join(missing_essential_list)}")

        # Select and Rename
        df_renamed = df[[id_col_found, name_col_found, price_col_found]].copy()
        df_renamed.columns = ['äº§å“ID', 'äº§å“åç§°', 'é‡‡è´­ä»·']

        # Robust Numeric Conversion for 'é‡‡è´­ä»·' (Handle comma decimals)
        col = "é‡‡è´­ä»·"
        initial_na_count = df_renamed[col].isna().sum()
        # Convert to string, replace comma with dot, strip whitespace
        col_as_str = df_renamed[col].astype(str)
        col_as_str_cleaned = col_as_str.str.replace(',', '.', regex=False).str.strip()
        # Convert to numeric, coerce errors
        df_renamed[col] = pd.to_numeric(col_as_str_cleaned, errors='coerce')
        final_na_count = df_renamed[col].isna().sum()
        newly_failed_count = final_na_count - initial_na_count
        if newly_failed_count > 0:
            st.warning(f"è­¦å‘Šï¼šåœ¨å®šä»·æ•°æ®çš„ '{col}' åˆ—(æ¥è‡ªæºæ–‡ä»¶åˆ— '{price_col_found}')ä¸­å‘ç° {newly_failed_count} ä¸ªæ— æ³•è§£æçš„éæ•°å€¼ï¼Œå·²æ›¿æ¢ä¸ºç©ºå€¼ (NaN)ã€‚")

        # Final cleaning
        pricing_df = df_renamed[['äº§å“ID', 'äº§å“åç§°', 'é‡‡è´­ä»·']].copy()
        pricing_df.dropna(subset=['é‡‡è´­ä»·'], inplace=True) # Drop rows where price is still NaN
        pricing_df.drop_duplicates(subset=['äº§å“ID'], keep='first', inplace=True) # Keep first valid entry per ID

        if pricing_df.empty: raise ValueError("å¤„ç†åæœªæ‰¾åˆ°åŒ…å«æœ‰æ•ˆ'é‡‡è´­ä»·'çš„å®šä»·æ•°æ®è¡Œã€‚")

        st.success(f"æˆåŠŸåŠ è½½å¹¶å¤„ç†äº† {len(pricing_df)} æ¡æœ‰æ•ˆå®šä»·æ•°æ®ã€‚")
        # Success: Return dataframe and None for error message
        return pricing_df, None

    except ValueError as ve:
        # Return None and error message
        return None, f"âŒ å®šä»·æ•°æ®æ ¼å¼æˆ–å†…å®¹é”™è¯¯: {ve}"
    except Exception as e:
        # Return None and error message
        # traceback.print_exc()
        return None, f"âŒ å®šä»·æ•°æ®æ–‡ä»¶è¯»å–æˆ–å¤„ç†å¤±è´¥: {e.__class__.__name__}: {e}"


# --- calculate_metrics å‡½æ•° ---
def calculate_metrics(sales_df, stock_df, purchase_df, start_date, end_date):
    """Calculates key metrics using standardized timezone for 'now'. Ensures robustness."""
    try:
        # Ensure dates are Timestamps for comparison
        start_ts = pd.Timestamp(start_date)
        end_ts = pd.Timestamp(end_date)
    except Exception as date_err:
        st.error(f"æ— æ³•è§£ææ—¥æœŸèŒƒå›´: {date_err}")
        return {}, pd.DataFrame(), False # Return empty results

    metrics_results = {}
    stock_analysis = pd.DataFrame()
    has_category_in_analysis = False

    try:
        # --- Sales Metrics ---
        if not isinstance(sales_df, pd.DataFrame) or 'è®¢å•æ—¥æœŸ' not in sales_df.columns or not pd.api.types.is_datetime64_any_dtype(sales_df['è®¢å•æ—¥æœŸ']):
            st.error("è®¡ç®—æŒ‡æ ‡é”™è¯¯ï¼š'è®¢å•æ•°æ®'æ— æ•ˆæˆ–ç¼ºå°‘æ­£ç¡®çš„'è®¢å•æ—¥æœŸ'åˆ—ã€‚")
            return metrics_results, stock_analysis, has_category_in_analysis # Return empty

        # Filter sales data for the period
        # Ensure end_ts includes the whole day if it doesn't have time component
        end_ts_inclusive = end_ts + pd.Timedelta(days=1) - pd.Timedelta(nanoseconds=1) if end_ts.time() == datetime.min.time() else end_ts
        sales_filtered = sales_df[(sales_df["è®¢å•æ—¥æœŸ"] >= start_ts) & (sales_df["è®¢å•æ—¥æœŸ"] <= end_ts_inclusive)].copy()

        # Calculate total sales quantity
        metrics_results["total_sales_period"] = 0
        if 'è´­ä¹°æ•°é‡' in sales_filtered.columns:
            # Ensure the column is numeric before summing
            sales_filtered['è´­ä¹°æ•°é‡_num_calc'] = pd.to_numeric(sales_filtered['è´­ä¹°æ•°é‡'], errors='coerce').fillna(0)
            metrics_results["total_sales_period"] = int(sales_filtered["è´­ä¹°æ•°é‡_num_calc"].sum())
        else:
             st.warning("é”€å”®æ•°æ®ç¼ºå°‘ 'è´­ä¹°æ•°é‡' åˆ—ï¼Œæ— æ³•è®¡ç®—æ€»é”€é‡ã€‚")


        # Calculate average daily sales
        num_days_period = max(1, (end_ts - start_ts).days + 1) # Add 1 to include both start and end date
        metrics_results["avg_daily_sales_period"] = round((metrics_results["total_sales_period"] / num_days_period), 1)

        # Calculate active selling days
        metrics_results["active_days_period"] = sales_filtered["è®¢å•æ—¥æœŸ"].nunique() if not sales_filtered.empty else 0

        # Find top selling product
        top_product_period = "æ— "
        top_selling_data = pd.Series(dtype=float)
        if 'äº§å“åç§°' in sales_filtered.columns and 'è´­ä¹°æ•°é‡_num_calc' in sales_filtered.columns and not sales_filtered.empty:
            try:
                # Group by product name and sum the numeric quantity
                top_selling_data = sales_filtered.groupby("äº§å“åç§°")["è´­ä¹°æ•°é‡_num_calc"].sum().sort_values(ascending=False)
                if not top_selling_data.empty:
                    top_product_period = str(top_selling_data.index[0]) # Get name of top product
            except Exception as e:
                st.warning(f"è®¡ç®—çƒ­é”€äº§å“æ—¶å‡ºé”™: {e}")
        metrics_results["top_product_period"] = top_product_period
        metrics_results["top_selling_period_chart_data"] = top_selling_data.head(10) # Data for pie chart

        # Calculate monthly trend data
        monthly_trend_data = pd.Series(dtype=float)
        if not sales_filtered.empty and 'è®¢å•æ—¥æœŸ' in sales_filtered.columns and 'è´­ä¹°æ•°é‡_num_calc' in sales_filtered.columns:
             try:
                 # Group by month and sum quantity
                 monthly_trend_data = sales_filtered.groupby(sales_filtered["è®¢å•æ—¥æœŸ"].dt.to_period("M"))['è´­ä¹°æ•°é‡_num_calc'].sum()
             except Exception as e:
                 st.warning(f"è®¡ç®—æœˆåº¦è¶‹åŠ¿æ—¶å‡ºé”™: {e}")
        metrics_results["monthly_trend_chart_data"] = monthly_trend_data

        # --- Stock Analysis ---
        if not isinstance(stock_df, pd.DataFrame) or stock_df.empty or "äº§å“ID" not in stock_df.columns or "å½“å‰åº“å­˜" not in stock_df.columns:
             st.warning("åº“å­˜æ•°æ®æ— æ•ˆæˆ–ç¼ºå°‘å¿…éœ€åˆ— ('äº§å“ID', 'å½“å‰åº“å­˜')ï¼Œæ— æ³•è¿›è¡Œè¯¦ç»†åº“å­˜åˆ†æã€‚")
             # Define empty dataframe with expected columns for consistency downstream
             stock_analysis = pd.DataFrame(columns=["äº§å“ID", "äº§å“åç§°", "å½“å‰åº“å­˜", "æœ€åé”€å”®æ—¥æœŸ", "å‹è´§æ—¶é—´_å¤©", "æœ€åé‡‡è´­æ—¥æœŸ", "æœ€åé‡‡è´­æ•°é‡", "å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­", "æœŸé—´é”€å”®é‡", "æœŸé—´æ—¥å‡é”€é‡", "é¢„è®¡å¯ç”¨å¤©æ•°", "äº§å“åˆ†ç±»"])
             metrics_results["total_stock_units"] = 0
             return metrics_results, stock_analysis, False # Return empty results but defined structure

        # Calculate total stock units
        stock_df['å½“å‰åº“å­˜_num_calc'] = pd.to_numeric(stock_df['å½“å‰åº“å­˜'], errors='coerce').fillna(0)
        metrics_results["total_stock_units"] = int(stock_df['å½“å‰åº“å­˜_num_calc'].sum())

        # Check if category data exists and is usable
        has_category = "äº§å“åˆ†ç±»" in stock_df.columns and not stock_df["äº§å“åˆ†ç±»"].isnull().all()

        # Base stock analysis dataframe from unique product IDs in stock data
        stock_analysis_cols = ["äº§å“ID", "äº§å“åç§°", "å½“å‰åº“å­˜"] # Start with essential cols
        if has_category: stock_analysis_cols.append("äº§å“åˆ†ç±»") # Add category if available
        # Ensure only columns that actually exist in stock_df are selected
        stock_analysis_cols_present = [col for col in stock_analysis_cols if col in stock_df.columns]
        # Use 'é‡‡è´­ä»·' if available for potential future use, but don't require it for analysis core
        if 'é‡‡è´­ä»·' in stock_df.columns:
            stock_analysis_cols_present.append('é‡‡è´­ä»·')

        # Drop duplicates based on Product ID, keeping the first occurrence
        stock_analysis_base = stock_df[stock_analysis_cols_present].drop_duplicates(subset=["äº§å“ID"], keep='first').copy()


        # --- Merge Last Sale Date ---
        last_sale_overall = pd.Series(dtype='datetime64[ns]')
        if isinstance(sales_df, pd.DataFrame) and not sales_df.empty and "äº§å“ID" in sales_df.columns and "è®¢å•æ—¥æœŸ" in sales_df.columns:
            try:
                # Use only valid sales records (where date is not NaT)
                valid_sales = sales_df.dropna(subset=['è®¢å•æ—¥æœŸ', 'äº§å“ID'])
                if not valid_sales.empty:
                    # Find the index of the latest sale date for each product ID
                    last_sale_idx = valid_sales.groupby("äº§å“ID")["è®¢å•æ—¥æœŸ"].idxmax()
                    # Create a mapping Series: Product ID -> Last Sale Date
                    last_sale_overall = valid_sales.loc[last_sale_idx].set_index('äº§å“ID')['è®¢å•æ—¥æœŸ'].rename("æœ€åé”€å”®æ—¥æœŸ")
            except Exception as e:
                st.warning(f"èšåˆæœ€åé”€å”®æ—¥æœŸæ—¶å‡ºé”™: {e}")

        # Merge last sale date into the stock analysis table
        stock_analysis = stock_analysis_base.merge(last_sale_overall, on="äº§å“ID", how="left")


        # --- Calculate Stock Aging (Days Since Last Sale) ---
        now_ts_aware = pd.Timestamp.now(tz=APP_TIMEZONE)
        now_ts_naive = now_ts_aware.tz_localize(None) # Use naive timestamp for calculations with naive dates from files

        if 'æœ€åé”€å”®æ—¥æœŸ' in stock_analysis.columns:
             # Ensure the merged date is datetime, handle potential errors post-merge
             stock_analysis["æœ€åé”€å”®æ—¥æœŸ"] = pd.to_datetime(stock_analysis["æœ€åé”€å”®æ—¥æœŸ"], errors='coerce').dt.tz_localize(None) # Make naive
             # Calculate difference only where last sale date is valid
             valid_last_sale_mask = stock_analysis["æœ€åé”€å”®æ—¥æœŸ"].notna()
             stock_analysis.loc[valid_last_sale_mask, "å‹è´§æ—¶é—´_å¤©"] = (now_ts_naive - stock_analysis.loc[valid_last_sale_mask, "æœ€åé”€å”®æ—¥æœŸ"]).dt.days
        else:
            # If column doesn't exist after merge (shouldn't happen if logic above is right)
            stock_analysis["å‹è´§æ—¶é—´_å¤©"] = np.nan

        # Fill NaN (never sold or error) with 9999, convert to int, clip max value
        stock_analysis["å‹è´§æ—¶é—´_å¤©"] = stock_analysis["å‹è´§æ—¶é—´_å¤©"].fillna(9999).astype(int).clip(upper=9999)


        # --- Merge Last Purchase Info ---
        # Initialize columns to default values
        stock_analysis["æœ€åé‡‡è´­æ—¥æœŸ"] = pd.NaT
        stock_analysis["æœ€åé‡‡è´­æ•°é‡"] = 0
        stock_analysis["å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­"] = 9999

        if isinstance(purchase_df, pd.DataFrame) and not purchase_df.empty and all(col in purchase_df.columns for col in ["äº§å“ID", "é‡‡è´­æ—¥æœŸ", "é‡‡è´­æ•°é‡"]):
            # Ensure purchase date is datetime and drop invalid rows
            purchase_df['é‡‡è´­æ—¥æœŸ'] = pd.to_datetime(purchase_df['é‡‡è´­æ—¥æœŸ'], errors='coerce')
            purchase_df_valid = purchase_df.dropna(subset=['é‡‡è´­æ—¥æœŸ', 'äº§å“ID', 'é‡‡è´­æ•°é‡'])

            if not purchase_df_valid.empty:
                 try:
                      # Ensure purchase quantity is numeric for potential use later (though not directly used for mapping date)
                      purchase_df_valid['é‡‡è´­æ•°é‡'] = pd.to_numeric(purchase_df_valid['é‡‡è´­æ•°é‡'], errors='coerce').fillna(0)
                      # Find index of the latest purchase date per product ID
                      last_purchase_idx = purchase_df_valid.groupby('äº§å“ID')['é‡‡è´­æ—¥æœŸ'].idxmax()
                      # Create mapping dataframe: Product ID -> Last Purchase Date, Last Purchase Qty
                      last_purchase_map = purchase_df_valid.loc[last_purchase_idx].set_index('äº§å“ID')

                      # Map last purchase date and quantity to stock analysis table
                      stock_analysis['æœ€åé‡‡è´­æ—¥æœŸ'] = stock_analysis['äº§å“ID'].map(last_purchase_map['é‡‡è´­æ—¥æœŸ'])
                      stock_analysis['æœ€åé‡‡è´­æ•°é‡'] = stock_analysis['äº§å“ID'].map(last_purchase_map['é‡‡è´­æ•°é‡'])

                      # Calculate days since last purchase
                      if 'æœ€åé‡‡è´­æ—¥æœŸ' in stock_analysis.columns:
                          # Ensure date is datetime and naive
                          stock_analysis["æœ€åé‡‡è´­æ—¥æœŸ"] = pd.to_datetime(stock_analysis["æœ€åé‡‡è´­æ—¥æœŸ"], errors='coerce').dt.tz_localize(None)
                          valid_purchase_dates_mask = stock_analysis['æœ€åé‡‡è´­æ—¥æœŸ'].notna()
                          stock_analysis.loc[valid_purchase_dates_mask, "å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­"] = (now_ts_naive - stock_analysis.loc[valid_purchase_dates_mask, "æœ€åé‡‡è´­æ—¥æœŸ"]).dt.days

                      # Fill NaN (no purchase record or error) with 9999, convert to int, clip max
                      stock_analysis["å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­"] = stock_analysis["å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­"].fillna(9999).astype(int).clip(upper=9999)
                      # Fill NaN quantity with 0 and convert to int
                      stock_analysis["æœ€åé‡‡è´­æ•°é‡"] = stock_analysis["æœ€åé‡‡è´­æ•°é‡"].fillna(0).astype(int)

                 except Exception as merge_err:
                      st.warning(f"åˆå¹¶é‡‡è´­æ•°æ®æ—¶å‡ºé”™: {merge_err}")
            else:
                 st.caption("æ— æœ‰æ•ˆçš„é‡‡è´­è®°å½•è¡Œå¯ä¾›åˆå¹¶ã€‚")


        # --- Calculate Sales Within Period ---
        stock_analysis["æœŸé—´é”€å”®é‡"] = 0 # Initialize
        qty_col_sales_filtered = 'è´­ä¹°æ•°é‡_num_calc' # Use the numeric column created earlier
        if not sales_filtered.empty and "äº§å“ID" in sales_filtered.columns and qty_col_sales_filtered in sales_filtered.columns:
             try:
                 # Aggregate sales within the filtered period by product ID
                 sales_in_period_agg = sales_filtered.groupby("äº§å“ID")[qty_col_sales_filtered].sum()
                 # Map the aggregated sales to the stock analysis table
                 if "äº§å“ID" in stock_analysis.columns:
                     stock_analysis['æœŸé—´é”€å”®é‡'] = stock_analysis['äº§å“ID'].map(sales_in_period_agg)
                     # Fill products with no sales in the period with 0, convert to int
                     stock_analysis["æœŸé—´é”€å”®é‡"] = stock_analysis["æœŸé—´é”€å”®é‡"].fillna(0).astype(int)
                 else:
                     st.warning("åº“å­˜åˆ†æç¼ºå°‘'äº§å“ID'åˆ—ï¼Œæ— æ³•åˆå¹¶æœŸé—´é”€å”®é‡ã€‚")
             except Exception as e:
                 st.error(f"è®¡ç®—æˆ–åˆå¹¶æœŸé—´é”€å”®é‡é”™è¯¯: {e}")


        # --- Calculate Average Daily Sales (Period) ---
        if "æœŸé—´é”€å”®é‡" in stock_analysis.columns:
             stock_analysis["æœŸé—´æ—¥å‡é”€é‡"] = (stock_analysis["æœŸé—´é”€å”®é‡"] / num_days_period).round(2)
        else:
             stock_analysis["æœŸé—´æ—¥å‡é”€é‡"] = 0.0 # Default if calculation failed


        # --- Calculate Estimated Stock Days ---
        stock_analysis['é¢„è®¡å¯ç”¨å¤©æ•°'] = 9999 # Initialize with default (infinite/unknown)
        if "æœŸé—´æ—¥å‡é”€é‡" in stock_analysis.columns and "å½“å‰åº“å­˜" in stock_analysis.columns:
             # Use the numeric stock column created earlier
             stock_analysis['å½“å‰åº“å­˜_num_calc'] = pd.to_numeric(stock_analysis['å½“å‰åº“å­˜'], errors='coerce').fillna(0)
             # Calculate only where average daily sales is positive
             mask_positive_sales = stock_analysis['æœŸé—´æ—¥å‡é”€é‡'] > 0
             stock_analysis.loc[mask_positive_sales, 'é¢„è®¡å¯ç”¨å¤©æ•°'] = \
                 stock_analysis.loc[mask_positive_sales, 'å½“å‰åº“å­˜_num_calc'] / stock_analysis.loc[mask_positive_sales, 'æœŸé—´æ—¥å‡é”€é‡']

             # Fill NaN (e.g., from 0 sales) with 9999, round result, convert to int, clip max
             stock_analysis['é¢„è®¡å¯ç”¨å¤©æ•°'] = stock_analysis['é¢„è®¡å¯ç”¨å¤©æ•°'].fillna(9999).round().astype(int).clip(upper=9999)


        # --- Handle Product Category ---
        if has_category and "äº§å“åˆ†ç±»" not in stock_analysis.columns and "äº§å“åˆ†ç±»" in stock_df.columns:
             # If category exists in original stock but not merged (e.g., due to drop_duplicates issue), try re-mapping
             try:
                 # Create map from original stock data (ensure duplicates removed first)
                 category_map = stock_df[['äº§å“ID', 'äº§å“åˆ†ç±»']].drop_duplicates(subset=['äº§å“ID']).set_index('äº§å“ID')['äº§å“åˆ†ç±»']
                 stock_analysis['äº§å“åˆ†ç±»'] = stock_analysis['äº§å“ID'].map(category_map)
                 stock_analysis['äº§å“åˆ†ç±»'] = stock_analysis['äº§å“åˆ†ç±»'].fillna("æœªåˆ†ç±»") # Fill missing categories
             except KeyError:
                 st.warning("é‡æ–°åˆå¹¶äº§å“åˆ†ç±»æ—¶é‡åˆ°KeyErrorï¼Œè·³è¿‡ã€‚")
             except Exception as cat_err:
                 st.warning(f"é‡æ–°åˆå¹¶äº§å“åˆ†ç±»æ—¶å‡ºé”™: {cat_err}")
        elif not has_category and "äº§å“åˆ†ç±»" in stock_analysis.columns:
             # If category column ended up in analysis but wasn't expected, drop it
             stock_analysis = stock_analysis.drop(columns=["äº§å“åˆ†ç±»"])
        elif has_category and "äº§å“åˆ†ç±»" in stock_analysis.columns:
             # If category is present as expected, ensure NaNs are filled
             stock_analysis['äº§å“åˆ†ç±»'] = stock_analysis['äº§å“åˆ†ç±»'].fillna("æœªåˆ†ç±»")

        # Final check if category column ended up in the result dataframe
        has_category_in_analysis = "äº§å“åˆ†ç±»" in stock_analysis.columns if not stock_analysis.empty else False


        # --- Cleanup Temporary Columns ---
        temp_cols_to_drop = ['è´­ä¹°æ•°é‡_num_calc', 'å½“å‰åº“å­˜_num_calc']
        stock_analysis = stock_analysis.drop(columns=[col for col in temp_cols_to_drop if col in stock_analysis.columns], errors='ignore')

        # --- Return results ---
        return metrics_results, stock_analysis, has_category_in_analysis

    except Exception as e:
        st.error(f"åœ¨ calculate_metrics ä¸­å‘ç”Ÿæœªé¢„æ–™çš„é”™è¯¯: {e.__class__.__name__}: {e}")
        traceback.print_exc() # Log detailed error to console
        # Return empty but defined structures
        return {}, pd.DataFrame(columns=["äº§å“ID", "äº§å“åç§°", "å½“å‰åº“å­˜", "æœ€åé”€å”®æ—¥æœŸ", "å‹è´§æ—¶é—´_å¤©", "æœ€åé‡‡è´­æ—¥æœŸ", "æœ€åé‡‡è´­æ•°é‡", "å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­", "æœŸé—´é”€å”®é‡", "æœŸé—´æ—¥å‡é”€é‡", "é¢„è®¡å¯ç”¨å¤©æ•°", "äº§å“åˆ†ç±»"]), False


# --- calculate_purchase_suggestions å‡½æ•° ---
def calculate_purchase_suggestions(stock_analysis_df, target_days, safety_days):
    """Calculates purchase suggestions based on stock analysis."""
    if not isinstance(stock_analysis_df, pd.DataFrame) or stock_analysis_df.empty:
        return pd.DataFrame() # Return empty if no input data

    df = stock_analysis_df.copy()
    required_cols = ["æœŸé—´æ—¥å‡é”€é‡", "å½“å‰åº“å­˜", "äº§å“ID", "äº§å“åç§°"] # Essential for calculation

    # Check if required columns exist
    if not all(col in df.columns for col in required_cols):
        st.warning("åº“å­˜åˆ†ææ•°æ®ç¼ºå°‘è®¡ç®—é‡‡è´­å»ºè®®çš„å¿…è¦åˆ— ('æœŸé—´æ—¥å‡é”€é‡', 'å½“å‰åº“å­˜', 'äº§å“ID', 'äº§å“åç§°')ã€‚")
        return pd.DataFrame() # Return empty

    try:
        # --- Ensure Numeric Types ---
        # Convert relevant columns to numeric, coercing errors and filling NaNs with 0
        df['æœŸé—´æ—¥å‡é”€é‡_num'] = pd.to_numeric(df['æœŸé—´æ—¥å‡é”€é‡'], errors='coerce').fillna(0)
        df['å½“å‰åº“å­˜_num'] = pd.to_numeric(df['å½“å‰åº“å­˜'], errors='coerce').fillna(0)

        # --- Calculate Target Stock Level ---
        df["ç›®æ ‡åº“å­˜æ°´å¹³"] = df["æœŸé—´æ—¥å‡é”€é‡_num"] * (target_days + safety_days)

        # --- Calculate Raw Suggestion ---
        df["å»ºè®®é‡‡è´­é‡_raw"] = df["ç›®æ ‡åº“å­˜æ°´å¹³"] - df["å½“å‰åº“å­˜_num"]

        # --- Final Suggestion (Round Up, Non-Negative Integer) ---
        # Apply math.ceil to round up, ensure it's at least 0, then convert to integer
        df["å»ºè®®é‡‡è´­é‡"] = df["å»ºè®®é‡‡è´­é‡_raw"].apply(
            lambda x: max(0, math.ceil(x)) if pd.notnull(x) and x > -float('inf') else 0 # Handle potential NaN/inf from raw calc
        ).astype(int)

        # --- Filter and Select Display Columns ---
        purchase_suggestions = df[df["å»ºè®®é‡‡è´­é‡"] > 0].copy() # Only show suggestions > 0

        # Define columns to display in the final suggestion table
        display_cols = ["äº§å“åç§°"] # Always show name
        # Add category if it exists in the suggestion dataframe
        if "äº§å“åˆ†ç±»" in purchase_suggestions.columns:
            display_cols.append("äº§å“åˆ†ç±»")

        # Add standard metric columns if they exist
        standard_cols = ["å½“å‰åº“å­˜", "é¢„è®¡å¯ç”¨å¤©æ•°", "æœŸé—´æ—¥å‡é”€é‡", "ç›®æ ‡åº“å­˜æ°´å¹³", "å»ºè®®é‡‡è´­é‡"]
        for col in standard_cols:
            if col in purchase_suggestions.columns:
                display_cols.append(col)

        # Ensure all selected display columns actually exist (safety check)
        final_display_cols = [col for col in display_cols if col in purchase_suggestions.columns]

        # Create the final dataframe with selected columns
        purchase_suggestions_final = purchase_suggestions[final_display_cols]

        # Sort by suggested quantity descending
        purchase_suggestions_final = purchase_suggestions_final.sort_values("å»ºè®®é‡‡è´­é‡", ascending=False)

        return purchase_suggestions_final

    except Exception as e:
        st.error(f"è®¡ç®—é‡‡è´­å»ºè®®æ—¶å‡ºé”™: {e}")
        traceback.print_exc() # Log detailed error
        return pd.DataFrame() # Return empty on error


# ======== Streamlit App Layout ========

st.set_page_config(page_title=f"TP.STER æ™ºèƒ½æ•°æ®å¹³å° {APP_VERSION}", layout="wide", page_icon="ğŸ“Š")

# Font Missing Hint
if not FONT_AVAILABLE:
    st.warning("""âš ï¸ **æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“...** å›¾è¡¨ä¸­çš„ä¸­æ–‡æ ‡ç­¾å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºã€‚è¯·å°è¯•å®‰è£… 'SimHei' æˆ–ç±»ä¼¼çš„ä¸­æ–‡å­—ä½“ï¼Œæˆ–å°†å­—ä½“æ–‡ä»¶ (å¦‚ `SimHei.ttf`) æ”¾ç½®äºè„šæœ¬ç›¸åŒç›®å½•ä¸‹ã€‚""", icon="â„¹ï¸")

# --- Sidebar ---
with st.sidebar:
    st.markdown(" ")
    logo_path = "tpster_logo.png" # Ensure your logo file is named this and in the same directory

    # --- MODIFIED LOGO DISPLAY (v2 - Base64) ---
    logo_data_uri = None
    placeholder_needed = True
    placeholder_url = "https://via.placeholder.com/180x80?text=TP.STER+LOGO"
    error_msg_logo = None

    if os.path.exists(logo_path):
        logo_data_uri = get_image_as_base64(logo_path)
        if logo_data_uri:
            placeholder_needed = False
        else:
            # File exists but encoding failed
            error_msg_logo = "Error loading logo file."
            placeholder_needed = True # Fallback to placeholder
    else:
        # File does not exist
        placeholder_needed = True


    image_source = logo_data_uri if not placeholder_needed else placeholder_url

    # Use markdown with centered div and img tag
    st.markdown(
        f"""
        <div style="text-align: center; margin-bottom: 5px;">
            <img src="{image_source}" alt="Logo" width="180" style="display: block; margin-left: auto; margin-right: auto;">
        </div>
        """,
        unsafe_allow_html=True
    )

    # Display caption or error centered below the image
    if placeholder_needed and not error_msg_logo:
        st.markdown("<div style='text-align: center; margin-top: 0px;'><p style='font-size: 0.8em; color: grey;'>Logo Placeholder</p></div>", unsafe_allow_html=True)
    elif error_msg_logo:
         st.markdown(f"<div style='text-align: center; margin-top: 0px;'><p style='font-size: 0.8em; color: red;'>{error_msg_logo}</p></div>", unsafe_allow_html=True)
    # --- END OF MODIFIED LOGO DISPLAY ---


    st.markdown(" ") # Add space after logo/caption
    st.markdown(f"<div style='text-align: center; font-size: 12px; color: gray;'>ç‰ˆæœ¬: {APP_VERSION}</div>", unsafe_allow_html=True)
    # Example User Info - Replace with actual authentication if needed
    st.markdown("<div style='text-align: center; font-size: 14px; color: gray; margin-bottom: 10px; margin-top: 5px;'>å½“å‰èº«ä»½ï¼š<strong>ç®¡ç†å‘˜</strong> (ç¤ºä¾‹)</div>", unsafe_allow_html=True)
    st.markdown("---")

    # --- File Uploaders ---
    st.markdown("#### ğŸ“‚ ç™¾è´§åŸæ•°æ® (é‡‡è´­å»ºè®®)")
    uploaded_main_file = st.file_uploader(
        label="ä¸Šä¼ ä¸»æ•°æ®æ–‡ä»¶ (Excel/CSV)",
        type=["xlsx", "xls", "csv"],
        key="main_data_uploader",
        help="ä¸Šä¼ åŒ…å«é”€å”®(è®¡ç®—éœ€æ±‚)ã€åº“å­˜(æ£€æŸ¥ä½åº“å­˜)åŠå¯é€‰é‡‡è´­æ•°æ®çš„æ–‡ä»¶ã€‚Exceléœ€å«'è®¢å•æ•°æ®', 'åº“å­˜æ•°æ®'è¡¨ã€‚CSVéœ€å«'DataType'åˆ—ã€‚"
    )
    st.divider()
    st.markdown("#### ğŸ·ï¸ ä»·æ ¼è°ƒæ•´")
    uploaded_pricing_file = st.file_uploader(
        label="ä¸Šä¼ å®šä»·æ–‡ä»¶ (Excel/CSV)",
        type=["xlsx", "xls", "csv"],
        key="pricing_data_uploader",
        help="ä¸Šä¼ ç”¨äºæ‰¹é‡è®¡ç®—é”€å”®ä»·æ ¼çš„æ–‡ä»¶ã€‚éœ€è¦åŒ…å« 'äº§å“ID'(æˆ–'å‹å·'), 'äº§å“åç§°'(æˆ–'å“å'), 'é‡‡è´­ä»·' åˆ—ã€‚"
    )
    st.divider()

    # --- NEW: Additional Data Uploaders ---
    st.markdown("#### ğŸ“Š è´¢åŠ¡æ•°æ® (å¯é€‰)")
    uploaded_financial_file = st.file_uploader(
        label="ä¸Šä¼ è´¢åŠ¡æ•°æ®æ–‡ä»¶ (Excel/CSV)",
        type=["xlsx", "xls", "csv"],
        key="financial_data_uploader",
        help="ä¸Šä¼ åŒ…å«æ”¶å…¥ã€æ”¯å‡ºã€åˆ©æ¶¦ç­‰è´¢åŠ¡æŒ‡æ ‡çš„æ–‡ä»¶ã€‚"
    )
    st.divider()

    st.markdown("#### ğŸ‘¥ CRM æ•°æ® (å¯é€‰)")
    uploaded_crm_file = st.file_uploader(
        label="ä¸Šä¼ CRMæ•°æ®æ–‡ä»¶ (Excel/CSV)",
        type=["xlsx", "xls", "csv"],
        key="crm_data_uploader",
        help="ä¸Šä¼ åŒ…å«å®¢æˆ·ä¿¡æ¯ã€å•†æœºã€æ´»åŠ¨ç­‰CRMæ•°æ®çš„æ–‡ä»¶ã€‚"
    )
    st.divider()

    st.markdown("#### ğŸ­ ç”Ÿäº§/è¿è¥æ•°æ® (å¯é€‰)")
    uploaded_production_file = st.file_uploader(
        label="ä¸Šä¼ ç”Ÿäº§æ•°æ®æ–‡ä»¶ (Excel/CSV)",
        type=["xlsx", "xls", "csv"],
        key="production_data_uploader",
        help="ä¸Šä¼ åŒ…å«ç”Ÿäº§è®¢å•ã€è®¾å¤‡åˆ©ç”¨ç‡ã€è´¨é‡æŒ‡æ ‡ç­‰æ•°æ®çš„æ–‡ä»¶ã€‚"
    )
    st.divider()

    st.markdown("#### ğŸ§‘â€ğŸ’¼ äººåŠ›èµ„æºæ•°æ® (å¯é€‰)")
    uploaded_hr_file = st.file_uploader(
        label="ä¸Šä¼ HRæ•°æ®æ–‡ä»¶ (Excel/CSV)",
        type=["xlsx", "xls", "csv"],
        key="hr_data_uploader",
        help="ä¸Šä¼ åŒ…å«å‘˜å·¥ä¿¡æ¯ã€å‡ºå‹¤ç­‰HRæ•°æ®çš„æ–‡ä»¶ã€‚"
    )
    st.divider()

    # --- Data Loading and State Management ---
    main_sales_data, main_stock_data, main_purchase_data = None, None, None
    pricing_data_loaded = None
    main_analysis_ready = False
    pricing_tool_ready = False
    financial_data_ready = False # New state
    crm_data_ready = False       # New state
    production_data_ready = False # New state
    hr_data_ready = False       # New state
    has_category_column_main = False
    # -- Process Main Data File --
    if uploaded_main_file:
        current_main_file_id = uploaded_main_file.file_id
        # Check if it's the same file that previously caused an error
        if current_main_file_id == st.session_state.last_main_file_id and st.session_state.main_load_error:
            st.error(st.session_state.main_load_error) # Show the stored error
            main_analysis_ready = False
        # Otherwise, try to load (new file or previously successful one)
        elif current_main_file_id != st.session_state.last_main_file_id or not st.session_state.main_load_error:
            st.session_state.last_main_file_id = current_main_file_id # Store current file ID
            with st.spinner("â³ æ­£åœ¨åŠ è½½åˆ†ææ•°æ®..."):
                uploaded_main_content = uploaded_main_file.getvalue()
                # Call modified load_data which returns error message
                sales_df, stock_df, purchase_df, error_msg = load_data(uploaded_main_content, uploaded_main_file.name)

            if error_msg:
                st.error(error_msg) # Show error returned by loading function
                st.session_state.main_load_error = error_msg # Store the error
                main_analysis_ready = False
            elif isinstance(sales_df, pd.DataFrame) and isinstance(stock_df, pd.DataFrame):
                st.session_state.main_load_error = None # Clear any previous error
                main_sales_data, main_stock_data, main_purchase_data = sales_df, stock_df, purchase_df
                main_analysis_ready = True
                # Check for category column after successful load
                if not main_stock_data.empty:
                    has_category_column_main = ("äº§å“åˆ†ç±»" in main_stock_data.columns and not main_stock_data["äº§å“åˆ†ç±»"].isnull().all())
            else:
                 # Handle unexpected case where load_data returned no error but invalid data
                unknown_error = "åŠ è½½åˆ†ææ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é—®é¢˜ï¼Œæœªæ”¶åˆ°æœ‰æ•ˆæ•°æ®ã€‚"
                st.error(unknown_error)
                st.session_state.main_load_error = unknown_error
                main_analysis_ready = False

    # If user clears the file uploader, reset the state
    elif not uploaded_main_file and st.session_state.last_main_file_id is not None:
        st.session_state.last_main_file_id = None
        st.session_state.main_load_error = None
        main_analysis_ready = False # Ensure state is reset


    # -- Process Pricing Data File (Similar Logic) --
    if uploaded_pricing_file:
        current_pricing_file_id = uploaded_pricing_file.file_id
        if current_pricing_file_id == st.session_state.last_pricing_file_id and st.session_state.pricing_load_error:
            st.error(st.session_state.pricing_load_error)
            pricing_tool_ready = False
        elif current_pricing_file_id != st.session_state.last_pricing_file_id or not st.session_state.pricing_load_error:
            st.session_state.last_pricing_file_id = current_pricing_file_id
            with st.spinner("â³ æ­£åœ¨åŠ è½½å®šä»·æ•°æ®..."):
                uploaded_pricing_content = uploaded_pricing_file.getvalue()
                # Call modified load_pricing_data
                pricing_df, error_msg = load_pricing_data(uploaded_pricing_content, uploaded_pricing_file.name)

            if error_msg:
                st.error(error_msg)
                st.session_state.pricing_load_error = error_msg
                pricing_tool_ready = False
            elif isinstance(pricing_df, pd.DataFrame):
                st.session_state.pricing_load_error = None
                pricing_data_loaded = pricing_df
                pricing_tool_ready = True
            else:
                unknown_error = "åŠ è½½å®šä»·æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é—®é¢˜ï¼Œæœªæ”¶åˆ°æœ‰æ•ˆæ•°æ®ã€‚"
                st.error(unknown_error)
                st.session_state.pricing_load_error = unknown_error
                pricing_tool_ready = False

    elif not uploaded_pricing_file and st.session_state.last_pricing_file_id is not None:
        st.session_state.last_pricing_file_id = None
        st.session_state.pricing_load_error = None
        pricing_tool_ready = False # Ensure state is reset

    # --- NEW: Process Additional Data Files (Placeholder Logic) ---
    # Initialize session state for new file types if they don't exist
    if 'financial_load_error' not in st.session_state: st.session_state.financial_load_error = None
    if 'last_financial_file_id' not in st.session_state: st.session_state.last_financial_file_id = None
    if 'crm_load_error' not in st.session_state: st.session_state.crm_load_error = None
    if 'last_crm_file_id' not in st.session_state: st.session_state.last_crm_file_id = None
    if 'production_load_error' not in st.session_state: st.session_state.production_load_error = None
    if 'last_production_file_id' not in st.session_state: st.session_state.last_production_file_id = None
    if 'hr_load_error' not in st.session_state: st.session_state.hr_load_error = None
    if 'last_hr_file_id' not in st.session_state: st.session_state.last_hr_file_id = None

    # Placeholder processing logic for Financial Data
    financial_data_loaded = None
    if uploaded_financial_file:
        current_financial_file_id = uploaded_financial_file.file_id
        if current_financial_file_id == st.session_state.last_financial_file_id and st.session_state.financial_load_error:
            st.error(f"è´¢åŠ¡æ•°æ®åŠ è½½é”™è¯¯: {st.session_state.financial_load_error}")
            financial_data_ready = False
        elif current_financial_file_id != st.session_state.last_financial_file_id or not st.session_state.financial_load_error:
            st.session_state.last_financial_file_id = current_financial_file_id
            try:
                # Placeholder: In a real scenario, call a load_financial_data function here
                financial_data_loaded = pd.read_excel(io.BytesIO(uploaded_financial_file.getvalue())) # Basic load example
                st.success("è´¢åŠ¡æ•°æ®æ–‡ä»¶å·²åŠ è½½ (å ä½ç¬¦)ã€‚")
                st.session_state.financial_load_error = None
                financial_data_ready = True
            except Exception as e:
                error_msg = f"åŠ è½½è´¢åŠ¡æ•°æ®å¤±è´¥: {e}"
                st.error(error_msg)
                st.session_state.financial_load_error = error_msg
                financial_data_ready = False
    elif not uploaded_financial_file and st.session_state.last_financial_file_id is not None:
        st.session_state.last_financial_file_id = None
        st.session_state.financial_load_error = None
        financial_data_ready = False

    # Placeholder processing logic for CRM Data (similar structure)
    crm_data_loaded = None
    if uploaded_crm_file:
        current_crm_file_id = uploaded_crm_file.file_id
        if current_crm_file_id == st.session_state.last_crm_file_id and st.session_state.crm_load_error:
            st.error(f"CRMæ•°æ®åŠ è½½é”™è¯¯: {st.session_state.crm_load_error}")
            crm_data_ready = False
        elif current_crm_file_id != st.session_state.last_crm_file_id or not st.session_state.crm_load_error:
            st.session_state.last_crm_file_id = current_crm_file_id
            try:
                crm_data_loaded = pd.read_excel(io.BytesIO(uploaded_crm_file.getvalue()))
                st.success("CRMæ•°æ®æ–‡ä»¶å·²åŠ è½½ (å ä½ç¬¦)ã€‚")
                st.session_state.crm_load_error = None
                crm_data_ready = True
            except Exception as e:
                error_msg = f"åŠ è½½CRMæ•°æ®å¤±è´¥: {e}"
                st.error(error_msg)
                st.session_state.crm_load_error = error_msg
                crm_data_ready = False
    elif not uploaded_crm_file and st.session_state.last_crm_file_id is not None:
        st.session_state.last_crm_file_id = None
        st.session_state.crm_load_error = None
        crm_data_ready = False

    # Placeholder processing logic for Production Data (similar structure)
    production_data_loaded = None
    if uploaded_production_file:
        current_production_file_id = uploaded_production_file.file_id
        if current_production_file_id == st.session_state.last_production_file_id and st.session_state.production_load_error:
            st.error(f"ç”Ÿäº§æ•°æ®åŠ è½½é”™è¯¯: {st.session_state.production_load_error}")
            production_data_ready = False
        elif current_production_file_id != st.session_state.last_production_file_id or not st.session_state.production_load_error:
            st.session_state.last_production_file_id = current_production_file_id
            try:
                production_data_loaded = pd.read_excel(io.BytesIO(uploaded_production_file.getvalue()))
                st.success("ç”Ÿäº§æ•°æ®æ–‡ä»¶å·²åŠ è½½ (å ä½ç¬¦)ã€‚")
                st.session_state.production_load_error = None
                production_data_ready = True
            except Exception as e:
                error_msg = f"åŠ è½½ç”Ÿäº§æ•°æ®å¤±è´¥: {e}"
                st.error(error_msg)
                st.session_state.production_load_error = error_msg
                production_data_ready = False
    elif not uploaded_production_file and st.session_state.last_production_file_id is not None:
        st.session_state.last_production_file_id = None
        st.session_state.production_load_error = None
        production_data_ready = False

    # Placeholder processing logic for HR Data (similar structure)
    hr_data_loaded = None
    if uploaded_hr_file:
        current_hr_file_id = uploaded_hr_file.file_id
        if current_hr_file_id == st.session_state.last_hr_file_id and st.session_state.hr_load_error:
            st.error(f"HRæ•°æ®åŠ è½½é”™è¯¯: {st.session_state.hr_load_error}")
            hr_data_ready = False
        elif current_hr_file_id != st.session_state.last_hr_file_id or not st.session_state.hr_load_error:
            st.session_state.last_hr_file_id = current_hr_file_id
            try:
                hr_data_loaded = pd.read_excel(io.BytesIO(uploaded_hr_file.getvalue()))
                st.success("HRæ•°æ®æ–‡ä»¶å·²åŠ è½½ (å ä½ç¬¦)ã€‚")
                st.session_state.hr_load_error = None
                hr_data_ready = True
            except Exception as e:
                error_msg = f"åŠ è½½HRæ•°æ®å¤±è´¥: {e}"
                st.error(error_msg)
                st.session_state.hr_load_error = error_msg
                hr_data_ready = False
    elif not uploaded_hr_file and st.session_state.last_hr_file_id is not None:
        st.session_state.last_hr_file_id = None
        st.session_state.hr_load_error = None
        hr_data_ready = False
    # --- END NEW: Process Additional Data Files ---

    # --- Analysis Parameters (Only show if main data loaded successfully) ---
    selected_category = "å…¨éƒ¨" # Default value
    try:
        default_end_date = datetime.now(APP_TIMEZONE).date()
    except Exception as e:
        st.warning(f"è·å–å½“å‰æ—¥æœŸæ—¶å‡ºé”™({APP_TIMEZONE_STR}): {e}ï¼Œä½¿ç”¨ UTCã€‚")
        default_end_date = datetime.utcnow().date()
    default_start_date = default_end_date - timedelta(days=89) # Default 90 day period
    date_range = (default_start_date, default_end_date) # Default range tuple
    target_days_input = 30 # Default target days
    safety_days_input = 7 # Default safety days

    if main_analysis_ready:
        st.markdown("#### âš™ï¸ åˆ†æå‚æ•°è®¾ç½®")

        # Category Filter
        if has_category_column_main:
            try:
                # Get unique categories, convert to string, sort, handle potential NaN/None
                all_categories = sorted([str(cat) for cat in main_stock_data["äº§å“åˆ†ç±»"].dropna().unique()])
                options = ["å…¨éƒ¨"] + [cat for cat in all_categories if cat != "å…¨éƒ¨"] # Ensure "å…¨éƒ¨" is first
                selected_category = st.selectbox(
                    "ğŸ—‚ï¸ äº§å“åˆ†ç±»ç­›é€‰",
                    options=options,
                    index=0, # Default to "å…¨éƒ¨"
                    key="category_select_key"
                )
            except Exception as cat_err:
                st.warning(f"åŠ è½½äº§å“åˆ†ç±»é€‰é¡¹æ—¶å‡ºé”™: {cat_err}")
                selected_category = "å…¨éƒ¨" # Fallback
                has_category_column_main = False # Disable filtering if error occurs
        else:
            selected_category = "å…¨éƒ¨" # Set default if no category data

        # Date Range Selector
        st.markdown("##### ğŸ—“ï¸ é”€å”®åˆ†æå‘¨æœŸ")
        min_date_allowed = None
        max_date_allowed = None
        if main_sales_data is not None and not main_sales_data.empty and 'è®¢å•æ—¥æœŸ' in main_sales_data.columns:
             # Use already converted and cleaned dates
             valid_dates = main_sales_data['è®¢å•æ—¥æœŸ'].dropna()
             if not valid_dates.empty:
                 try:
                      min_date_allowed = valid_dates.min().date()
                      max_date_allowed = valid_dates.max().date()
                 except Exception as date_parse_err:
                      st.warning(f"æ— æ³•è§£æé”€å”®æ•°æ®ä¸­çš„æ—¥æœŸèŒƒå›´: {date_parse_err}")

        # Determine final min/max for the date picker, considering data range and default range
        final_min_date = min(min_date_allowed, default_start_date) if min_date_allowed else default_start_date
        final_max_date = max(max_date_allowed, default_end_date) if max_date_allowed else default_end_date
        # Ensure min is not after max
        if final_min_date > final_max_date: final_min_date = final_max_date

        # Adjust default start/end to be within the allowed range
        actual_default_start = max(final_min_date, default_start_date)
        actual_default_end = min(final_max_date, default_end_date)
        # Ensure start is not after end in default values
        if actual_default_start > actual_default_end: actual_default_start = actual_default_end

        try:
            date_range_input = st.date_input(
                "é€‰æ‹©å‘¨æœŸ",
                value=(actual_default_start, actual_default_end), # Use adjusted defaults
                min_value=final_min_date,
                max_value=final_max_date,
                key="date_range_selector",
                help="é€‰æ‹©ç”¨äºè®¡ç®—æœŸé—´æ—¥å‡é”€é‡ç­‰æŒ‡æ ‡çš„æ—¶é—´èŒƒå›´ã€‚"
            )
            # Validate the input tuple/list
            if isinstance(date_range_input, (tuple, list)) and len(date_range_input) == 2:
                start_d, end_d = date_range_input
                if start_d <= end_d:
                    date_range = (start_d, end_d) # Update date_range if valid
                else:
                    st.warning("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸï¼Œä½¿ç”¨ä¸Šæ¬¡æœ‰æ•ˆæˆ–é»˜è®¤èŒƒå›´ã€‚")
                    # Keep previous date_range value
            else:
                # Handle cases where date_input might return a single date if max_value = min_value
                 if isinstance(date_range_input, datetime.date):
                     date_range = (date_range_input, date_range_input)
                 else:
                     st.warning("æ—¥æœŸèŒƒå›´é€‰æ‹©æ— æ•ˆï¼Œå°†ä½¿ç”¨ä¸Šæ¬¡æœ‰æ•ˆæˆ–é»˜è®¤èŒƒå›´ã€‚")
                     # Keep previous date_range value
        except Exception as date_err:
            st.warning(f"æ—¥æœŸèŒƒå›´è®¾ç½®æ—¶å‡ºé”™: {date_err}ï¼Œå°†ä½¿ç”¨é»˜è®¤èŒƒå›´ã€‚")
            date_range = (actual_default_start, actual_default_end) # Fallback to defaults on error

        # Inventory/Purchase Parameters
        st.markdown("##### âš™ï¸ åº“å­˜ä¸é‡‡è´­å‚æ•°")
        target_days_input = st.number_input(
            "ç›®æ ‡åº“å­˜å¤©æ•°",
            min_value=1, max_value=180, value=30, step=1,
            key="target_days_key",
            help="æœŸæœ›åº“å­˜èƒ½æ»¡è¶³å¤šå°‘å¤©çš„é”€å”®"
        )
        safety_days_input = st.number_input(
            "å®‰å…¨åº“å­˜å¤©æ•°",
            min_value=0, max_value=90, value=7, step=1,
            key="safety_days_key",
            help="é¢å¤–çš„ç¼“å†²å¤©æ•°"
        )

# --- Main Area ---
st.markdown(f"""<div style='text-align: center; padding: 15px 0 10px 0;'><h1 style='margin-bottom: 5px; color: #262730;'>ğŸ“Š TP.STER æ™ºèƒ½æ•°æ®å¹³å° {APP_VERSION}</h1><p style='color: #5C5C5C; font-size: 18px; font-weight: 300; margin-top: 5px;'>æ´å¯Ÿæ•°æ®ä»·å€¼ Â· é©±åŠ¨æ™ºèƒ½å†³ç­– Â· ä¼˜åŒ–ä¾›åº”é“¾ç®¡ç†</p></div>""", unsafe_allow_html=True)
st.divider()

# --- Main Content ---
# Display Welcome Message if no files are uploaded at all (Check all potential uploaders now)
if not any([uploaded_main_file, uploaded_pricing_file, uploaded_financial_file, uploaded_crm_file, uploaded_production_file, uploaded_hr_file]): # Adjusted condition
    # Use the Centered and refined welcome message
    st.markdown(
        f"""
        <div style='text-align: center; max-width: 800px; margin: auto; padding-top: 20px; padding-bottom: 30px;'>

        æœ¬å¹³å°é€šè¿‡é›†æˆåŒ–åˆ†æï¼ŒåŠ©æ‚¨è½»æ¾ç®¡ç†ä¸šåŠ¡æ•°æ®ï¼Œæ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š

        <div style='text-align: left; display: inline-block; margin-top: 15px; margin-bottom: 20px;'>
        <ul style="list-style-type: none; padding-left: 0; font-size: 16px;">
            <li style="margin-bottom: 10px;">ğŸ“Š &nbsp; <strong>é”€å”®åˆ†æ:</strong> è¿½è¸ªè¶‹åŠ¿ï¼Œèšç„¦æ ¸å¿ƒäº§å“ã€‚</li>
            <li style="margin-bottom: 10px;">ğŸ“¦ &nbsp; <strong>åº“å­˜åˆ†æ:</strong> è¯„ä¼°å¥åº·åº¦ï¼Œä¼˜åŒ–å‘¨è½¬ã€‚</li>
            <li style="margin-bottom: 10px;">ğŸ›’ &nbsp; <strong>é‡‡è´­å»ºè®®:</strong> æ™ºèƒ½é¢„æµ‹ï¼Œç²¾å‡†è¡¥è´§ã€‚</li>
            <li style="margin-bottom: 10px;">ğŸ·ï¸ &nbsp; <strong>å®šä»·å·¥å…·:</strong> æˆæœ¬+åˆ©æ¶¦ï¼Œä¸€é”®å®šä»·ã€‚</li>
            <li style="margin-bottom: 10px;">ğŸ’° &nbsp; <strong>è´¢åŠ¡æŒ‡æ ‡:</strong> æ¦‚è§ˆå…³é”®è´¢åŠ¡æ•°æ®ã€‚</li>
            <li style="margin-bottom: 10px;">ğŸ‘¥ &nbsp; <strong>CRM æ‘˜è¦:</strong> æ´å¯Ÿå®¢æˆ·å…³ç³»åŠ¨æ€ã€‚</li>
            <li style="margin-bottom: 10px;">ğŸ­ &nbsp; <strong>ç”Ÿäº§ç›‘æ§:</strong> è·Ÿè¸ªç”Ÿäº§è¿è¥æ•ˆç‡ã€‚</li>
            <li style="margin-bottom: 10px;">ğŸ§‘â€ğŸ’¼ &nbsp; <strong>HR æ¦‚è§ˆ:</strong> æŒæ¡äººåŠ›èµ„æºçŠ¶å†µã€‚</li>
        </ul>
        </div>

        <hr style='margin-top: 15px; margin-bottom: 25px; border-top: 1px solid #eee;'>

        #### **å¼€å§‹ä½¿ç”¨**

        <p style="font-size: 16px; margin-bottom: 20px;">è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶ (æ”¯æŒ <code>.xlsx</code>, <code>.xls</code>, <code>.csv</code>):</p>

        <div style='text-align: left; max-width: 600px; margin: auto; background-color: #f8f9fa; padding: 15px; border-radius: 5px; border: 1px solid #e9ecef;'>
        <ol style="padding-left: 25px; margin-bottom: 0;">
            <li style="margin-bottom: 15px;">
                <strong>æ ¸å¿ƒæ•°æ® (ç”¨äºåˆ†æä¸å»ºè®®):</strong><br>
                ä¸Šä¼ åŒ…å« <code>è®¢å•æ•°æ®</code> å’Œ <code>åº“å­˜æ•°æ®</code> çš„æ–‡ä»¶ã€‚<br>
                <span style="font-size: 0.9em; color: #6c757d;"><em>(Excelæ–‡ä»¶éœ€åŒ…å«åä¸º "è®¢å•æ•°æ®" å’Œ "åº“å­˜æ•°æ®" çš„å·¥ä½œè¡¨)</em></span><br>
                <span style="font-size: 0.9em; color: #6c757d;"><em>(CSVæ–‡ä»¶éœ€åŒ…å« 'DataType' åˆ—åŒºåˆ†æ•°æ®ç±»å‹)</em></span>
            </li>
            <li>
                <strong>å®šä»·æ•°æ® (ç”¨äºä»·æ ¼è®¡ç®—):</strong><br>
                ä¸Šä¼ åŒ…å« <code>äº§å“ID</code>(æˆ–<code>å‹å·</code>)ã€<code>äº§å“åç§°</code>(æˆ–<code>å“å</code>)ã€<code>é‡‡è´­ä»·</code> çš„æ–‡ä»¶ã€‚
            </li>
        </ol>
        </div>

        </div>
        """, unsafe_allow_html=True)

# Display content if at least one file was uploaded and processed (or attempted) - Check all potential uploaders
elif any([uploaded_main_file, uploaded_pricing_file, uploaded_financial_file, uploaded_crm_file, uploaded_production_file, uploaded_hr_file]): # Adjusted condition

    metrics = {}
    stock_analysis = pd.DataFrame()
    purchase_suggestions = pd.DataFrame()
    has_category_data = False # Will be set by calculate_metrics
    data_filtered_message = ""

    # Perform main analysis only if data is ready
    if main_analysis_ready:
        st.markdown(f"### ğŸ“ˆ **ä¸»æ•°æ®åˆ†æ** ({date_range[0].strftime('%Y-%m-%d')} åˆ° {date_range[1].strftime('%Y-%m-%d')})")

        # --- Apply Category Filtering if selected ---
        sales_calc = main_sales_data.copy()
        stock_calc = main_stock_data.copy()
        # Ensure purchase_calc is a DataFrame even if main_purchase_data was None initially
        purchase_calc = main_purchase_data.copy() if main_purchase_data is not None else pd.DataFrame()

        if selected_category != "å…¨éƒ¨" and has_category_column_main:
            data_filtered_message = f"åˆ†æå·²ç­›é€‰ï¼Œä»…åŒ…å«åˆ†ç±»ï¼š**{selected_category}**"
            st.info(data_filtered_message) # Show filter info early

            # Filter stock data first
            if "äº§å“åˆ†ç±»" in stock_calc.columns and not stock_calc.empty:
                 category_product_ids = stock_calc[stock_calc["äº§å“åˆ†ç±»"] == selected_category]["äº§å“ID"].unique()
                 stock_calc = stock_calc[stock_calc["äº§å“ID"].isin(category_product_ids)] # Filter stock

                 # Filter sales and purchase based on the product IDs from the filtered stock
                 if not sales_calc.empty and "äº§å“ID" in sales_calc.columns:
                     sales_calc = sales_calc[sales_calc["äº§å“ID"].isin(category_product_ids)]
                 if purchase_calc is not None and not purchase_calc.empty and "äº§å“ID" in purchase_calc.columns:
                     purchase_calc = purchase_calc[purchase_calc["äº§å“ID"].isin(category_product_ids)]
            else:
                 st.warning("åº“å­˜æ•°æ®ä¸­æ— 'äº§å“åˆ†ç±»'åˆ—æˆ–æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æŒ‰åˆ†ç±»ç­›é€‰ã€‚å°†åˆ†æå…¨éƒ¨æ•°æ®ã€‚")
                 data_filtered_message = "" # Clear filter message if filtering failed

        # --- Run Calculations ---
        start_date_dt, end_date_dt = date_range
        try:
            with st.spinner('â³ æ­£åœ¨åˆ†æä¸»æ•°æ®...'):
                 # Pass the potentially filtered dataframes to calculation functions
                 metrics, stock_analysis, has_category_data = calculate_metrics(
                     sales_calc, stock_calc, purchase_calc, start_date_dt, end_date_dt
                 )
                 # Calculate suggestions based on the result of calculate_metrics
                 purchase_suggestions = calculate_purchase_suggestions(
                     stock_analysis, target_days_input, safety_days_input
                 )
        except Exception as calc_error:
            st.error(f"âŒ åœ¨æ‰§è¡Œä¸»æ•°æ®è®¡ç®—æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {calc_error}")
            traceback.print_exc()
            # Reset results to prevent downstream errors
            metrics = {}
            stock_analysis = pd.DataFrame()
            purchase_suggestions = pd.DataFrame()
            # Keep main_analysis_ready as True if initial load was ok, but show error

        # --- Display KPIs (if calculation succeeded) ---
        if main_analysis_ready and metrics: # Check if metrics dictionary was populated
            if data_filtered_message:
                st.markdown(f"**ç­›é€‰åˆ†ç±»:** `{selected_category}`") # Show filter again if applied

            kpi_cols = st.columns(5)
            # Use .get() with default values for robustness
            kpi_cols[0].metric("æœŸé—´æ€»é”€é‡", f"{metrics.get('total_sales_period', 0):,} ä¸ª")
            kpi_cols[1].metric("æœŸé—´æ—¥å‡é”€é‡", f"{metrics.get('avg_daily_sales_period', 0):,.1f} ä¸ª/å¤©")

            # Calculate SKU count from the final stock_analysis dataframe
            sku_count = stock_analysis['äº§å“ID'].nunique() if isinstance(stock_analysis, pd.DataFrame) and not stock_analysis.empty else 0
            kpi_cols[2].metric("åˆ†æäº§å“ SKU æ•°", f"{sku_count:,}")

            # Calculate total stock from the final stock_analysis dataframe
            total_stock_kpi = 0
            if isinstance(stock_analysis, pd.DataFrame) and 'å½“å‰åº“å­˜' in stock_analysis.columns:
                 total_stock_kpi = int(pd.to_numeric(stock_analysis['å½“å‰åº“å­˜'], errors='coerce').fillna(0).sum())
            kpi_cols[3].metric("å½“å‰æ€»åº“å­˜", f"{total_stock_kpi:,} ä¸ª")

            kpi_cols[4].metric("æœŸé—´çƒ­é”€äº§å“", metrics.get('top_product_period', 'æ— '))
            st.divider()
        elif main_analysis_ready and not metrics:
             # This case might happen if calculate_metrics itself failed internally but didn't raise exception caught above
             st.warning("ä¸»æ•°æ®åˆ†æè®¡ç®—å®Œæˆï¼Œä½†æœªèƒ½ç”Ÿæˆå…³é”®æŒ‡æ ‡ã€‚è¯·æ£€æŸ¥æ•°æ®æˆ–è®¡ç®—é€»è¾‘ã€‚")
             st.divider()
        # If main_analysis_ready is False (loading failed), KPIs won't show. Error is shown in sidebar.

    # --- Display Tabs ---
    # --- Define Tabs (Including New Modules) ---
    tab_list = [
        "ğŸ“Š é”€å”®åˆ†æ", "ğŸ“¦ åº“å­˜åˆ†æ", "ğŸ›’ é‡‡è´­å»ºè®®", "ğŸ·ï¸ å®šä»·å·¥å…·", # Existing
        "ğŸ’° è´¢åŠ¡æŒ‡æ ‡", "ğŸ‘¥ CRMæ‘˜è¦", "ğŸ­ ç”Ÿäº§ç›‘æ§", "ğŸ§‘â€ğŸ’¼ HRæ¦‚è§ˆ", # New
        "ğŸ”” å¾…åŠæé†’", "ğŸ“ˆ è‡ªå®šä¹‰åˆ†æ" # New utility tabs
    ]
    tabs = st.tabs(tab_list)

    # Assign tabs to variables for clarity
    tab_sales = tabs[0]
    tab_inventory = tabs[1]
    tab_purchase = tabs[2]
    tab_pricing = tabs[3]
    tab_financial = tabs[4]
    tab_crm = tabs[5]
    tab_production = tabs[6]
    tab_hr = tabs[7]
    tab_alerts = tabs[8]
    tab_custom_analysis = tabs[9]
    # --- End Define Tabs ---
    # --- End Define Tabs ---

    # --- Sales Analysis Tab ---
    with tab_sales:
        if main_analysis_ready and metrics: # Only show content if main analysis ran successfully
             st.subheader("é”€å”®è¶‹åŠ¿ä¸å æ¯”åˆ†æ")
             chart_cols = st.columns([2, 1])

             # --- Monthly Sales Trend (Line Chart) ---
             with chart_cols[0]:
                 st.markdown("###### æœˆåº¦é”€å”®è¶‹åŠ¿ (æŒ‰è´­ä¹°æ•°é‡)")
                 monthly_data = metrics.get('monthly_trend_chart_data')
                 # Check if data is valid Series and contains meaningful data
                 if isinstance(monthly_data, pd.Series) and not monthly_data.empty and not (monthly_data.isnull().all() or (monthly_data == 0).all()):
                     fig_line = None # Initialize figure variable
                     try:
                          # Convert PeriodIndex to Timestamp for plotting
                          plot_index = monthly_data.index.to_timestamp()
                          fig_line, ax_line = plt.subplots(figsize=(8, 3))
                          ax_line.plot(plot_index, monthly_data.values, marker='o', linestyle='-', linewidth=1.5, markersize=4, color='#1f77b4')

                          xlabel, ylabel = "æœˆä»½", "è´­ä¹°æ•°é‡ (ä¸ª)"
                          # Apply Chinese font if available
                          if FONT_AVAILABLE and chinese_font:
                              ax_line.set_xlabel(xlabel, fontproperties=chinese_font, fontsize=9)
                              ax_line.set_ylabel(ylabel, fontproperties=chinese_font, fontsize=9)
                              # Set font for tick labels
                              for label in ax_line.get_xticklabels() + ax_line.get_yticklabels():
                                   label.set_fontproperties(chinese_font)
                                   label.set_fontsize(8)
                          else:
                              ax_line.set_xlabel(xlabel, fontsize=9)
                              ax_line.set_ylabel(ylabel, fontsize=9)
                              for label in ax_line.get_xticklabels() + ax_line.get_yticklabels():
                                   label.set_fontsize(8)

                          # Format x-axis dates
                          ax_line.xaxis.set_major_formatter(mdates.DateFormatter("%Yå¹´%mæœˆ"))
                          # Adjust date locator interval based on data length
                          ax_line.xaxis.set_major_locator(mdates.MonthLocator(interval=max(1, len(plot_index)//6)))
                          ax_line.grid(axis='y', linestyle=':', alpha=0.7)
                          fig_line.autofmt_xdate(rotation=30, ha='right') # Auto format date labels
                          plt.tight_layout() # Adjust layout
                          st.pyplot(fig_line, clear_figure=True) # Display plot

                     except AttributeError as attr_err:
                          st.warning(f"æœˆåº¦è¶‹åŠ¿å›¾çš„æ—¥æœŸæ ¼å¼æ— æ³•è¢«æ­£ç¡®å¤„ç†ä»¥è¿›è¡Œç»˜å›¾: {attr_err}")
                     except Exception as e:
                          st.error(f"ç»˜åˆ¶æœˆåº¦è¶‹åŠ¿å›¾æ—¶å‡ºé”™: {e}")
                          if fig_line is not None: plt.close(fig_line) # Close plot if error occurred during processing
                 else:
                     st.caption("æ— è¶³å¤Ÿæ•°æ®ç»˜åˆ¶æœˆåº¦é”€å”®è¶‹åŠ¿ã€‚")

             # --- Sales Share (Pie Chart) ---
             with chart_cols[1]:
                 st.markdown("###### æœŸé—´é”€é‡å æ¯” Top 10")
                 top_data = metrics.get('top_selling_period_chart_data')
                 # Check if data is valid Series and has a positive sum
                 if isinstance(top_data, pd.Series) and not top_data.empty and top_data.sum() > 0:
                     fig_pie = None # Initialize figure variable
                     try:
                          pie_labels = [str(label) for label in top_data.index] # Ensure labels are strings
                          pie_values = top_data.values
                          fig_pie, ax_pie = plt.subplots(figsize=(5, 3)) # Adjust figure size if needed

                          # Define text properties, considering font availability
                          text_props = {'fontproperties': chinese_font, 'size': 8} if FONT_AVAILABLE and chinese_font else {'size': 8}
                          legend_props = {'prop': chinese_font} if FONT_AVAILABLE and chinese_font else {}
                          title_fontprop = chinese_font if FONT_AVAILABLE and chinese_font else None

                          # Use a suitable colormap
                          colors = plt.get_cmap('Pastel1').colors

                          wedges, texts, autotexts = ax_pie.pie(
                              pie_values,
                              autopct='%1.1f%%', # Format percentage
                              startangle=90,
                              pctdistance=0.85, # Distance of percentage text from center
                              colors=colors,
                              textprops=text_props # Apply font props to percentage labels
                          )

                          # Set font for autotexts (percentages) explicitly if font is available
                          if FONT_AVAILABLE and chinese_font:
                             plt.setp(autotexts, fontproperties=chinese_font)

                          ax_pie.axis('equal') # Equal aspect ratio ensures pie is drawn as a circle.

                          # Add legend
                          legend_title = "äº§å“åç§°"
                          # Position legend outside the pie
                          ax_pie.legend(wedges, pie_labels,
                                        title=legend_title,
                                        loc="center left",
                                        bbox_to_anchor=(1.05, 0, 0.5, 1), # Adjust anchor to position legend
                                        fontsize=8,
                                        prop=legend_props.get('prop'), # Font for legend items
                                        title_fontproperties=title_fontprop) # Font for legend title

                          plt.subplots_adjust(left=0.1, right=0.65) # Adjust subplot to make space for legend
                          st.pyplot(fig_pie, clear_figure=True) # Display plot

                     except Exception as e:
                          st.error(f"ç»˜åˆ¶é”€é‡å æ¯”å›¾æ—¶å‡ºé”™: {e}")
                          if fig_pie is not None: plt.close(fig_pie) # Close plot on error
                 else:
                     st.caption("æ— è¶³å¤Ÿæ•°æ®ç»˜åˆ¶é”€é‡å æ¯”å›¾ã€‚")
        elif not main_analysis_ready:
            st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ æœ‰æ•ˆçš„ **ç™¾è´§åŸæ•°æ®** æ–‡ä»¶ä»¥æŸ¥çœ‹é”€å”®åˆ†æã€‚")
            if st.session_state.main_load_error:
                st.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {st.session_state.main_load_error}")


    # --- Inventory Analysis Tab ---
    with tab_inventory:
        if main_analysis_ready and isinstance(stock_analysis, pd.DataFrame) and not stock_analysis.empty:
             st.subheader("åº“å­˜å¥åº·åº¦ä¸è´¦é¾„åˆ†æ")

             # --- Stock Aging Distribution (Bar Chart) ---
             st.markdown("###### åº“å­˜è´¦é¾„åˆ†å¸ƒ (æŒ‰ SKU æ•°)")
             # Helper function for bucketing (can be defined inside or outside)
             def get_age_bucket(days):
                 try:
                      if pd.isna(days): return "æœªçŸ¥"
                      days_int = int(float(days)) # Ensure it's treated as number
                 except (ValueError, TypeError, OverflowError):
                      return "æœªçŸ¥" # Handle non-numeric gracefully

                 if days_int == 9999: return "ä»æœªå”®å‡º" # Special code for never sold / no last sale date
                 elif days_int <= 30: return "0-30 å¤©"
                 elif days_int <= 60: return "31-60 å¤©"
                 elif days_int <= 90: return "61-90 å¤©"
                 elif days_int <= 180: return "91-180 å¤©"
                 else: return "181+ å¤©"

             try:
                 if 'å‹è´§æ—¶é—´_å¤©' in stock_analysis.columns:
                     # Apply the bucketing function
                     stock_analysis['åº“å­˜è´¦é¾„åˆ†ç»„'] = stock_analysis['å‹è´§æ—¶é—´_å¤©'].apply(get_age_bucket)
                     # Define the desired order for the categories
                     age_order = ["0-30 å¤©", "31-60 å¤©", "61-90 å¤©", "91-180 å¤©", "181+ å¤©", "ä»æœªå”®å‡º", "æœªçŸ¥"]
                     # Group by the new category, count SKUs, reindex to enforce order, fill missing groups with 0
                     aging_data_sku = stock_analysis.groupby('åº“å­˜è´¦é¾„åˆ†ç»„', observed=False).size().reindex(age_order).fillna(0)
                     # Filter out categories with zero count for cleaner chart
                     aging_data_sku = aging_data_sku[aging_data_sku > 0]

                     if not aging_data_sku.empty:
                          # Use Streamlit's built-in bar chart
                          st.bar_chart(aging_data_sku, use_container_width=True)
                          st.caption("åº“å­˜è´¦é¾„æ ¹æ®äº§å“æœ€åé”€å”®æ—¥æœŸè®¡ç®—ã€‚'ä»æœªå”®å‡º'è¡¨ç¤ºæ— é”€å”®è®°å½•æˆ–æ— æ³•ç¡®å®šæœ€åé”€å”®æ—¥æœŸã€‚")
                     else:
                          st.caption("æ— æœ‰æ•ˆçš„åº“å­˜è´¦é¾„æ•°æ®å¯ä¾›ç»˜åˆ¶ã€‚")
                 else:
                     st.warning("æ— æ³•è®¡ç®—åº“å­˜è´¦é¾„åˆ†å¸ƒï¼Œç¼ºå°‘ 'å‹è´§æ—¶é—´_å¤©' æ•°æ®ã€‚")
             except Exception as e:
                 st.error(f"ç”Ÿæˆåº“å­˜è´¦é¾„å›¾è¡¨æ—¶å‡ºé”™: {e}")
                 traceback.print_exc()

             # --- Inventory Details Table ---
             st.markdown("---")
             st.markdown("###### åº“å­˜æ˜ç»†ä¸çŠ¶æ€")
             # Sort the full dataframe (e.g., by stock age descending)
             stock_analysis_sorted = stock_analysis.sort_values("å‹è´§æ—¶é—´_å¤©", ascending=False) if 'å‹è´§æ—¶é—´_å¤©' in stock_analysis.columns else stock_analysis
             # Limit display rows
             stock_analysis_display_limited = stock_analysis_sorted.head(TOP_N_DISPLAY)
             st.info(f"ğŸ’¡ ä¸‹è¡¨æ˜¾ç¤ºåº“å­˜è¯¦ç»†ä¿¡æ¯ï¼ˆæœ€å¤šæ˜¾ç¤ºå‰ {TOP_N_DISPLAY} æ¡è®°å½•ï¼ŒæŒ‰å‹è´§å¤©æ•°é™åºæ’åˆ—ï¼‰ã€‚å®Œæ•´æ•°æ®å¯ä¸‹è½½ã€‚")

             # Configure columns for st.data_editor
             dynamic_stock_config = {}
             base_stock_configs = {
                 "äº§å“åç§°": st.column_config.TextColumn("äº§å“åç§°", width="medium", help="äº§å“åç§°"),
                 "äº§å“åˆ†ç±»": st.column_config.TextColumn("åˆ†ç±»", width="small", help="äº§å“æ‰€å±åˆ†ç±»"),
                 "å½“å‰åº“å­˜": st.column_config.NumberColumn("å½“å‰åº“å­˜", format="%d ä¸ª", help="å½“å‰å®é™…åº“å­˜æ•°é‡"),
                 "æœŸé—´æ—¥å‡é”€é‡": st.column_config.NumberColumn("æœŸé—´æ—¥å‡é”€å”®", format="%.2f ä¸ª/å¤©", help="æ‰€é€‰åˆ†æå‘¨æœŸå†…çš„å¹³å‡æ¯æ—¥é”€å”®æ•°é‡"),
                 "é¢„è®¡å¯ç”¨å¤©æ•°": st.column_config.NumberColumn("é¢„è®¡å¯ç”¨å¤©æ•°", help="å½“å‰åº“å­˜é¢„è®¡å¯ç»´æŒå¤©æ•° (9999ä»£è¡¨>9999å¤©æˆ–æ— è¿‘æœŸé”€é‡)", format="%d å¤©"),
                 "å‹è´§æ—¶é—´_å¤©": st.column_config.NumberColumn("å‹è´§å¤©æ•°", help="è‡ªä¸Šæ¬¡å”®å‡ºè‡³ä»Šçš„å¤©æ•° (9999ä»£è¡¨ä»æœªå”®å‡ºæˆ–æ— è®°å½•)", format="%d å¤©"),
                 "æœ€åé”€å”®æ—¥æœŸ": st.column_config.DateColumn("æœ€åé”€å”®æ—¥æœŸ", format="YYYY-MM-DD", help="è¯¥äº§å“æœ€åä¸€æ¬¡æœ‰é”€å”®è®°å½•çš„æ—¥æœŸ"),
                 "å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­": st.column_config.NumberColumn("è·ä¸Šæ¬¡é‡‡è´­", help="è‡ªä¸Šæ¬¡é‡‡è´­è‡³ä»Šçš„å¤©æ•° (9999ä»£è¡¨æ— é‡‡è´­è®°å½•)", format="%d å¤©"),
                 "æœŸé—´é”€å”®é‡": st.column_config.NumberColumn("æœŸé—´é”€å”®é‡", format="%d ä¸ª", help="æ‰€é€‰åˆ†æå‘¨æœŸå†…çš„æ€»é”€å”®æ•°é‡"),
                 "æœ€åé‡‡è´­æ—¥æœŸ": st.column_config.DateColumn("æœ€åé‡‡è´­æ—¥æœŸ", format="YYYY-MM-DD", help="è¯¥äº§å“æœ€åä¸€æ¬¡æœ‰é‡‡è´­è®°å½•çš„æ—¥æœŸ"),
                 "æœ€åé‡‡è´­æ•°é‡": st.column_config.NumberColumn("æœ€åé‡‡è´­æ•°é‡", format="%d ä¸ª", help="æœ€åä¸€æ¬¡é‡‡è´­çš„æ•°é‡"),
                 "é‡‡è´­ä»·": st.column_config.NumberColumn("é‡‡è´­ä»· (â‚¬)", format="%.2f", help="åº“å­˜æ•°æ®ä¸­è®°å½•çš„é‡‡è´­å•ä»·"),
                 # Add other relevant columns if needed
             }

             # Define which columns to show and in what order
             stock_cols_to_show_final = ["äº§å“åç§°"]
             if has_category_data: stock_cols_to_show_final.append("äº§å“åˆ†ç±»")
             stock_cols_to_show_final.extend([
                 "å½“å‰åº“å­˜", "æœŸé—´æ—¥å‡é”€é‡", "é¢„è®¡å¯ç”¨å¤©æ•°", "å‹è´§æ—¶é—´_å¤©",
                 "æœ€åé”€å”®æ—¥æœŸ", "å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­", "æœŸé—´é”€å”®é‡", "æœ€åé‡‡è´­æ—¥æœŸ", "æœ€åé‡‡è´­æ•°é‡", "é‡‡è´­ä»·"
             ])

             # Filter columns to only those present in the dataframe and configure them
             final_stock_cols = []
             for col_name in stock_cols_to_show_final:
                  if col_name in stock_analysis_display_limited.columns:
                       final_stock_cols.append(col_name)
                       if col_name in base_stock_configs:
                           dynamic_stock_config[col_name] = base_stock_configs[col_name]
                       #else: # Add default config for columns not explicitly defined? Optional.
                           #dynamic_stock_config[col_name] = st.column_config.TextColumn(col_name)


             # Display the data editor table
             st.data_editor(
                 stock_analysis_display_limited[final_stock_cols], # Use filtered columns
                 use_container_width=True,
                 num_rows="fixed", # Fixed height based on TOP_N_DISPLAY
                 disabled=True, # Make table read-only
                 key="stock_data_editor",
                 column_config=dynamic_stock_config,
                 hide_index=True
             )
             st.caption(f"æ³¨ï¼šè¡¨æ ¼ä»…æ˜¾ç¤ºæ’åºåçš„å‰ {len(stock_analysis_display_limited)} æ¡è®°å½• (å…± {len(stock_analysis_sorted)} æ¡)ã€‚")

             # --- Download Button for Full Inventory Analysis ---
             try:
                 # Select columns for download (exclude temporary/grouping cols)
                 stock_cols_download = [col for col in stock_analysis_sorted.columns if col not in ['åº“å­˜è´¦é¾„åˆ†ç»„']]
                 # Ensure columns exist
                 stock_cols_download = [col for col in stock_cols_download if col in stock_analysis_sorted.columns]

                 df_to_download_stock = stock_analysis_sorted[stock_cols_download]
                 # Prepare Excel file in memory
                 excel_buffer_stock = io.BytesIO()
                 with pd.ExcelWriter(excel_buffer_stock, engine='openpyxl') as writer:
                     df_to_download_stock.to_excel(writer, index=False, sheet_name='åº“å­˜åˆ†æè¯¦ç»†æ•°æ®')
                 excel_buffer_stock.seek(0) # Rewind buffer

                 # Generate filename
                 download_filename_stock = f"åº“å­˜åˆ†æ_{selected_category}_{start_date_dt.strftime('%Y%m%d')}-{end_date_dt.strftime('%Y%m%d')}.xlsx"

                 # Create download button
                 st.download_button(
                     label=f"ğŸ“¥ ä¸‹è½½å®Œæ•´åº“å­˜åˆ†æè¡¨ ({len(stock_analysis_sorted)}æ¡)",
                     data=excel_buffer_stock,
                     file_name=download_filename_stock,
                     mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                     key="download_stock_analysis"
                 )
             except Exception as e:
                 st.error(f"ç”Ÿæˆåº“å­˜åˆ†æ Excel ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                 traceback.print_exc()

        elif not main_analysis_ready:
             st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ æœ‰æ•ˆçš„ **ç™¾è´§åŸæ•°æ®** æ–‡ä»¶ä»¥æŸ¥çœ‹åº“å­˜åˆ†æã€‚")
             if st.session_state.main_load_error:
                  st.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {st.session_state.main_load_error}")
        else: # main_analysis_ready is True, but stock_analysis is empty or invalid
             st.info("æ— è¯¦ç»†åº“å­˜æ•°æ®å¯ä¾›åˆ†æã€‚è¯·æ£€æŸ¥ä¸Šä¼ çš„â€œç™¾è´§åŸæ•°æ®â€æ–‡ä»¶ä¸­çš„ 'åº“å­˜æ•°æ®' æˆ–ç­›é€‰æ¡ä»¶ã€‚")


    # --- Purchase Suggestions Tab ---
    with tab_purchase:
         st.subheader("æ™ºèƒ½é‡‡è´­å»ºè®®")
         if main_analysis_ready and isinstance(purchase_suggestions, pd.DataFrame) and not purchase_suggestions.empty:
             # Data is ready and suggestions exist
             purchase_suggestions_full = purchase_suggestions # Keep the full dataframe
             purchase_suggestions_display_limited = purchase_suggestions_full.head(TOP_N_DISPLAY) # Limit for display

             st.info(f"ğŸ’¡ ä¸‹è¡¨æ˜¾ç¤ºå»ºè®®é‡‡è´­çš„äº§å“ï¼ˆæœ€å¤šæ˜¾ç¤ºå‰ {TOP_N_DISPLAY} æ¡ï¼ŒæŒ‰å»ºè®®é‡‡è´­é‡é™åºæ’åˆ—ï¼‰ã€‚å®Œæ•´å»ºè®®å¯ä¸‹è½½ã€‚")

             # Configure columns for display
             dynamic_purchase_config = {}
             base_purchase_configs = {
                  "äº§å“åç§°": st.column_config.TextColumn("äº§å“åç§°", width="medium"),
                  "äº§å“åˆ†ç±»": st.column_config.TextColumn("åˆ†ç±»", width="small"),
                  "å½“å‰åº“å­˜": st.column_config.NumberColumn("å½“å‰åº“å­˜", format="%d ä¸ª"),
                  "æœŸé—´æ—¥å‡é”€é‡": st.column_config.NumberColumn("æœŸé—´æ—¥å‡é”€å”®", format="%.2f ä¸ª/å¤©"),
                  "é¢„è®¡å¯ç”¨å¤©æ•°": st.column_config.NumberColumn("å½“å‰å¯ç”¨å¤©æ•°", format="%d å¤©", help="åŸºäºå½“å‰åº“å­˜å’ŒæœŸé—´æ—¥å‡é”€é‡çš„ä¼°ç®—"),
                  "ç›®æ ‡åº“å­˜æ°´å¹³": st.column_config.NumberColumn("ç›®æ ‡åº“å­˜", help=f"ç›®æ ‡({target_days_input}å¤©)+å®‰å…¨({safety_days_input}å¤©)æ‰€éœ€åº“å­˜é‡", format="%.0f ä¸ª"),
                  "å»ºè®®é‡‡è´­é‡": st.column_config.NumberColumn("å»ºè®®é‡‡è´­é‡", format="%d ä¸ª", width="large", help="å»ºè®®è¡¥å……çš„æ•°é‡ (å·²å‘ä¸Šå–æ•´)")
             }

             # Determine columns to show based on what's available in purchase_suggestions_display_limited
             purchase_cols_to_show = []
             for col_name in purchase_suggestions_display_limited.columns:
                  if col_name in base_purchase_configs:
                       purchase_cols_to_show.append(col_name)
                       dynamic_purchase_config[col_name] = base_purchase_configs[col_name]
                  # Add handling for unexpected columns if necessary

             # Display the data editor table
             st.data_editor(
                 purchase_suggestions_display_limited[purchase_cols_to_show],
                 use_container_width=True,
                 num_rows="fixed",
                 disabled=True, # Read-only
                 key="purchase_data_editor",
                 column_config=dynamic_purchase_config,
                 hide_index=True
             )
             st.caption(f"æ³¨ï¼šè¡¨æ ¼ä»…æ˜¾ç¤ºå»ºè®®é‡‡è´­é‡æœ€å¤šçš„å‰ {len(purchase_suggestions_display_limited)} æ¡å»ºè®® (å…± {len(purchase_suggestions_full)} æ¡)ã€‚")

             # --- Download Button for Full Purchase Suggestions ---
             try:
                 df_to_download_purchase = purchase_suggestions_full # Use the full dataframe
                 excel_buffer_purchase = io.BytesIO()
                 with pd.ExcelWriter(excel_buffer_purchase, engine='openpyxl') as writer:
                     df_to_download_purchase.to_excel(writer, index=False, sheet_name='é‡‡è´­å»ºè®®æ¸…å•')
                 excel_buffer_purchase.seek(0)

                 # Generate filename including parameters
                 download_filename_purchase = f"é‡‡è´­å»ºè®®_{selected_category}_{start_date_dt.strftime('%Y%m%d')}-{end_date_dt.strftime('%Y%m%d')}_T{target_days_input}S{safety_days_input}.xlsx"

                 st.download_button(
                     label=f"ğŸ“¤ ä¸‹è½½å®Œæ•´é‡‡è´­å»ºè®® ({len(purchase_suggestions_full)}æ¡)",
                     data=excel_buffer_purchase,
                     file_name=download_filename_purchase,
                     mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                     key="download_purchase_suggestions"
                 )
             except Exception as e:
                 st.error(f"ç”Ÿæˆé‡‡è´­å»ºè®® Excel ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                 traceback.print_exc()

         elif main_analysis_ready and isinstance(purchase_suggestions, pd.DataFrame) and purchase_suggestions.empty:
             # Analysis ran, but no suggestions were generated
             st.success("âœ… æ ¹æ®å½“å‰åº“å­˜å’Œæ‰€é€‰å‚æ•°ï¼Œæš‚æ— äº§å“éœ€è¦ç«‹å³é‡‡è´­ã€‚")
         elif not main_analysis_ready:
             # Main data wasn't loaded successfully
             st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æœ‰æ•ˆçš„ **ç™¾è´§åŸæ•°æ®** æ–‡ä»¶ä»¥ç”Ÿæˆé‡‡è´­å»ºè®®ã€‚")
             if st.session_state.main_load_error:
                  st.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {st.session_state.main_load_error}")
         else:
             # Catchall for other states (e.g., purchase suggestion calculation failed)
             st.warning("æ— æ³•ç”Ÿæˆé‡‡è´­å»ºè®®ã€‚è¯·æ£€æŸ¥åº“å­˜åˆ†ææ•°æ®å’Œå‚æ•°ã€‚")


    # --- Pricing Tool Tab ---
    with tab_pricing:
        st.subheader("ğŸ·ï¸ é”€å”®ä»·æ ¼è®¡ç®—å™¨")
        if pricing_tool_ready and isinstance(pricing_data_loaded, pd.DataFrame):
            # Pricing data loaded successfully
            st.markdown("æ ¹æ®ä¸Šä¼ çš„ **ä»·æ ¼è°ƒæ•´** æ–‡ä»¶ï¼ˆåŒ…å«äº§å“IDã€åç§°ã€é‡‡è´­ä»·ï¼‰è®¡ç®—å»ºè®®é”€å”®ä»·ã€‚")
            st.markdown("---")

            # Margin Input
            desired_margin_percent_calc = st.number_input(
                label="è¯·è¾“å…¥æœŸæœ›çš„ç»Ÿä¸€åˆ©æ¶¦ç‡ (%)",
                min_value=0.0, max_value=99.99, # Margin cannot be 100% or more
                value=35.0, step=0.5, format="%.2f",
                key="margin_calculator_input_pricing",
                help="è¾“å…¥0åˆ°99.99ä¹‹é—´çš„ç™¾åˆ†æ¯”ã€‚åˆ©æ¶¦ç‡ = (é”€å”®ä»· - é‡‡è´­ä»·) / é”€å”®ä»·ã€‚"
            )
            desired_margin_decimal_calc = desired_margin_percent_calc / 100.0

            # Perform calculation if margin is valid
            if desired_margin_decimal_calc < 1.0:
                pricing_df = pricing_data_loaded.copy()
                # Ensure 'é‡‡è´­ä»·' is numeric after loading
                pricing_df['é‡‡è´­ä»·'] = pd.to_numeric(pricing_df['é‡‡è´­ä»·'], errors='coerce')
                # Filter for valid positive cost prices only
                pricing_df_valid = pricing_df[pricing_df['é‡‡è´­ä»·'] > 0].copy()

                if not pricing_df_valid.empty:
                     # Calculate raw suggested price based on margin
                     # Formula: SalesPrice = CostPrice / (1 - Margin)
                     raw_suggested_price = pricing_df_valid['é‡‡è´­ä»·'] / (1 - desired_margin_decimal_calc)

                     # Round UP to 2 decimal places (ceiling)
                     # Multiply by 100, apply ceiling, divide by 100
                     pricing_df_valid['å»ºè®®é”€å”®ä»·'] = (raw_suggested_price * 100).apply(math.ceil) / 100

                     st.markdown(f"##### åŸºäºæœŸæœ›åˆ©æ¶¦ç‡: `{desired_margin_percent_calc:.2f}%` çš„è®¡ç®—ç»“æœ ({len(pricing_df_valid)} æ¡)")

                     # Prepare dataframe for display
                     output_pricing_df = pricing_df_valid[['äº§å“ID', 'äº§å“åç§°', 'é‡‡è´­ä»·', 'å»ºè®®é”€å”®ä»·']].copy()
                     output_pricing_df.columns = ['äº§å“ID', 'äº§å“åç§°', 'é‡‡è´­ä»· (â‚¬)', 'å»ºè®®é”€å”®ä»· (â‚¬)'] # Rename for clarity

                     # Display results using st.dataframe with formatting
                     st.dataframe(
                         output_pricing_df.style.format({
                             'é‡‡è´­ä»· (â‚¬)': '{:.2f}',
                             'å»ºè®®é”€å”®ä»· (â‚¬)': '{:.2f}'
                         }),
                         use_container_width=True,
                         hide_index=True
                     )
                     st.caption(f"è®¡ç®—å…¬å¼: å»ºè®®é”€å”®ä»· = ceiling(é‡‡è´­ä»· / (1 - {desired_margin_decimal_calc:.4f}), 2ä½å°æ•°)")

                     # --- Download Button for Pricing Results ---
                     try:
                         pricing_excel_buffer = io.BytesIO()
                         with pd.ExcelWriter(pricing_excel_buffer, engine='openpyxl') as writer:
                             output_pricing_df.to_excel(writer, index=False, sheet_name='å®šä»·è®¡ç®—ç»“æœ')
                         pricing_excel_buffer.seek(0)

                         # Generate filename including margin and date
                         pricing_download_filename = f"å®šä»·è®¡ç®—_{datetime.now(APP_TIMEZONE).strftime('%Y%m%d')}_{desired_margin_percent_calc:.0f}pct_rounded_up.xlsx"

                         st.download_button(
                             label=f"ğŸ“¥ ä¸‹è½½å®šä»·ç»“æœ ({len(output_pricing_df)}æ¡, å·²å‘ä¸Šå–æ•´)",
                             data=pricing_excel_buffer,
                             file_name=pricing_download_filename,
                             mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                             key="download_pricing_calc"
                         )
                     except Exception as e_down:
                         st.error(f"ç”Ÿæˆå®šä»·ç»“æœ Excel ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e_down}")
                         traceback.print_exc()
                else:
                     # If no rows had valid positive cost price after filtering
                     st.warning("âš ï¸ ä¸Šä¼ çš„å®šä»·æ•°æ®ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ­£é‡‡è´­ä»·ï¼ˆå¤§äº0ï¼‰ï¼Œæ— æ³•è®¡ç®—ã€‚")
            else:
                # Margin input was 100% or more
                st.error("âŒ åˆ©æ¶¦ç‡ä¸èƒ½ä¸º 100% æˆ–æ›´é«˜ï¼Œæ— æ³•è®¡ç®—é”€å”®ä»·ã€‚")

        elif not pricing_tool_ready:
             # Pricing data failed to load or wasn't uploaded
             st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ æœ‰æ•ˆçš„ **ä»·æ ¼è°ƒæ•´** æ–‡ä»¶ (éœ€å« 'äº§å“ID'/'å‹å·', 'äº§å“åç§°'/'å“å', 'é‡‡è´­ä»·' åˆ—) ä»¥ä½¿ç”¨æ­¤å·¥å…·ã€‚")
             # Show specific error if loading failed
             if st.session_state.pricing_load_error:
                 st.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {st.session_state.pricing_load_error}") # Corrected indentation
     # --- NEW: Financial Metrics Tab ---
    with tab_financial:
        st.subheader("ğŸ’° è´¢åŠ¡æŒ‡æ ‡æ¦‚è§ˆ")
        if financial_data_ready and isinstance(financial_data_loaded, pd.DataFrame):
            st.success("è´¢åŠ¡æ•°æ®å·²åŠ è½½ã€‚")
            st.markdown("_(æ­¤å¤„å°†æ˜¾ç¤ºå…³é”®è´¢åŠ¡æŒ‡æ ‡ï¼Œä¾‹å¦‚æ€»æ”¶å…¥ã€æ€»æ”¯å‡ºã€å‡€åˆ©æ¶¦ã€åº”æ”¶/åº”ä»˜è´¦æ¬¾ç­‰)_")
            st.dataframe(financial_data_loaded.head(), use_container_width=True) # Display sample data
            # TODO: Implement actual financial metric calculations and display
        elif uploaded_financial_file and not financial_data_ready:
            st.warning("è´¢åŠ¡æ•°æ®æ–‡ä»¶å·²ä¸Šä¼ ï¼Œä½†åŠ è½½æˆ–å¤„ç†å¤±è´¥ã€‚è¯·æ£€æŸ¥ä¾§è¾¹æ é”™è¯¯ä¿¡æ¯ã€‚")
        else:
            st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼  **è´¢åŠ¡æ•°æ®** æ–‡ä»¶ä»¥æŸ¥çœ‹æ­¤æ¨¡å—ã€‚")

    # --- NEW: CRM Summary Tab ---
    with tab_crm:
        st.subheader("ğŸ‘¥ CRM æ‘˜è¦")
        if crm_data_ready and isinstance(crm_data_loaded, pd.DataFrame):
            st.success("CRM æ•°æ®å·²åŠ è½½ã€‚")
            st.markdown("_(æ­¤å¤„å°†æ˜¾ç¤º CRM ç›¸å…³æ‘˜è¦ï¼Œä¾‹å¦‚æ–°å¢æ½œåœ¨å®¢æˆ·ã€å®¢æˆ·æ´»åŠ¨æ¦‚è§ˆç­‰)_")
            st.dataframe(crm_data_loaded.head(), use_container_width=True) # Display sample data
            # TODO: Implement actual CRM metric calculations and display
        elif uploaded_crm_file and not crm_data_ready:
            st.warning("CRM æ•°æ®æ–‡ä»¶å·²ä¸Šä¼ ï¼Œä½†åŠ è½½æˆ–å¤„ç†å¤±è´¥ã€‚è¯·æ£€æŸ¥ä¾§è¾¹æ é”™è¯¯ä¿¡æ¯ã€‚")
        else:
            st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼  **CRM æ•°æ®** æ–‡ä»¶ä»¥æŸ¥çœ‹æ­¤æ¨¡å—ã€‚")

    # --- NEW: Production Monitoring Tab ---
    with tab_production:
        st.subheader("ğŸ­ ç”Ÿäº§/è¿è¥ç›‘æ§")
        if production_data_ready and isinstance(production_data_loaded, pd.DataFrame):
            st.success("ç”Ÿäº§æ•°æ®å·²åŠ è½½ã€‚")
            st.markdown("_(æ­¤å¤„å°†æ˜¾ç¤ºç”Ÿäº§/è¿è¥ç›¸å…³æŒ‡æ ‡ï¼Œä¾‹å¦‚è®¢å•å®Œæˆç‡ã€è®¾å¤‡åˆ©ç”¨ç‡ã€è´¨é‡æŒ‡æ ‡ç­‰)_")
            st.dataframe(production_data_loaded.head(), use_container_width=True) # Display sample data
            # TODO: Implement actual production metric calculations and display
        elif uploaded_production_file and not production_data_ready:
            st.warning("ç”Ÿäº§æ•°æ®æ–‡ä»¶å·²ä¸Šä¼ ï¼Œä½†åŠ è½½æˆ–å¤„ç†å¤±è´¥ã€‚è¯·æ£€æŸ¥ä¾§è¾¹æ é”™è¯¯ä¿¡æ¯ã€‚")
        else:
            st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼  **ç”Ÿäº§/è¿è¥æ•°æ®** æ–‡ä»¶ä»¥æŸ¥çœ‹æ­¤æ¨¡å—ã€‚")

    # --- NEW: HR Overview Tab ---
    with tab_hr:
        st.subheader("ğŸ§‘â€ğŸ’¼ äººåŠ›èµ„æºæ¦‚è§ˆ")
        if hr_data_ready and isinstance(hr_data_loaded, pd.DataFrame):
            st.success("HR æ•°æ®å·²åŠ è½½ã€‚")
            st.markdown("_(æ­¤å¤„å°†æ˜¾ç¤º HR ç›¸å…³æ¦‚è§ˆï¼Œä¾‹å¦‚å‘˜å·¥æ€»æ•°ã€éƒ¨é—¨åˆ†å¸ƒã€å‡ºå‹¤æ¦‚è§ˆç­‰)_")
            st.dataframe(hr_data_loaded.head(), use_container_width=True) # Display sample data
            # TODO: Implement actual HR metric calculations and display
        elif uploaded_hr_file and not hr_data_ready:
            st.warning("HR æ•°æ®æ–‡ä»¶å·²ä¸Šä¼ ï¼Œä½†åŠ è½½æˆ–å¤„ç†å¤±è´¥ã€‚è¯·æ£€æŸ¥ä¾§è¾¹æ é”™è¯¯ä¿¡æ¯ã€‚")
        else:
            st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼  **äººåŠ›èµ„æºæ•°æ®** æ–‡ä»¶ä»¥æŸ¥çœ‹æ­¤æ¨¡å—ã€‚")

    # --- NEW: Alerts & Tasks Tab ---
    with tab_alerts:
        st.subheader("ğŸ”” å¾…åŠäº‹é¡¹ä¸æé†’")
        st.markdown("_(æ­¤åŒºåŸŸå°†æ•´åˆå…³é”®æé†’ï¼Œä¾‹å¦‚ä½åº“å­˜é¢„è­¦ã€å»ºè®®é‡‡è´­é¡¹ç›®ç­‰)_")
        # Example: Display low stock items (requires modification in calculate_metrics or here)
        if main_analysis_ready and isinstance(stock_analysis, pd.DataFrame) and not stock_analysis.empty and 'é¢„è®¡å¯ç”¨å¤©æ•°' in stock_analysis.columns:
            low_stock_threshold_days = 7 # Example threshold
            low_stock_items = stock_analysis[stock_analysis['é¢„è®¡å¯ç”¨å¤©æ•°'] <= low_stock_threshold_days]
            if not low_stock_items.empty:
                st.warning(f"âš ï¸ **ä½åº“å­˜é¢„è­¦** (é¢„è®¡å¯ç”¨å¤©æ•° <= {low_stock_threshold_days} å¤©):")
                st.dataframe(low_stock_items[['äº§å“åç§°', 'å½“å‰åº“å­˜', 'é¢„è®¡å¯ç”¨å¤©æ•°']].head(10), use_container_width=True, hide_index=True)
            else:
                st.success("âœ… å½“å‰æ— æ˜æ˜¾ä½åº“å­˜é£é™©ã€‚")
        else:
            st.info("éœ€è¦åŠ è½½æœ‰æ•ˆçš„ **ç™¾è´§åŸæ•°æ®** ä»¥ç”Ÿæˆåº“å­˜é¢„è­¦ã€‚")

        # Example: Display purchase suggestions summary
        if main_analysis_ready and isinstance(purchase_suggestions, pd.DataFrame) and not purchase_suggestions.empty:
             st.info(f"ğŸ›’ **é‡‡è´­å»ºè®®æé†’**: {len(purchase_suggestions)} ä¸ªäº§å“å»ºè®®é‡‡è´­ã€‚è¯¦æƒ…è¯·è§ 'é‡‡è´­å»ºè®®' æ ‡ç­¾é¡µã€‚")
        # TODO: Add other potential alerts (e.g., overdue tasks if CRM data is available)

    # --- NEW: Custom Analysis Tab ---
    with tab_custom_analysis:
        st.subheader("ğŸ“ˆ è‡ªå®šä¹‰åˆ†æ (å ä½ç¬¦)")
        st.info("æ­¤åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ã€‚æœªæ¥å°†å…è®¸æ‚¨åŸºäºå·²åŠ è½½çš„æ•°æ®è¿›è¡Œæ›´çµæ´»çš„æ¢ç´¢å’ŒæŠ¥è¡¨ç”Ÿæˆã€‚")
        # Placeholder for future features like:
        # - Selecting data source (Sales, Stock, Finance, etc.)
        # - Choosing columns for grouping/aggregation
        # - Selecting chart types
        # - Saving custom views


# Fallback message if file upload was attempted but processing failed for *both* types (or only one was attempted and failed)
# This might be redundant now with errors shown in sidebar/tabs, but can be a final catch-all. Check all potential uploads.
elif any([(uploaded_main_file and not main_analysis_ready),
          (uploaded_pricing_file and not pricing_tool_ready),
          (uploaded_financial_file and not financial_data_ready), # Check new states
          (uploaded_crm_file and not crm_data_ready),             # Check new states
          (uploaded_production_file and not production_data_ready), # Check new states
          (uploaded_hr_file and not hr_data_ready)]):             # Check new states
    st.error("âŒ éƒ¨åˆ†ä¸Šä¼ çš„æ–‡ä»¶å¤„ç†å¤±è´¥æˆ–åŒ…å«æ— æ•ˆæ•°æ®ã€‚è¯·æ£€æŸ¥ä¾§è¾¹æ çš„é”™è¯¯ä¿¡æ¯ï¼Œå¹¶æ ¹æ®æç¤ºæ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå†…å®¹ã€‚")


# Footer (appears only if any file was uploaded or attempted)
if any([uploaded_main_file, uploaded_pricing_file, uploaded_financial_file, uploaded_crm_file, uploaded_production_file, uploaded_hr_file]): # Ensure footer shows if any file is uploaded
    st.markdown("---")
    try:
        current_year = datetime.now(APP_TIMEZONE).year
    except NameError: # Fallback if APP_TIMEZONE wasn't defined early
        current_year = datetime.utcnow().year
    st.markdown( f"""<div style='text-align: center; font-size: 14px; color: gray;'> TP.STER æ™ºèƒ½æ•°æ®å¹³å° {APP_VERSION} @ {current_year}</div>""", unsafe_allow_html=True)

# --- END OF SCRIPT ---