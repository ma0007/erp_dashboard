# -*- coding: utf-8 -*-
# v2.5.8: Final fixes including pricing ceiling rounding, chart rewrite/cleanup, and welcome screen redesign.
# v2.5.9: Implemented session state to prevent rerun loops on file load errors.
# v2.5.9-mobile-logo-center-v2: Centered logo in sidebar using base64 data URI and HTML/CSS.

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.font_manager import FontProperties # ä¸­æ–‡å­—ä½“
import io
import os
import numpy as np
from datetime import datetime, timedelta
import pytz # Using pytz for timezone handling
import streamlit.components.v1 as components
import traceback # For detailed error logging if needed
import math # Added for ceiling rounding
import base64 # Added for image encoding
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from openai import OpenAI
import requests # Added for API calls
import time # Added for retry logic



# --- Constants ---
TOP_N_DISPLAY = 150 # Max rows to display in tables (performance)
APP_TIMEZONE_STR = 'Europe/Athens' # Define standard timezone for calculations
API_MAX_RETRIES = 3 # Max retries for API calls
API_TIMEOUT = 10 # Timeout in seconds for API calls
LOW_STOCK_THRESHOLD = 10 # Threshold for low stock warning
INVENTORY_API_URL = "https://api.example.com/inventory" # Default inventory API URL

# Use current date for versioning
try:
    # Attempt to get current time in the specified timezone for versioning
    APP_VERSION = f"v2.5.9-{datetime.now(pytz.timezone(APP_TIMEZONE_STR)).strftime('%Y%m%d')}"
except Exception:
    # Fallback if timezone setup fails early
    APP_VERSION = "v2.5.9-unknown_date"


# --- Timezone Setup ---
try:
    APP_TIMEZONE = pytz.timezone(APP_TIMEZONE_STR)
except pytz.exceptions.UnknownTimeZoneError:
    st.warning(f"æ— æ³•è¯†åˆ«çš„æ—¶åŒº '{APP_TIMEZONE_STR}'ï¼Œå°†å›é€€åˆ° UTCã€‚")
    APP_TIMEZONE = pytz.utc # Fallback
    APP_TIMEZONE_STR = 'UTC'

# --- Update version string again now that timezone is confirmed ---
APP_VERSION = f"v2.5.9-{datetime.now(APP_TIMEZONE).strftime('%Y%m%d')}"

# --- Matplotlib ä¸­æ–‡æ˜¾ç¤ºè®¾ç½® ---
# Try to find the font in common locations or the script directory
font_paths_to_check = [
    'SimHei.ttf', # Check local directory first
    '/usr/share/fonts/truetype/simhei/SimHei.ttf', # Linux common path
    '/Library/Fonts/SimHei.ttf', # macOS common path
    'C:/Windows/Fonts/simhei.ttf' # Windows common path
]
font_path = None
for p in font_paths_to_check:
    if os.path.exists(p):
        font_path = p
        break

FONT_AVAILABLE = False
chinese_font = None
if font_path:
    try:
        chinese_font = FontProperties(fname=font_path, size=10)
        FONT_AVAILABLE = True
        print(f"æˆåŠŸåŠ è½½å­—ä½“: {font_path}") # Log success
    except Exception as font_err:
        print(f"åŠ è½½å­—ä½“æ–‡ä»¶ '{font_path}' æ—¶å‡ºé”™: {font_err}") # Log error to console
else:
    print(f"è­¦å‘Šï¼šåœ¨å¸¸è§ä½ç½®åŠè„šæœ¬ç›®å½•æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“æ–‡ä»¶ (å¦‚ SimHei.ttf)ã€‚") # Log warning

# Ensure negative signs display correctly even with CJK fonts
plt.rcParams['axes.unicode_minus'] = False

# --- Initialize Session State (For Preventing Rerun Loops) ---
if 'main_load_error' not in st.session_state:
    st.session_state.main_load_error = None
if 'last_main_file_id' not in st.session_state:
    st.session_state.last_main_file_id = None


# ======== Helper Functions ========

# --- Function to encode image to base64 ---
def get_image_as_base64(path):
    """Reads an image file and returns its base64 encoded data URI."""
    try:
        with open(path, "rb") as image_file:
            encoded_string = base64.b64encode(image_file.read()).decode()
        # Basic image type detection from extension
        image_type = os.path.splitext(path)[1].lower().replace('.', '')
        # Add more types if needed, provide fallback
        supported_types = ['png', 'jpg', 'jpeg', 'gif', 'svg', 'webp']
        if image_type not in supported_types:
            image_type = 'png' # Default fallback if extension is unknown/unsupported
        return f"data:image/{image_type};base64,{encoded_string}"
    except FileNotFoundError:
        print(f"Error: Logo file not found at {path}")
        return None
    except Exception as e:
        print(f"Error encoding image {path}: {e}")
        return None

# --- AI Response Generation Function ---
@st.cache_data(ttl=timedelta(minutes=10), show_spinner=False)
def generate_ai_response(prompt: str, analysis_data: dict) -> str:
    """ç”ŸæˆAIåˆ†æå›å¤(ä»…å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸é‡å¤å±•ç¤ºå®Œæ•´åˆ†æ)"""
    # è·å–ç”¨æˆ·é€‰æ‹©çš„API
    api_provider = st.session_state.get("api_provider", "deepseek")
    
    # é¢„åŠ è½½å®¢æˆ·ç«¯å‡å°‘å»¶è¿Ÿ
    if api_provider == "deepseek":
        from deepseek_api import DeepSeekClient
        try:
            client = DeepSeekClient(api_key=st.session_state.get("deepseek_api_key") or st.secrets.get("DEEPSEEK_API_KEY"))
        except:
            client = DeepSeekClient()  # ä½¿ç”¨å†…ç½®å¯†é’¥
    else:
        client = OpenAI(api_key=st.session_state.get("openai_api_key") or st.secrets["OPENAI_API_KEY"])
    
    # ä¸“ä¸šç³»ç»Ÿæç¤º
    system_prompt = """ä½ æ˜¯å¸Œè…Šé›…å…¸ç™¾è´§åŸä¸“ä¸šæ‰¹å‘å¸‚åœºåˆ†æå¸ˆï¼Œç²¾é€šä¸­æ–‡ã€å¸Œè…Šè¯­ã€è‹±è¯­ã€‚
    ä¸“é•¿é¢†åŸŸ:
    - å¸Œè…ŠåŠæ¬§æ´²æ‰¹å‘å¸‚åœºè¡Œæƒ…
    - é›¶å”®ç™¾è´§è¶‹åŠ¿åˆ†æ
    - èŒä¸šç»ç†äººè§†è§’
    
    å½“å‰åˆ†ææ•°æ®:
    æ´å¯Ÿ: {insights}
    é£é™©: {alerts}
    å»ºè®®: {recommendations}
    
    å·¥ä½œè¦æ±‚:
    1. ä½¿ç”¨æ¬§å…ƒ(â‚¬)è®¡ç®—(æ±‡ç‡:1â‚¬=1.08USD)
    2. æ ¹æ®ç”¨æˆ·è¯­è¨€è‡ªåŠ¨åˆ‡æ¢å“åº”è¯­è¨€
    3. ç»“åˆå¸Œè…Šå¸‚åœºç‰¹ç‚¹åˆ†æ
    4. æä¾›ä¸“ä¸šæ‰¹å‘å¸‚åœºè§è§£
    5. å›ç­”ç®€æ˜æ‰¼è¦"""
    
    messages = [
        {"role": "system", "content": system_prompt.format(
            insights=", ".join(str(i) for i in analysis_data.get("top_insights", [])[:3]),
            alerts=", ".join(str(a) for a in analysis_data.get("risk_alerts", [])[:3]),
            recommendations=", ".join(str(r) for r in analysis_data.get("recommendations", [])[:3])
        )},
        {"role": "user", "content": prompt}
    ]
    
    # ä¼˜åŒ–APIè°ƒç”¨å‚æ•°
    params = {
        "model": "deepseek-chat" if api_provider == "deepseek" else "gpt-3.5-turbo",
        "messages": messages,
        "temperature": 0.3,
        "max_tokens": 800  # å‡å°‘tokenæ•°é‡
    }
    
    # ä»…OpenAIæ”¯æŒstreamå‚æ•°
    if api_provider != "deepseek":
        params["stream"] = False
    
    response = client.chat.completions.create(**params)
    
    return response.choices[0].message.content

# --- load_data å‡½æ•° (For Main Analysis) ---
@st.cache_data(ttl=timedelta(minutes=10))
def load_data(uploaded_file_content, uploaded_file_name):
    """
    Loads data from uploaded file content (Excel or CSV) for main analysis.
    Returns (sales_df, stock_df, purchase_df, error_message)
    On success, error_message is None.
    On failure, dataframes are None and error_message contains the error string.
    """
    try:
        file_ext = os.path.splitext(uploaded_file_name)[1].lower()
        file_buffer = io.BytesIO(uploaded_file_content)
        sales_df, stock_df, purchase_df = None, None, None

        if file_ext == '.csv':
            df = pd.read_csv(file_buffer)
            if 'DataType' not in df.columns: raise ValueError("CSVæ–‡ä»¶å¿…é¡»åŒ…å« 'DataType' åˆ—ä»¥åŒºåˆ† 'è®¢å•æ•°æ®', 'åº“å­˜æ•°æ®', 'é‡‡è´­æ•°æ®'.")
            sales_df = df[df['DataType'] == 'è®¢å•æ•°æ®'].copy()
            stock_df = df[df['DataType'] == 'åº“å­˜æ•°æ®'].copy()
            purchase_df = df[df['DataType'] == 'é‡‡è´­æ•°æ®'].copy()
            if sales_df.empty: raise ValueError("åœ¨CSVä¸­æœªæ‰¾åˆ° DataType ä¸º 'è®¢å•æ•°æ®' çš„æ•°æ®.")
            # Handle potentially empty but present stock/purchase data in CSV
            if stock_df.empty and 'åº“å­˜æ•°æ®' in df['DataType'].unique():
                st.warning("è­¦å‘Šï¼šCSVä¸­DataTypeä¸º'åº“å­˜æ•°æ®'çš„éƒ¨åˆ†ä¸ºç©ºã€‚å°†ä½¿ç”¨ç©ºç»“æ„ã€‚")
                stock_df = pd.DataFrame(columns=["äº§å“ID", "å½“å‰åº“å­˜", "äº§å“åç§°", "é‡‡è´­ä»·", "äº§å“åˆ†ç±»"])
            elif not stock_df.empty and 'åº“å­˜æ•°æ®' not in df['DataType'].unique():
                 # Should not happen if logic above is right, but as safety
                 st.warning("è­¦å‘Šï¼šå‘ç°åº“å­˜æ•°æ®ï¼Œä½†æœªæ ‡è®°ä¸º 'åº“å­˜æ•°æ®' DataTypeã€‚")

            if purchase_df.empty and 'é‡‡è´­æ•°æ®' in df['DataType'].unique():
                st.caption("æ³¨æ„ï¼šCSVä¸­DataTypeä¸º'é‡‡è´­æ•°æ®'çš„éƒ¨åˆ†ä¸ºç©ºã€‚å°†ä½¿ç”¨ç©ºç»“æ„ã€‚")
                purchase_df = pd.DataFrame(columns=["é‡‡è´­æ—¥æœŸ", "äº§å“ID", "é‡‡è´­æ•°é‡", "äº§å“åˆ†ç±»"])
            elif not purchase_df.empty and 'é‡‡è´­æ•°æ®' not in df['DataType'].unique():
                 st.warning("è­¦å‘Šï¼šå‘ç°é‡‡è´­æ•°æ®ï¼Œä½†æœªæ ‡è®°ä¸º 'é‡‡è´­æ•°æ®' DataTypeã€‚")

        elif file_ext in ['.xlsx', '.xls']:
            try:
                xls = pd.ExcelFile(file_buffer)
                required_sheets = ["è®¢å•æ•°æ®", "åº“å­˜æ•°æ®"]
                if not all(sheet in xls.sheet_names for sheet in required_sheets):
                    raise ValueError(f"Excelæ–‡ä»¶å¿…é¡»è‡³å°‘åŒ…å«ä»¥ä¸‹å·¥ä½œè¡¨: {', '.join(required_sheets)}")

                sales_df = pd.read_excel(xls, sheet_name="è®¢å•æ•°æ®")
                stock_df = pd.read_excel(xls, sheet_name="åº“å­˜æ•°æ®")

                if "é‡‡è´­æ•°æ®" in xls.sheet_names:
                    purchase_df = pd.read_excel(xls, sheet_name="é‡‡è´­æ•°æ®")
                    if purchase_df.empty: st.caption("æ³¨æ„ï¼š'é‡‡è´­æ•°æ®' å·¥ä½œè¡¨ä¸ºç©ºã€‚å°†ä½¿ç”¨ç©ºç»“æ„ã€‚")
                else:
                    st.warning("è­¦å‘Šï¼šExcelæ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'é‡‡è´­æ•°æ®' å·¥ä½œè¡¨ã€‚é‡‡è´­ç›¸å…³åˆ†æå°†å—é™ã€‚å°†ä½¿ç”¨ç©ºç»“æ„ã€‚")
                    purchase_df = pd.DataFrame(columns=["é‡‡è´­æ—¥æœŸ", "äº§å“ID", "é‡‡è´­æ•°é‡", "äº§å“åˆ†ç±»"]) # Ensure it exists even if sheet missing

            except Exception as e:
                raise ValueError(f"è¯»å–Excelæ–‡ä»¶ç»“æ„æˆ–å†…å®¹æ—¶å‡ºé”™: {e}")
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ã€‚è¯·ä¸Šä¼  .csv, .xlsx, æˆ– .xls æ–‡ä»¶ã€‚")

        # Ensure dataframes are not None before proceeding
        if sales_df is None: sales_df = pd.DataFrame() # Should have been caught earlier
        if stock_df is None: stock_df = pd.DataFrame(columns=["äº§å“ID", "å½“å‰åº“å­˜", "äº§å“åç§°", "é‡‡è´­ä»·", "äº§å“åˆ†ç±»"])
        if purchase_df is None: purchase_df = pd.DataFrame(columns=["é‡‡è´­æ—¥æœŸ", "äº§å“ID", "é‡‡è´­æ•°é‡", "äº§å“åˆ†ç±»"])


        # --- Date Conversions ---
        date_cols_map = {'sales': (sales_df, 'è®¢å•æ—¥æœŸ'), 'purchase': (purchase_df, 'é‡‡è´­æ—¥æœŸ')}
        for key, (df_loop, date_col) in date_cols_map.items():
             if isinstance(df_loop, pd.DataFrame) and not df_loop.empty and date_col in df_loop.columns:
                 original_count = len(df_loop)
                 # Coerce errors first, then handle NaT if needed, then convert valid to datetime
                 df_loop[date_col] = pd.to_datetime(df_loop[date_col], errors='coerce')
                 failed_count = df_loop[date_col].isna().sum() # Count NaNs/NaTs directly
                 if failed_count > 0:
                     st.warning(f"è­¦å‘Šï¼šåœ¨ '{key}' æ•°æ®çš„ '{date_col}' åˆ—ä¸­å‘ç° {failed_count} ä¸ªæ— æ•ˆæ—¥æœŸæ ¼å¼ï¼Œè¿™äº›å€¼å·²è¢«ç½®ç©ºã€‚åç»­åˆ†æå¯èƒ½å¿½ç•¥è¿™äº›è¡Œã€‚")
                     df_loop.dropna(subset=[date_col], inplace=True) # Drop rows where date conversion failed

        # --- Column Validation ---
        # æ·»åŠ  "é”€å”®é¢" åˆ°å¿…éœ€åˆ—
        required_sales_cols = ["è®¢å•æ—¥æœŸ", "äº§å“ID", "é”€å”®æ•°é‡", "äº§å“åç§°", "é”€å”®é¢"]
        required_stock_cols = ["äº§å“ID", "å½“å‰åº“å­˜", "äº§å“åç§°"] # Keep 'é‡‡è´­ä»·' optional here, validate numeric later
        required_purchase_cols = ["é‡‡è´­æ—¥æœŸ", "äº§å“ID", "é‡‡è´­æ•°é‡"] # Optional 'äº§å“åˆ†ç±»'å’Œ'é”€å”®æ•°é‡'

        if not all(col in sales_df.columns for col in required_sales_cols):
            raise ValueError(f"'è®¢å•æ•°æ®' ç¼ºå°‘å¿…éœ€åˆ—: {', '.join([c for c in required_sales_cols if c not in sales_df.columns])}")
        if not stock_df.empty and not all(col in stock_df.columns for col in required_stock_cols):
             raise ValueError(f"'åº“å­˜æ•°æ®' ç¼ºå°‘å¿…éœ€åˆ—: {', '.join([c for c in required_stock_cols if c not in stock_df.columns])}")
        # Only validate purchase columns if the purchase_df is not empty
        if purchase_df is not None and not purchase_df.empty and not all(col in purchase_df.columns for col in required_purchase_cols):
            raise ValueError(f"'é‡‡è´­æ•°æ®' å·¥ä½œè¡¨å­˜åœ¨ä½†ç¼ºå°‘å¿…éœ€åˆ—: {', '.join([c for c in required_purchase_cols if c not in purchase_df.columns])}")

        # --- Numeric Conversion ---
        num_cols_map = {
            # æ·»åŠ  "é”€å”®é¢" åˆ°éœ€è¦è½¬æ¢çš„æ•°å€¼åˆ—
            'sales': (sales_df, ["é”€å”®æ•°é‡", "é”€å”®é¢"]),
            'stock': (stock_df, ["å½“å‰åº“å­˜", "é‡‡è´­ä»·"]), #é‡‡è´­ä»· optional but convert if present
            'purchase': (purchase_df, ["é‡‡è´­æ•°é‡"])
        }

        return sales_df, stock_df, purchase_df, None
    except Exception as e:
        error_msg = f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}"
        print(error_msg)
        traceback.print_exc()
        return None, None, None, error_msg

# --- åº“å­˜APIæ¥å£åŠŸèƒ½ ---
def get_inventory_data():
    """ä»APIè·å–åº“å­˜æ•°æ®"""
    for attempt in range(API_MAX_RETRIES):
        try:
            response = requests.get(
                INVENTORY_API_URL,
                timeout=API_TIMEOUT,
                headers={'Content-Type': 'application/json'}
            )
            response.raise_for_status()
            
            # éªŒè¯è¿”å›æ•°æ®æ ¼å¼
            data = response.json()
            if not isinstance(data, list):
                raise ValueError("APIè¿”å›æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸ºåˆ—è¡¨")
                
            return pd.DataFrame(data)
            
        except requests.exceptions.RequestException as e:
            if attempt == API_MAX_RETRIES - 1:  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                st.error(f"è·å–åº“å­˜æ•°æ®å¤±è´¥: {str(e)}")
                return pd.DataFrame()
            time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
            
        except ValueError as e:
            st.error(f"æ•°æ®æ ¼å¼é”™è¯¯: {str(e)}")
            return pd.DataFrame()
            
    return pd.DataFrame()  # é»˜è®¤è¿”å›ç©ºDataFrame

def show_inventory_warning():
    """æ˜¾ç¤ºåº“å­˜é¢„è­¦"""
    st.header("ğŸ“Š åº“å­˜é¢„è­¦")
    inventory_df = get_inventory_data()
    if not inventory_df.empty:
        low_stock = inventory_df[inventory_df['quantity'] < LOW_STOCK_THRESHOLD]
        if not low_stock.empty:
            st.warning(f"âš ï¸ æœ‰ {len(low_stock)} ä¸ªå•†å“åº“å­˜ä½äºé˜ˆå€¼ {LOW_STOCK_THRESHOLD}")
            st.dataframe(low_stock[['product_name', 'quantity']])
        else:
            st.success("âœ… æ‰€æœ‰å•†å“åº“å­˜å……è¶³")
    else:
        st.info("â„¹ï¸ æœªè·å–åˆ°åº“å­˜æ•°æ®")

# --- æ•°å€¼è½¬æ¢å¤„ç† ---
def process_numeric_data(num_cols_map):
    """å¤„ç†æ•°å€¼åˆ—è½¬æ¢"""
    for key, (df_loop, cols) in num_cols_map.items():
        if isinstance(df_loop, pd.DataFrame) and not df_loop.empty:
            for col in cols:
                if col in df_loop.columns: # Check if column exists before conversion
                    # Store original dtype to check if conversion actually happens
                    # original_dtype = df_loop[col].dtype
                    initial_na_count = df_loop[col].isna().sum()
                    # Force conversion to numeric, coercing errors to NaN
                    df_loop[col] = pd.to_numeric(df_loop[col], errors='coerce')
                    final_na_count = df_loop[col].isna().sum()
                    newly_failed_count = final_na_count - initial_na_count
                    if newly_failed_count > 0:
                        st.warning(f"è­¦å‘Šï¼šåœ¨ '{key}' æ•°æ®çš„ '{col}' åˆ—ä¸­å‘ç° {newly_failed_count} ä¸ªæ— æ³•è§£æçš„éæ•°å€¼ï¼Œå·²æ›¿æ¢ä¸ºç©ºå€¼ (NaN)ã€‚")
                    # Fill NaN resulting from coercion or original NaNs with 0
                    df_loop[col] = df_loop[col].fillna(0)
                    # Attempt integer conversion only if feasible
                    try:
                        # Check if all non-zero values are whole numbers after filling NaNs
                        non_zero_mask = df_loop[col] != 0
                        if not non_zero_mask.any() or (df_loop.loc[non_zero_mask, col] % 1 == 0).all():
                            # Convert to largest integer type to avoid overflow
                            df_loop[col] = df_loop[col].astype(np.int64)
                        # Otherwise, keep as float if there are decimals
                    except Exception:
                        # If int conversion fails for any reason, keep as float
                        pass
                elif col in ["é‡‡è´­ä»·"] and key == 'stock':
                    # If optional 'é‡‡è´­ä»·' is missing in stock, add it as 0 float
                    df_loop[col] = 0.0
                    st.caption(f"æ³¨æ„ï¼šåº“å­˜æ•°æ®ä¸­æœªæ‰¾åˆ° '{col}' åˆ—ï¼Œå°†å‡è®¾å…¶å€¼ä¸º 0ã€‚")

    return num_cols_map
# --- calculate_metrics å‡½æ•° ---
def calculate_metrics(sales_df, stock_df, purchase_df, start_date, end_date, dead_stock_threshold_days=90, dead_stock_sales_threshold=0.1, ai_model_version="deepseek-v3"):
    """Calculates key metrics including dead stock detection and AI analysis.
    Args:
        ai_model_version: AIæ¨¡å‹ç‰ˆæœ¬ï¼Œé»˜è®¤ä¸ºdeepseek-v3
    Args:
        dead_stock_threshold_days: Number of days without sales to consider as dead stock
        dead_stock_sales_threshold: Daily sales threshold below which stock is considered slow-moving
    Returns:
        metrics_results: Dictionary containing calculated metrics
        stock_analysis: DataFrame with detailed stock analysis
        has_category_in_analysis: Boolean indicating if category data exists
        ai_analysis: Dictionary containing AI-generated insights and recommendations
    """
    try:
        # Ensure dates are Timestamps for comparison
        start_ts = pd.Timestamp(start_date)
        end_ts = pd.Timestamp(end_date)
        now_ts = pd.Timestamp.now(tz=APP_TIMEZONE).tz_localize(None)
    except Exception as date_err:
        st.error(f"æ— æ³•è§£ææ—¥æœŸèŒƒå›´: {date_err}")
        return {}, pd.DataFrame(), False # Return empty results

    metrics_results = {}
    stock_analysis = pd.DataFrame()
    has_category_in_analysis = False
    # åˆå§‹åŒ–æ¯›åˆ©ç›¸å…³æŒ‡æ ‡
    metrics_results["total_gross_profit_period"] = 0
    metrics_results["overall_gross_margin_period"] = 0.0

    try:
        # --- Sales Metrics ---
        if not isinstance(sales_df, pd.DataFrame) or 'è®¢å•æ—¥æœŸ' not in sales_df.columns or not pd.api.types.is_datetime64_any_dtype(sales_df['è®¢å•æ—¥æœŸ']):
            st.error("è®¡ç®—æŒ‡æ ‡é”™è¯¯ï¼š'è®¢å•æ•°æ®'æ— æ•ˆæˆ–ç¼ºå°‘æ­£ç¡®çš„'è®¢å•æ—¥æœŸ'åˆ—ã€‚")
            return metrics_results, stock_analysis, has_category_in_analysis # Return empty

        # Filter sales data for the period
        # Ensure end_ts includes the whole day if it doesn't have time component
        end_ts_inclusive = end_ts + pd.Timedelta(days=1) - pd.Timedelta(nanoseconds=1) if end_ts.time() == datetime.min.time() else end_ts
        sales_filtered = sales_df[(sales_df["è®¢å•æ—¥æœŸ"] >= start_ts) & (sales_df["è®¢å•æ—¥æœŸ"] <= end_ts_inclusive)].copy()

        # Calculate total sales quantity
        metrics_results["total_sales_period"] = 0
        if 'é”€å”®æ•°é‡' in sales_filtered.columns:
            # Ensure the column is numeric before summing
            # åŒæ—¶å¤„ç†é”€å”®é¢
            sales_filtered['é”€å”®æ•°é‡_num_calc'] = pd.to_numeric(sales_filtered['é”€å”®æ•°é‡'], errors='coerce').fillna(0)
            sales_filtered['é”€å”®é¢_num_calc'] = pd.to_numeric(sales_filtered['é”€å”®é¢'], errors='coerce').fillna(0)
            metrics_results["total_sales_period"] = int(sales_filtered["é”€å”®æ•°é‡_num_calc"].sum())
            metrics_results["total_revenue_period"] = float(sales_filtered['é”€å”®é¢_num_calc'].sum()) # è®¡ç®—æ€»é”€å”®é¢
        else:
             st.warning("é”€å”®æ•°æ®ç¼ºå°‘ 'é”€å”®æ•°é‡' æˆ– 'é”€å”®é¢' åˆ—ï¼Œæ— æ³•è®¡ç®—å®Œæ•´é”€å”®æŒ‡æ ‡ã€‚")
        # Calculate average daily sales
        num_days_period = max(1, (end_ts - start_ts).days + 1) # Add 1 to include both start and end date
        metrics_results["avg_daily_sales_period"] = round((metrics_results["total_sales_period"] / num_days_period), 1)

        # Calculate active selling days
        metrics_results["active_days_period"] = sales_filtered["è®¢å•æ—¥æœŸ"].nunique() if not sales_filtered.empty else 0

        # Find top selling product
        top_product_period = "æ— "
        top_selling_data = pd.Series(dtype=float)
        if 'äº§å“åç§°' in sales_filtered.columns and 'é”€å”®æ•°é‡_num_calc' in sales_filtered.columns and not sales_filtered.empty:
            try:
                # Group by product name and sum the numeric quantity
                top_selling_data = sales_filtered.groupby("äº§å“åç§°")["é”€å”®æ•°é‡_num_calc"].sum().sort_values(ascending=False)
                if not top_selling_data.empty:
                    top_product_period = str(top_selling_data.index[0]) # Get name of top product
            except Exception as e:
                st.warning(f"è®¡ç®—çƒ­é”€äº§å“æ—¶å‡ºé”™: {e}")
        metrics_results["top_product_period"] = top_product_period
        metrics_results["top_selling_period_chart_data"] = top_selling_data.head(10) # Data for pie chart

        # --- é”€å”®è¶‹åŠ¿ä¸å æ¯”åˆ†æ ---
        # 1. æœˆåº¦è¶‹åŠ¿æ•°æ®
        monthly_trend_data = pd.Series(dtype=float)
        # 2. äº§å“ç±»åˆ«é”€å”®å æ¯”
        category_sales_data = pd.Series(dtype=float)
        # 3. æ¸ é“é”€å”®å æ¯”
        channel_sales_data = pd.Series(dtype=float)
        
        if not sales_filtered.empty and 'è®¢å•æ—¥æœŸ' in sales_filtered.columns and 'é”€å”®æ•°é‡_num_calc' in sales_filtered.columns:
            try:
                # 1. æœˆåº¦è¶‹åŠ¿æ•°æ®
                monthly_trend_data = sales_filtered.groupby(sales_filtered["è®¢å•æ—¥æœŸ"].dt.to_period("M"))['é”€å”®æ•°é‡_num_calc'].sum()
                
                # 2. äº§å“ç±»åˆ«é”€å”®å æ¯” (å¦‚æœå­˜åœ¨ç±»åˆ«æ•°æ®)
                if 'äº§å“åˆ†ç±»' in sales_filtered.columns and not sales_filtered['äº§å“åˆ†ç±»'].isnull().all():
                    category_sales_data = sales_filtered.groupby("äº§å“åˆ†ç±»")['é”€å”®æ•°é‡_num_calc'].sum().sort_values(ascending=False)
                
                # 3. æ¸ é“é”€å”®å æ¯” (å¦‚æœå­˜åœ¨æ¸ é“æ•°æ®)
                if 'é”€å”®æ¸ é“' in sales_filtered.columns and not sales_filtered['é”€å”®æ¸ é“'].isnull().all():
                    channel_sales_data = sales_filtered.groupby("é”€å”®æ¸ é“")['é”€å”®æ•°é‡_num_calc'].sum().sort_values(ascending=False)
                    
            except Exception as e:
                st.warning(f"è®¡ç®—é”€å”®è¶‹åŠ¿ä¸å æ¯”æ—¶å‡ºé”™: {e}")
                
        metrics_results.update({
            "monthly_trend_chart_data": monthly_trend_data,
            "category_sales_distribution": category_sales_data,
            "channel_sales_distribution": channel_sales_data
        })

        # --- Stock Analysis ---
        if not isinstance(stock_df, pd.DataFrame) or stock_df.empty or "äº§å“ID" not in stock_df.columns or "å½“å‰åº“å­˜" not in stock_df.columns:
             st.warning("åº“å­˜æ•°æ®æ— æ•ˆæˆ–ç¼ºå°‘å¿…éœ€åˆ— ('äº§å“ID', 'å½“å‰åº“å­˜')ï¼Œæ— æ³•è¿›è¡Œè¯¦ç»†åº“å­˜åˆ†æã€‚")
             # Define empty dataframe with expected columns for consistency downstream (æ·»åŠ æ¯›åˆ©åˆ—)
             stock_analysis = pd.DataFrame(columns=["äº§å“ID", "äº§å“åç§°", "å½“å‰åº“å­˜", "æœ€åé”€å”®æ—¥æœŸ", "å‹è´§æ—¶é—´_å¤©", "æœ€åé‡‡è´­æ—¥æœŸ", "æœ€åé‡‡è´­æ•°é‡", "å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­", "æœŸé—´é”€å”®é‡", "æœŸé—´é”€å”®é¢", "æœŸé—´é”€å”®æˆæœ¬", "æœŸé—´æ¯›åˆ©", "æ¯›åˆ©ç‡", "æœŸé—´æ—¥å‡é”€é‡", "é¢„è®¡å¯ç”¨å¤©æ•°", "äº§å“åˆ†ç±»"])
             metrics_results["total_stock_units"] = 0
             return metrics_results, stock_analysis, False # Return empty results but defined structure

        # Calculate total stock units
        stock_df['å½“å‰åº“å­˜_num_calc'] = pd.to_numeric(stock_df['å½“å‰åº“å­˜'], errors='coerce').fillna(0)
        metrics_results["total_stock_units"] = int(stock_df['å½“å‰åº“å­˜_num_calc'].sum())

        # Check if category data exists and is usable
        has_category = "äº§å“åˆ†ç±»" in stock_df.columns and not stock_df["äº§å“åˆ†ç±»"].isnull().all()

        # Base stock analysis dataframe from unique product IDs in stock data
        stock_analysis_cols = ["äº§å“ID", "äº§å“åç§°", "å½“å‰åº“å­˜"] # Start with essential cols
        if has_category: stock_analysis_cols.append("äº§å“åˆ†ç±»") # Add category if available
        # Ensure only columns that actually exist in stock_df are selected
        stock_analysis_cols_present = [col for col in stock_analysis_cols if col in stock_df.columns]
        # Ensure 'é‡‡è´­ä»·' is included if available, as it's needed for margin calculation
        if 'é‡‡è´­ä»·' in stock_df.columns:
            stock_analysis_cols_present.append('é‡‡è´­ä»·')
        else:
            # å¦‚æœåº“å­˜æ•°æ®ä¸­æ²¡æœ‰é‡‡è´­ä»·ï¼Œåˆ™æ— æ³•è®¡ç®—æ¯›åˆ©
            st.warning("è­¦å‘Šï¼šåº“å­˜æ•°æ®ç¼ºå°‘ 'é‡‡è´­ä»·' åˆ—ï¼Œæ— æ³•è®¡ç®—æ¯›åˆ©ç‡ã€‚")
            # æ·»åŠ ä¸€ä¸ªç©ºçš„é‡‡è´­ä»·åˆ—ä»¥ä¾¿åç»­æµç¨‹ä¸æŠ¥é”™ï¼Œä½†æ¯›åˆ©ä¼šæ˜¯ NaN
            stock_df['é‡‡è´­ä»·'] = np.nan
            if 'é‡‡è´­ä»·' not in stock_analysis_cols_present:
                stock_analysis_cols_present.append('é‡‡è´­ä»·')
        # Drop duplicates based on Product ID, keeping the first occurrence
        stock_analysis_base = stock_df[stock_analysis_cols_present].drop_duplicates(subset=["äº§å“ID"], keep='first').copy()


        # --- Merge Last Sale Date ---
        last_sale_overall = pd.Series(dtype='datetime64[ns]')
        if isinstance(sales_df, pd.DataFrame) and not sales_df.empty and "äº§å“ID" in sales_df.columns and "è®¢å•æ—¥æœŸ" in sales_df.columns:
            try:
                # Use only valid sales records (where date is not NaT)
                valid_sales = sales_df.dropna(subset=['è®¢å•æ—¥æœŸ', 'äº§å“ID'])
                if not valid_sales.empty:
                    # Find the index of the latest sale date for each product ID
                    last_sale_idx = valid_sales.groupby("äº§å“ID")["è®¢å•æ—¥æœŸ"].idxmax()
                    # Create a mapping Series: Product ID -> Last Sale Date
                    last_sale_overall = valid_sales.loc[last_sale_idx].set_index('äº§å“ID')['è®¢å•æ—¥æœŸ'].rename("æœ€åé”€å”®æ—¥æœŸ")
            except Exception as e:
                st.warning(f"èšåˆæœ€åé”€å”®æ—¥æœŸæ—¶å‡ºé”™: {e}")

        # Merge last sale date into the stock analysis table
        stock_analysis = stock_analysis_base.merge(last_sale_overall, on="äº§å“ID", how="left")


        # --- Calculate Stock Aging (Days Since Last Sale) ---
        now_ts_aware = pd.Timestamp.now(tz=APP_TIMEZONE)
        now_ts_naive = now_ts_aware.tz_localize(None) # Use naive timestamp for calculations with naive dates from files

        if 'æœ€åé”€å”®æ—¥æœŸ' in stock_analysis.columns:
             # Ensure the merged date is datetime, handle potential errors post-merge
             stock_analysis["æœ€åé”€å”®æ—¥æœŸ"] = pd.to_datetime(stock_analysis["æœ€åé”€å”®æ—¥æœŸ"], errors='coerce').dt.tz_localize(None) # Make naive
             # Calculate difference only where last sale date is valid
             valid_last_sale_mask = stock_analysis["æœ€åé”€å”®æ—¥æœŸ"].notna()
             stock_analysis.loc[valid_last_sale_mask, "å‹è´§æ—¶é—´_å¤©"] = (now_ts_naive - stock_analysis.loc[valid_last_sale_mask, "æœ€åé”€å”®æ—¥æœŸ"]).dt.days
        else:
            # If column doesn't exist after merge (shouldn't happen if logic above is right)
            stock_analysis["å‹è´§æ—¶é—´_å¤©"] = np.nan

        # Fill NaN (never sold or error) with 9999, convert to int, clip max value
        stock_analysis["å‹è´§æ—¶é—´_å¤©"] = stock_analysis["å‹è´§æ—¶é—´_å¤©"].fillna(9999).astype(int).clip(upper=9999)
        
        # è®¡ç®—æ€»é”€é‡: æŒ‰äº§å“IDèšåˆé”€å”®æ•°æ®
        if not sales_df.empty:
            sales_by_product = sales_df.groupby("äº§å“ID")["é”€å”®æ•°é‡"].sum().reset_index()
            sales_by_product.rename(columns={"é”€å”®æ•°é‡": "æ€»é”€é‡"}, inplace=True)
            stock_analysis = stock_analysis.merge(
                sales_by_product,
                on="äº§å“ID",
                how="left"
            )
            stock_analysis["æ€»é”€é‡"] = stock_analysis["æ€»é”€é‡"].fillna(0)
        else:
            stock_analysis["æ€»é”€é‡"] = 0
            
        # è®¡ç®—æœŸé—´æ—¥å‡é”€é‡ = æ€»é”€é‡ / åˆ†æå¤©æ•°
        analysis_days = (end_ts - start_ts).days + 1  # +1 to include both start and end dates
        stock_analysis["æœŸé—´æ—¥å‡é”€é‡"] = stock_analysis["æ€»é”€é‡"] / analysis_days
        stock_analysis["æœŸé—´æ—¥å‡é”€é‡"] = stock_analysis["æœŸé—´æ—¥å‡é”€é‡"].fillna(0)
        
        # --- åŠ¨é”€åˆ†æ ---
        # æ˜¯å¦åŠ¨é”€ (90å¤©å†…æ˜¯å¦æœ‰é”€å”®)
        stock_analysis["æ˜¯å¦åŠ¨é”€"] = stock_analysis["å‹è´§æ—¶é—´_å¤©"] <= dead_stock_threshold_days
        
        # æ»é”€ç­‰çº§ (é«˜/ä¸­/ä½)
        conditions = [
            (stock_analysis["å‹è´§æ—¶é—´_å¤©"] > dead_stock_threshold_days) &
            (stock_analysis["æœŸé—´æ—¥å‡é”€é‡"] < dead_stock_sales_threshold),
            (stock_analysis["å‹è´§æ—¶é—´_å¤©"] > dead_stock_threshold_days//2) &
            (stock_analysis["æœŸé—´æ—¥å‡é”€é‡"] < dead_stock_sales_threshold*2),
            True  # é»˜è®¤å€¼
        ]
        choices = ["é«˜", "ä¸­", "ä½"]
        stock_analysis["æ»é”€ç­‰çº§"] = np.select(conditions, choices, default="ä½")


        # --- Merge Last Purchase Info ---
        # Initialize columns to default values
        stock_analysis["æœ€åé‡‡è´­æ—¥æœŸ"] = pd.NaT
        stock_analysis["æœ€åé‡‡è´­æ•°é‡"] = 0
        stock_analysis["å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­"] = 9999

        if isinstance(purchase_df, pd.DataFrame) and not purchase_df.empty and all(col in purchase_df.columns for col in ["äº§å“ID", "é‡‡è´­æ—¥æœŸ", "é‡‡è´­æ•°é‡"]):
            # Ensure purchase date is datetime and drop invalid rows
            purchase_df['é‡‡è´­æ—¥æœŸ'] = pd.to_datetime(purchase_df['é‡‡è´­æ—¥æœŸ'], errors='coerce')
            purchase_df_valid = purchase_df.dropna(subset=['é‡‡è´­æ—¥æœŸ', 'äº§å“ID', 'é‡‡è´­æ•°é‡'])

            if not purchase_df_valid.empty:
                 try:
                      # Ensure purchase quantity is numeric for potential use later (though not directly used for mapping date)
                      purchase_df_valid['é‡‡è´­æ•°é‡'] = pd.to_numeric(purchase_df_valid['é‡‡è´­æ•°é‡'], errors='coerce').fillna(0)
                      # Find index of the latest purchase date per product ID
                      last_purchase_idx = purchase_df_valid.groupby('äº§å“ID')['é‡‡è´­æ—¥æœŸ'].idxmax()
                      # Create mapping dataframe: Product ID -> Last Purchase Date, Last Purchase Qty
                      last_purchase_map = purchase_df_valid.loc[last_purchase_idx].set_index('äº§å“ID')

                      # Map last purchase date and quantity to stock analysis table
                      stock_analysis['æœ€åé‡‡è´­æ—¥æœŸ'] = stock_analysis['äº§å“ID'].map(last_purchase_map['é‡‡è´­æ—¥æœŸ'])
                      stock_analysis['æœ€åé‡‡è´­æ•°é‡'] = stock_analysis['äº§å“ID'].map(last_purchase_map['é‡‡è´­æ•°é‡'])

                      # Calculate days since last purchase
                      if 'æœ€åé‡‡è´­æ—¥æœŸ' in stock_analysis.columns:
                          # Ensure date is datetime and naive
                          stock_analysis["æœ€åé‡‡è´­æ—¥æœŸ"] = pd.to_datetime(stock_analysis["æœ€åé‡‡è´­æ—¥æœŸ"], errors='coerce').dt.tz_localize(None)
                          valid_purchase_dates_mask = stock_analysis['æœ€åé‡‡è´­æ—¥æœŸ'].notna()
                          stock_analysis.loc[valid_purchase_dates_mask, "å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­"] = (now_ts_naive - stock_analysis.loc[valid_purchase_dates_mask, "æœ€åé‡‡è´­æ—¥æœŸ"]).dt.days

                      # Fill NaN (no purchase record or error) with 9999, convert to int, clip max
                      stock_analysis["å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­"] = stock_analysis["å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­"].fillna(9999).astype(int).clip(upper=9999)

                 except Exception as merge_err:
                      st.warning(f"åˆå¹¶é‡‡è´­æ•°æ®æ—¶å‡ºé”™: {merge_err}")
            else:
                 st.caption("æ— æœ‰æ•ˆçš„é‡‡è´­è®°å½•è¡Œå¯ä¾›åˆå¹¶ã€‚")


        # --- Calculate Sales & Revenue Within Period ---
        stock_analysis["æœŸé—´é”€å”®é‡"] = 0 # Initialize
        stock_analysis["æœŸé—´é”€å”®é¢"] = 0.0 # Initialize
        qty_col_sales_filtered = 'é”€å”®æ•°é‡_num_calc'
        revenue_col_sales_filtered = 'é”€å”®é¢_num_calc'

        if not sales_filtered.empty and "äº§å“ID" in sales_filtered.columns and qty_col_sales_filtered in sales_filtered.columns and revenue_col_sales_filtered in sales_filtered.columns:
             try:
                 # Aggregate sales quantity and revenue within the filtered period by product ID
                 sales_in_period_agg = sales_filtered.groupby("äº§å“ID").agg(
                     æœŸé—´é”€å”®é‡_agg=(qty_col_sales_filtered, 'sum'),
                     æœŸé—´é”€å”®é¢_agg=(revenue_col_sales_filtered, 'sum')
                 )
                 # Map the aggregated sales to the stock analysis table
                 if "äº§å“ID" in stock_analysis.columns:
                     stock_analysis['æœŸé—´é”€å”®é‡'] = stock_analysis['äº§å“ID'].map(sales_in_period_agg['æœŸé—´é”€å”®é‡_agg'])
                     stock_analysis['æœŸé—´é”€å”®é¢'] = stock_analysis['äº§å“ID'].map(sales_in_period_agg['æœŸé—´é”€å”®é¢_agg'])
                     # Fill products with no sales in the period with 0
                     stock_analysis["æœŸé—´é”€å”®é‡"] = stock_analysis["æœŸé—´é”€å”®é‡"].fillna(0).astype(int)
                     stock_analysis["æœŸé—´é”€å”®é¢"] = stock_analysis["æœŸé—´é”€å”®é¢"].fillna(0.0).astype(float)
                 else:
                     st.warning("åº“å­˜åˆ†æç¼ºå°‘'äº§å“ID'åˆ—ï¼Œæ— æ³•åˆå¹¶æœŸé—´é”€å”®æ•°æ®ã€‚")
             except Exception as e:
                 st.error(f"è®¡ç®—æˆ–åˆå¹¶æœŸé—´é”€å”®æ•°æ®é”™è¯¯: {e}")
        # --- Calculate Average Daily Sales (Period) ---
        if "æœŸé—´é”€å”®é‡" in stock_analysis.columns:
             stock_analysis["æœŸé—´æ—¥å‡é”€é‡"] = (stock_analysis["æœŸé—´é”€å”®é‡"] / num_days_period).round(2)
        else:
             stock_analysis["æœŸé—´æ—¥å‡é”€é‡"] = 0.0 # Default if calculation failed


        # --- Calculate Gross Margin ---
        stock_analysis['æœŸé—´é”€å”®æˆæœ¬'] = 0.0
        stock_analysis['æœŸé—´æ¯›åˆ©'] = 0.0
        stock_analysis['æ¯›åˆ©ç‡'] = 0.0

        if 'æœŸé—´é”€å”®é‡' in stock_analysis.columns and 'é‡‡è´­ä»·' in stock_analysis.columns:
            # Ensure 'é‡‡è´­ä»·' is numeric before calculation
            stock_analysis['é‡‡è´­ä»·_num_calc'] = pd.to_numeric(stock_analysis['é‡‡è´­ä»·'], errors='coerce').fillna(0)
            stock_analysis['æœŸé—´é”€å”®æˆæœ¬'] = stock_analysis['æœŸé—´é”€å”®é‡'] * stock_analysis['é‡‡è´­ä»·_num_calc']
            stock_analysis['æœŸé—´é”€å”®æˆæœ¬'] = stock_analysis['æœŸé—´é”€å”®æˆæœ¬'].fillna(0.0) # Ensure no NaNs

            if 'æœŸé—´é”€å”®é¢' in stock_analysis.columns:
                stock_analysis['æœŸé—´æ¯›åˆ©'] = stock_analysis['æœŸé—´é”€å”®é¢'] - stock_analysis['æœŸé—´é”€å”®æˆæœ¬']
                stock_analysis['æœŸé—´æ¯›åˆ©'] = stock_analysis['æœŸé—´æ¯›åˆ©'].fillna(0.0) # Ensure no NaNs

                # Calculate margin rate, handle division by zero
                stock_analysis['æ¯›åˆ©ç‡'] = np.where(
                    stock_analysis['æœŸé—´é”€å”®é¢'] != 0, # Condition: sales amount is not zero
                    stock_analysis['æœŸé—´æ¯›åˆ©'] / stock_analysis['æœŸé—´é”€å”®é¢'], # Value if true
                    0.0 # Value if false (sales amount is zero)
                )
                stock_analysis['æ¯›åˆ©ç‡'] = stock_analysis['æ¯›åˆ©ç‡'].fillna(0.0) # Ensure no NaNs from other sources
            else:
                st.warning("æ— æ³•è®¡ç®—æ¯›åˆ©å’Œæ¯›åˆ©ç‡ï¼Œç¼ºå°‘ 'æœŸé—´é”€å”®é¢' æ•°æ®ã€‚")
        else:
            st.warning("æ— æ³•è®¡ç®—é”€å”®æˆæœ¬å’Œæ¯›åˆ©ï¼Œç¼ºå°‘ 'æœŸé—´é”€å”®é‡' æˆ– 'é‡‡è´­ä»·' æ•°æ®ã€‚")

        # --- Calculate Estimated Stock Days ---
        stock_analysis['é¢„è®¡å¯ç”¨å¤©æ•°'] = 9999 # Initialize with default (infinite/unknown)
        if "æœŸé—´æ—¥å‡é”€é‡" in stock_analysis.columns and "å½“å‰åº“å­˜" in stock_analysis.columns:
             # Use the numeric stock column created earlier
             stock_analysis['å½“å‰åº“å­˜_num_calc'] = pd.to_numeric(stock_analysis['å½“å‰åº“å­˜'], errors='coerce').fillna(0)
             # Calculate only where average daily sales is positive
             mask_positive_sales = stock_analysis['æœŸé—´æ—¥å‡é”€é‡'] > 0
             stock_analysis.loc[mask_positive_sales, 'é¢„è®¡å¯ç”¨å¤©æ•°'] = \
                 stock_analysis.loc[mask_positive_sales, 'å½“å‰åº“å­˜_num_calc'] / stock_analysis.loc[mask_positive_sales, 'æœŸé—´æ—¥å‡é”€é‡']

             # Fill NaN (e.g., from 0 sales) with 9999, round result, convert to int, clip max
             stock_analysis['é¢„è®¡å¯ç”¨å¤©æ•°'] = stock_analysis['é¢„è®¡å¯ç”¨å¤©æ•°'].fillna(9999).round().astype(int).clip(upper=9999)
        # --- Handle Product Category ---
        if has_category and "äº§å“åˆ†ç±»" not in stock_analysis.columns and "äº§å“åˆ†ç±»" in stock_df.columns:
             # If category exists in original stock but not merged (e.g., due to drop_duplicates issue), try re-mapping
             try:
                 # Create map from original stock data (ensure duplicates removed first)
                 category_map = stock_df[['äº§å“ID', 'äº§å“åˆ†ç±»']].drop_duplicates(subset=['äº§å“ID']).set_index('äº§å“ID')['äº§å“åˆ†ç±»']
                 stock_analysis['äº§å“åˆ†ç±»'] = stock_analysis['äº§å“ID'].map(category_map)
                 stock_analysis['äº§å“åˆ†ç±»'] = stock_analysis['äº§å“åˆ†ç±»'].fillna("æœªåˆ†ç±»") # Fill missing categories
             except KeyError:
                 st.warning("é‡æ–°åˆå¹¶äº§å“åˆ†ç±»æ—¶é‡åˆ°KeyErrorï¼Œè·³è¿‡ã€‚")
             except Exception as cat_err:
                 st.warning(f"é‡æ–°åˆå¹¶äº§å“åˆ†ç±»æ—¶å‡ºé”™: {cat_err}")
        elif not has_category and "äº§å“åˆ†ç±»" in stock_analysis.columns:
             # If category column ended up in analysis but wasn't expected, drop it
             stock_analysis = stock_analysis.drop(columns=["äº§å“åˆ†ç±»"])
        elif has_category and "äº§å“åˆ†ç±»" in stock_analysis.columns:
             # If category is present as expected, ensure NaNs are filled
             stock_analysis['äº§å“åˆ†ç±»'] = stock_analysis['äº§å“åˆ†ç±»'].fillna("æœªåˆ†ç±»")

        # Final check if category column ended up in the result dataframe
        has_category_in_analysis = "äº§å“åˆ†ç±»" in stock_analysis.columns if not stock_analysis.empty else False


        # --- Calculate Overall Gross Margin Metrics ---
        if 'æœŸé—´æ¯›åˆ©' in stock_analysis.columns and 'æœŸé—´é”€å”®é¢' in stock_analysis.columns:
            total_gross_profit = stock_analysis['æœŸé—´æ¯›åˆ©'].sum()
            total_revenue = stock_analysis['æœŸé—´é”€å”®é¢'].sum() # Use already calculated sum if available, or sum here
            metrics_results["total_gross_profit_period"] = float(total_gross_profit)
            if total_revenue != 0:
                metrics_results["overall_gross_margin_period"] = float(total_gross_profit / total_revenue)
            else:
                metrics_results["overall_gross_margin_period"] = 0.0
        else:
            # Set defaults if calculation wasn't possible
            metrics_results["total_gross_profit_period"] = 0.0
            metrics_results["overall_gross_margin_period"] = 0.0


        # --- Cleanup Temporary Columns ---
        temp_cols_to_drop = ['é”€å”®æ•°é‡_num_calc', 'é”€å”®é¢_num_calc', 'å½“å‰åº“å­˜_num_calc', 'é‡‡è´­ä»·_num_calc']
        stock_analysis = stock_analysis.drop(columns=[col for col in temp_cols_to_drop if col in stock_analysis.columns], errors='ignore')

        # --- Return results ---
        # --- AI Analysis Section ---
        ai_analysis = {
            "top_insights": [],
            "recommendations": [],
            "risk_alerts": []
        }

        # 1. è¯†åˆ«æ»é”€å•†å“
        dead_stock = stock_analysis[stock_analysis["å‹è´§æ—¶é—´_å¤©"] > dead_stock_threshold_days]
        if not dead_stock.empty:
            ai_analysis["risk_alerts"].append({
                "type": "dead_stock",
                "count": len(dead_stock),
                "total_value": (dead_stock["å½“å‰åº“å­˜"] * dead_stock["é‡‡è´­ä»·"]).sum(),
                "items": dead_stock[["äº§å“ID", "äº§å“åç§°", "å½“å‰åº“å­˜", "å‹è´§æ—¶é—´_å¤©"]].to_dict('records')
            })
            ai_analysis["recommendations"].append("è€ƒè™‘å¯¹{}ç§æ»é”€å•†å“è¿›è¡Œä¿ƒé”€æˆ–æ¸…ä»“å¤„ç†".format(len(dead_stock)))

        # 2. è¯†åˆ«çƒ­é”€å•†å“åº“å­˜ä¸è¶³
        fast_moving = stock_analysis[
            (stock_analysis["æœŸé—´æ—¥å‡é”€é‡"] > dead_stock_sales_threshold * 3) &
            (stock_analysis["é¢„è®¡å¯ç”¨å¤©æ•°"] < 7)
        ]
        if not fast_moving.empty:
            ai_analysis["risk_alerts"].append({
                "type": "low_stock_fast_moving",
                "count": len(fast_moving),
                "items": fast_moving[["äº§å“ID", "äº§å“åç§°", "å½“å‰åº“å­˜", "æœŸé—´æ—¥å‡é”€é‡", "é¢„è®¡å¯ç”¨å¤©æ•°"]].to_dict('records')
            })
            ai_analysis["recommendations"].append("å»ºè®®ä¼˜å…ˆè¡¥è´§{}ç§çƒ­é”€ä¸”åº“å­˜ä¸è¶³çš„å•†å“".format(len(fast_moving)))

        # 3. é”€å”®è¶‹åŠ¿åˆ†æ
        if "monthly_trend_chart_data" in metrics_results and not metrics_results["monthly_trend_chart_data"].empty:
            trend_data = metrics_results["monthly_trend_chart_data"]
            last_month = trend_data.index[-1]
            growth_rate = (trend_data[last_month] - trend_data[last_month-1]) / trend_data[last_month-1] * 100
            ai_analysis["top_insights"].append({
                "type": "sales_trend",
                "current_month": str(last_month),
                "growth_rate": round(growth_rate, 1),
                "trend": "ä¸Šå‡" if growth_rate > 0 else "ä¸‹é™"
            })

        return metrics_results, stock_analysis, has_category_in_analysis, ai_analysis

    except Exception as e:
        st.error(f"åœ¨ calculate_metrics ä¸­å‘ç”Ÿæœªé¢„æ–™çš„é”™è¯¯: {e.__class__.__name__}: {e}")
        return {}, pd.DataFrame(), False, {}  # Return empty results including ai_analysis
        traceback.print_exc() # Log detailed error to console
        # Return empty but defined structures (add margin columns)
        return {}, pd.DataFrame(columns=["äº§å“ID", "äº§å“åç§°", "å½“å‰åº“å­˜", "æœ€åé”€å”®æ—¥æœŸ", "å‹è´§æ—¶é—´_å¤©", "æœ€åé‡‡è´­æ—¥æœŸ", "æœ€åé‡‡è´­æ•°é‡", "å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­", "æœŸé—´é”€å”®é‡", "æœŸé—´é”€å”®é¢", "æœŸé—´é”€å”®æˆæœ¬", "æœŸé—´æ¯›åˆ©", "æ¯›åˆ©ç‡", "æœŸé—´æ—¥å‡é”€é‡", "é¢„è®¡å¯ç”¨å¤©æ•°", "äº§å“åˆ†ç±»"]), False


# --- calculate_purchase_suggestions å‡½æ•° ---
def calculate_purchase_suggestions(stock_analysis_df, target_days, safety_days,
                                   seasonal_factors=None, promotion_plans=None,
                                   supplier_lead_times=None):
    """è®¡ç®—æ™ºèƒ½é‡‡è´­å»ºè®®ï¼Œè€ƒè™‘å¤šç§å› ç´ ï¼š
    - åŸºç¡€åº“å­˜æ°´å¹³
    - å­£èŠ‚æ€§å› ç´  (å¯é€‰)
    - ä¿ƒé”€è®¡åˆ’ (å¯é€‰)
    - ä¾›åº”å•†äº¤è´§å‘¨æœŸ (å¯é€‰)
    """
    if not isinstance(stock_analysis_df, pd.DataFrame) or stock_analysis_df.empty:
        return pd.DataFrame() # Return empty if no input data

    df = stock_analysis_df.copy()
    required_cols = ["æœŸé—´æ—¥å‡é”€é‡", "å½“å‰åº“å­˜", "äº§å“ID", "äº§å“åç§°"] # åŸºç¡€è®¡ç®—å¿…éœ€åˆ—

    # Check if required columns exist
    if not all(col in df.columns for col in required_cols):
        st.warning("åº“å­˜åˆ†ææ•°æ®ç¼ºå°‘è®¡ç®—é‡‡è´­å»ºè®®çš„å¿…è¦åˆ— ('æœŸé—´æ—¥å‡é”€é‡', 'å½“å‰åº“å­˜', 'äº§å“ID', 'äº§å“åç§°')ã€‚")
        return pd.DataFrame() # Return empty

    try:
        # --- Ensure Numeric Types ---
        # Convert relevant columns to numeric, coercing errors and filling NaNs with 0
        df['æœŸé—´æ—¥å‡é”€é‡_num'] = pd.to_numeric(df['æœŸé—´æ—¥å‡é”€é‡'], errors='coerce').fillna(0)
        df['å½“å‰åº“å­˜_num'] = pd.to_numeric(df['å½“å‰åº“å­˜'], errors='coerce').fillna(0)

        # --- è®¡ç®—åŸºç¡€ç›®æ ‡åº“å­˜æ°´å¹³ ---
        df["åŸºç¡€ç›®æ ‡åº“å­˜"] = df["æœŸé—´æ—¥å‡é”€é‡_num"] * (target_days + safety_days)

        # --- åº”ç”¨æ™ºèƒ½è°ƒæ•´å› å­ ---
        # 1. å­£èŠ‚æ€§å› ç´ è°ƒæ•´
        if seasonal_factors and "äº§å“ID" in seasonal_factors.columns and "å­£èŠ‚ç³»æ•°" in seasonal_factors.columns:
            df = df.merge(seasonal_factors[["äº§å“ID", "å­£èŠ‚ç³»æ•°"]], on="äº§å“ID", how="left")
            df["å­£èŠ‚ç³»æ•°"] = df["å­£èŠ‚ç³»æ•°"].fillna(1.0) # é»˜è®¤æ— å­£èŠ‚å½±å“
            df["å­£èŠ‚è°ƒæ•´é”€é‡"] = df["æœŸé—´æ—¥å‡é”€é‡_num"] * df["å­£èŠ‚ç³»æ•°"]
        else:
            df["å­£èŠ‚è°ƒæ•´é”€é‡"] = df["æœŸé—´æ—¥å‡é”€é‡_num"]

        # 2. ä¿ƒé”€è®¡åˆ’è°ƒæ•´
        if promotion_plans and "äº§å“ID" in promotion_plans.columns and "ä¿ƒé”€ç³»æ•°" in promotion_plans.columns:
            df = df.merge(promotion_plans[["äº§å“ID", "ä¿ƒé”€ç³»æ•°"]], on="äº§å“ID", how="left")
            df["ä¿ƒé”€ç³»æ•°"] = df["ä¿ƒé”€ç³»æ•°"].fillna(1.0) # é»˜è®¤æ— ä¿ƒé”€å½±å“
            df["ä¿ƒé”€è°ƒæ•´é”€é‡"] = df["å­£èŠ‚è°ƒæ•´é”€é‡"] * df["ä¿ƒé”€ç³»æ•°"]
        else:
            df["ä¿ƒé”€è°ƒæ•´é”€é‡"] = df["å­£èŠ‚è°ƒæ•´é”€é‡"]

        # 3. ä¾›åº”å•†äº¤è´§å‘¨æœŸè°ƒæ•´
        if supplier_lead_times and "äº§å“ID" in supplier_lead_times.columns and "äº¤è´§å‘¨æœŸ" in supplier_lead_times.columns:
            df = df.merge(supplier_lead_times[["äº§å“ID", "äº¤è´§å‘¨æœŸ"]], on="äº§å“ID", how="left")
            df["äº¤è´§å‘¨æœŸ"] = df["äº¤è´§å‘¨æœŸ"].fillna(7) # é»˜è®¤7å¤©äº¤è´§å‘¨æœŸ
            adjusted_target_days = target_days + df["äº¤è´§å‘¨æœŸ"]
        else:
            adjusted_target_days = target_days

        # --- è®¡ç®—æœ€ç»ˆç›®æ ‡åº“å­˜æ°´å¹³ ---
        df["ç›®æ ‡åº“å­˜æ°´å¹³"] = df["ä¿ƒé”€è°ƒæ•´é”€é‡"] * (adjusted_target_days + safety_days)

        # --- è®¡ç®—åŸå§‹å»ºè®®é‡ ---
        df["å»ºè®®é‡‡è´­é‡_raw"] = df["ç›®æ ‡åº“å­˜æ°´å¹³"] - df["å½“å‰åº“å­˜_num"]

        # --- Final Suggestion (Round Up, Non-Negative Integer) ---
        # Apply math.ceil to round up, ensure it's at least 0, then convert to integer
        df["å»ºè®®é‡‡è´­é‡"] = df["å»ºè®®é‡‡è´­é‡_raw"].apply(
            lambda x: max(0, math.ceil(x)) if pd.notnull(x) and x > -float('inf') else 0 # Handle potential NaN/inf from raw calc
        ).astype(int)

        # --- Filter and Select Display Columns ---
        purchase_suggestions = df[df["å»ºè®®é‡‡è´­é‡"] > 0].copy() # Only show suggestions > 0

        # Define columns to display in the final suggestion table
        display_cols = ["äº§å“åç§°"] # å§‹ç»ˆæ˜¾ç¤ºäº§å“åç§°
        # æ·»åŠ åˆ†ç±»ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "äº§å“åˆ†ç±»" in purchase_suggestions.columns:
            display_cols.append("äº§å“åˆ†ç±»")

        # æ·»åŠ æ ‡å‡†æŒ‡æ ‡åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        standard_cols = ["å½“å‰åº“å­˜", "é¢„è®¡å¯ç”¨å¤©æ•°", "æœŸé—´æ—¥å‡é”€é‡", "å­£èŠ‚è°ƒæ•´é”€é‡",
                        "ä¿ƒé”€è°ƒæ•´é”€é‡", "ç›®æ ‡åº“å­˜æ°´å¹³", "å»ºè®®é‡‡è´­é‡"]
        for col in standard_cols:
            if col in purchase_suggestions.columns:
                display_cols.append(col)

        # Ensure all selected display columns actually exist (safety check)
        final_display_cols = [col for col in display_cols if col in purchase_suggestions.columns]

        # Create the final dataframe with selected columns
        purchase_suggestions_final = purchase_suggestions[final_display_cols]

        # Sort by suggested quantity descending
        purchase_suggestions_final = purchase_suggestions_final.sort_values("å»ºè®®é‡‡è´­é‡", ascending=False)

        return purchase_suggestions_final

    except Exception as e:
        st.error(f"è®¡ç®—é‡‡è´­å»ºè®®æ—¶å‡ºé”™: {e}")
        traceback.print_exc() # Log detailed error
        return pd.DataFrame() # Return empty on error


# ======== Streamlit App Layout ========
import requests
import io
import json

st.set_page_config(page_title=f"TP.STER æ™ºèƒ½æ•°æ®å¹³å° {APP_VERSION}", layout="wide", page_icon="ğŸ“Š")

# Font Missing Hint
if not FONT_AVAILABLE:
    st.warning("""âš ï¸ **æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“...** å›¾è¡¨ä¸­çš„ä¸­æ–‡æ ‡ç­¾å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºã€‚è¯·å°è¯•å®‰è£… 'SimHei' æˆ–ç±»ä¼¼çš„ä¸­æ–‡å­—ä½“ï¼Œæˆ–å°†å­—ä½“æ–‡ä»¶ (å¦‚ `SimHei.ttf`) æ”¾ç½®äºè„šæœ¬ç›¸åŒç›®å½•ä¸‹ã€‚""", icon="â„¹ï¸")

# --- Sidebar ---
with st.sidebar:
    st.markdown(" ")
    logo_path = "tpster_logo.png" # Ensure your logo file is named this and in the same directory

    # --- MODIFIED LOGO DISPLAY (v2 - Base64) ---
    logo_data_uri = None
    placeholder_needed = True
    placeholder_url = "https://via.placeholder.com/180x80?text=TP.STER+LOGO"
    error_msg_logo = None

    if os.path.exists(logo_path):
        logo_data_uri = get_image_as_base64(logo_path)
        if logo_data_uri:
            placeholder_needed = False
        else:
            # File exists but encoding failed
            error_msg_logo = "Error loading logo file."
            placeholder_needed = True # Fallback to placeholder
    else:
        # File does not exist
        placeholder_needed = True


    image_source = logo_data_uri if not placeholder_needed else placeholder_url

    # Use markdown with centered div and img tag
    st.markdown(
        f"""
        <div style="text-align: center; margin-bottom: 5px;">
            <img src="{image_source}" alt="Logo" width="180" style="display: block; margin-left: auto; margin-right: auto;">
        </div>
        """,
        unsafe_allow_html=True
    )

    # Display caption or error centered below the image
    if placeholder_needed and not error_msg_logo:
        st.markdown("<div style='text-align: center; margin-top: 0px;'><p style='font-size: 0.8em; color: grey;'>Logo Placeholder</p></div>", unsafe_allow_html=True)
    elif error_msg_logo:
         st.markdown(f"<div style='text-align: center; margin-top: 0px;'><p style='font-size: 0.8em; color: red;'>{error_msg_logo}</p></div>", unsafe_allow_html=True)
    # --- END OF MODIFIED LOGO DISPLAY ---

    # Initialize variables
    uploaded_main_file = None
    uploaded_financial_file = None
    uploaded_crm_file = None
    main_analysis_ready = False
    financial_data_ready = False
    crm_data_ready = False

    # AI Configuration Section
    with st.expander("ğŸ¤– AIåŠŸèƒ½é…ç½®"):
        ai_enabled = st.checkbox("å¯ç”¨AIåˆ†æåŠŸèƒ½", value=False)
        if ai_enabled:
            st.info("AIåŠŸèƒ½å°†æä¾›æ™ºèƒ½åˆ†æå»ºè®®")
            ai_model = st.selectbox(
                "é€‰æ‹©AIæ¨¡å‹",
                ["é”€å”®é¢„æµ‹", "åº“å­˜ä¼˜åŒ–", "é‡‡è´­å»ºè®®"],
                index=0
            )
            # å†…ç½®DeepSeek APIå¯†é’¥
            deepseek_key = "sk-28e9f3ab9805477c8996c6919b109e1f"

    st.markdown(" ") # Add space after logo/caption
    st.markdown(f"<div style='text-align: center; font-size: 12px; color: gray;'>ç‰ˆæœ¬: {APP_VERSION}</div>", unsafe_allow_html=True)
    # Example User Info - Replace with actual authentication if needed
    st.markdown("<div style='text-align: center; font-size: 14px; color: gray; margin-bottom: 10px; margin-top: 5px;'>å½“å‰èº«ä»½ï¼š<strong>ç®¡ç†å‘˜</strong> (ç¤ºä¾‹)</div>", unsafe_allow_html=True)

    # --- Real-time Clock Display using HTML/JS Component ---
    html_code = """
    <div style='text-align: center; font-size: 12px; color: gray; margin-top: 8px;'>
        ğŸ‡¬ğŸ‡· å¸Œè…Š: <span id="athens-time">--:--:--</span>
    </div>
    <div style='text-align: center; font-size: 12px; color: gray; margin-top: 2px; margin-bottom: 8px;'>
        ğŸ‡¨ğŸ‡³ åŒ—äº¬: <span id="beijing-time">--:--:--</span>
    </div>

    <script>
    function updateClocks() {
      const now = new Date();
      const options = {
        year: 'numeric', month: '2-digit', day: '2-digit',
        hour: '2-digit', minute: '2-digit', second: '2-digit',
        hour12: false // Use 24-hour format
      };

      try {
        // Athens Time (using Europe/Athens)
        const athensTimeStr = now.toLocaleString('sv-SE', { ...options, timeZone: 'Europe/Athens' }); // sv-SE gives YYYY-MM-DD format
        document.getElementById('athens-time').innerText = athensTimeStr;
      } catch (e) {
        document.getElementById('athens-time').innerText = 'æ—¶é—´åŠ è½½é”™è¯¯';
        console.error("Error getting Athens time:", e);
      }

      try {
        // Beijing Time (using Asia/Shanghai)
        const beijingTimeStr = now.toLocaleString('sv-SE', { ...options, timeZone: 'Asia/Shanghai' }); // sv-SE gives YYYY-MM-DD format
        document.getElementById('beijing-time').innerText = beijingTimeStr;
      } catch (e) {
        document.getElementById('beijing-time').innerText = 'æ—¶é—´åŠ è½½é”™è¯¯';
        console.error("Error getting Beijing time:", e);
      }
    }

    // Initial call to display time immediately
    updateClocks();

    // Update every second
    setInterval(updateClocks, 1000);
    </script>
    """
    components.html(html_code, height=60) # Adjust height as needed

    st.markdown("---")

    # --- File Uploaders ---
    st.markdown("#### ğŸ“‚ ç™¾è´§åŸæ•°æ® (é‡‡è´­å»ºè®®)")
    
    # æ•°æ®æºé€‰æ‹©
    data_source = st.radio(
        "é€‰æ‹©æ•°æ®æ¥æº",
        ["ä¸Šä¼ æ–‡ä»¶", "ERPç³»ç»ŸAPI"],
        horizontal=True,
        key="main_data_source"
    )
    
    if data_source == "ä¸Šä¼ æ–‡ä»¶":
        uploaded_main_file = st.file_uploader(
            label="ä¸Šä¼ ä¸»æ•°æ®æ–‡ä»¶ (Excel/CSV)",
            type=["xlsx", "xls", "csv"],
            key="main_data_uploader",
            help="ä¸Šä¼ åŒ…å«é”€å”®(è®¡ç®—éœ€æ±‚)ã€åº“å­˜(æ£€æŸ¥ä½åº“å­˜)åŠå¯é€‰é‡‡è´­æ•°æ®çš„æ–‡ä»¶ã€‚Exceléœ€å«'è®¢å•æ•°æ®', 'åº“å­˜æ•°æ®'è¡¨ã€‚CSVéœ€å«'DataType'åˆ—ã€‚"
        )
    else:
        # ERP API é…ç½®
        with st.expander("ERP API é…ç½®"):
            api_url = st.text_input("APIåœ°å€", "https://erp.example.com/api")
            api_key = st.text_input("APIå¯†é’¥", type="password")
            endpoint = st.selectbox(
                "æ•°æ®ç«¯ç‚¹",
                ["/inventory", "/sales", "/purchase"]
            )
        
        if st.button("ä»ERPç³»ç»Ÿè·å–æ•°æ®"):
            try:
                with st.spinner("æ­£åœ¨ä»ERPç³»ç»Ÿè·å–æ•°æ®..."):
                    # è°ƒç”¨ERP APIè·å–æ•°æ®
                    headers = {"Authorization": f"Bearer {api_key}"}
                    params = {
                        "start_date": start_date_dt.strftime("%Y-%m-%d"),
                        "end_date": end_date_dt.strftime("%Y-%m-%d")
                    }
                    response = requests.get(
                        url=f"{api_url}{endpoint}",
                        headers=headers,
                        params=params,
                        timeout=30
                    )
                    response.raise_for_status()
                    
                    # è½¬æ¢APIå“åº”ä¸ºDataFrame
                    data = response.json()
                    uploaded_main_file = io.StringIO(json.dumps(data))
                    st.success("æ•°æ®è·å–æˆåŠŸ!")
            except requests.exceptions.RequestException as e:
                st.error(f"ä»ERPç³»ç»Ÿè·å–æ•°æ®å¤±è´¥: {str(e)}")
                st.exception(e)
    st.divider()
    # --- NEW: Additional Data Uploaders ---
    st.markdown("#### ğŸ“Š è´¢åŠ¡æ•°æ® (å¯é€‰)")
    
    # è´¢åŠ¡æ•°æ®æºé€‰æ‹©
    fin_data_source = st.radio(
        "é€‰æ‹©è´¢åŠ¡æ•°æ®æ¥æº",
        ["ä¸Šä¼ æ–‡ä»¶", "ERPç³»ç»ŸAPI"],
        horizontal=True,
        key="fin_data_source"
    )
    
    if fin_data_source == "ä¸Šä¼ æ–‡ä»¶":
        uploaded_financial_file = st.file_uploader(
            label="ä¸Šä¼ è´¢åŠ¡æ•°æ®æ–‡ä»¶ (Excel/CSV)",
            type=["xlsx", "xls", "csv"],
            key="financial_data_uploader",
            help="ä¸Šä¼ åŒ…å«æ”¶å…¥ã€æ”¯å‡ºã€åˆ©æ¶¦ç­‰è´¢åŠ¡æŒ‡æ ‡çš„æ–‡ä»¶ã€‚"
        )
    else:
        # ERPè´¢åŠ¡APIé…ç½®
        with st.expander("ERPè´¢åŠ¡APIé…ç½®"):
            fin_api_url = st.text_input("è´¢åŠ¡APIåœ°å€", "https://erp.example.com/finance/api")
            fin_api_key = st.text_input("è´¢åŠ¡APIå¯†é’¥", type="password", key="fin_api_key")
            fin_endpoint = st.selectbox(
                "è´¢åŠ¡æ•°æ®ç«¯ç‚¹",
                ["/balance_sheet", "/income_statement", "/cash_flow"],
                key="fin_endpoint"
            )
        
        if st.button("ä»ERPç³»ç»Ÿè·å–è´¢åŠ¡æ•°æ®"):
            try:
                with st.spinner("æ­£åœ¨ä»ERPç³»ç»Ÿè·å–è´¢åŠ¡æ•°æ®..."):
                    # è°ƒç”¨ERPè´¢åŠ¡APIè·å–æ•°æ®
                    headers = {"Authorization": f"Bearer {fin_api_key}"}
                    params = {"start_date": start_date_dt, "end_date": end_date_dt}
                    response = requests.get(
                        url=f"{fin_api_url}{fin_endpoint}",
                        headers=headers,
                        params=params,
                        timeout=30
                    )
                    response.raise_for_status()
                    
                    # è½¬æ¢APIå“åº”ä¸ºDataFrame
                    data = response.json()
                    uploaded_financial_file = io.StringIO(json.dumps(data))
                    st.success("è´¢åŠ¡æ•°æ®è·å–æˆåŠŸ!")
            except requests.exceptions.RequestException as e:
                st.error(f"ä»ERPç³»ç»Ÿè·å–è´¢åŠ¡æ•°æ®å¤±è´¥: {str(e)}")
                st.exception(e)
    st.divider()

    st.markdown("#### ğŸ‘¥ CRM æ•°æ® (å¯é€‰)")
    uploaded_crm_file = st.file_uploader(
        label="ä¸Šä¼ CRMæ•°æ®æ–‡ä»¶ (Excel/CSV)",
        type=["xlsx", "xls", "csv"],
        key="crm_data_uploader",
        help="ä¸Šä¼ åŒ…å«å®¢æˆ·è”ç³»ä¿¡æ¯ã€è´­ä¹°å†å²ç­‰CRMæ•°æ®çš„æ–‡ä»¶ã€‚"
    )
    st.divider()


    # --- Data Loading and State Management ---
    main_sales_data, main_stock_data, main_purchase_data = None, None, None
    
    # AI Analysis Section - æ›´é†’ç›®çš„å±•ç¤ºä½ç½®
    if ai_enabled and main_analysis_ready:
        st.markdown("---")
        st.markdown("## ğŸš€ AIæ™ºèƒ½åˆ†ææŠ¥å‘Š")
        st.markdown("---")
        with st.spinner("ğŸ¤– æ­£åœ¨ä½¿ç”¨DeepSeek AIåˆ†ææ•°æ®..."):
            try:
                if ai_model == "é”€å”®é¢„æµ‹":
                    # Call DeepSeek API for sales prediction
                    try:
                        headers = {
                            "Authorization": f"Bearer {deepseek_key}",  # ä½¿ç”¨å†…ç½®APIå¯†é’¥
                            "Content-Type": "application/json"
                        }
                        payload = {
                            "model": "deepseek-sales-forecast",
                            "data": main_sales_data.to_dict(orient="records")
                        }
                        response = requests.post(
                            "https://api.deepseek.com/v1/sales/forecast",
                            headers=headers,
                            json=payload,
                            timeout=30
                        )
                        response.raise_for_status()
                        forecast = response.json()
                        st.success("âœ… é”€å”®é¢„æµ‹åˆ†æå®Œæˆ")
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("é¢„è®¡ä¸‹æœˆé”€å”®é¢å¢é•¿",
                                    f"{forecast['growth_rate']}%",
                                    delta=f"{forecast['growth_rate']}%")
                            st.write("ğŸ“ˆ è¶‹åŠ¿åˆ†æ:", forecast['trend_analysis'])
                        
                        with col2:
                            st.write("ğŸ”¥ çƒ­é—¨å•†å“é¢„æµ‹:")
                            for product in forecast['top_products']:
                                st.markdown(f"- {product}")
                        
                        # æ·»åŠ å¯è§†åŒ–å›¾è¡¨
                        st.line_chart(pd.DataFrame({
                            'æœˆä»½': forecast['monthly_trend']['months'],
                            'é”€å”®é¢': forecast['monthly_trend']['values']
                        }).set_index('æœˆä»½'))
                    except Exception as e:
                        st.error(f"é”€å”®é¢„æµ‹å¤±è´¥: {str(e)}")
                elif ai_model == "åº“å­˜ä¼˜åŒ–":
                    # Placeholder for inventory optimization logic
                    st.success("åº“å­˜ä¼˜åŒ–å»ºè®®ç”Ÿæˆ")
                    st.write("å»ºè®®å‡å°‘ä»¥ä¸‹å•†å“åº“å­˜: å•†å“A, å•†å“B")
                elif ai_model == "é‡‡è´­å»ºè®®":
                    # Placeholder for purchase recommendation logic
                    st.success("é‡‡è´­å»ºè®®ç”Ÿæˆ")
                    st.write("å»ºè®®ä¼˜å…ˆé‡‡è´­: å•†å“C, å•†å“D")
            except Exception as e:
                st.error(f"AIåˆ†æå¤±è´¥: {str(e)}")
    main_analysis_ready = False
    financial_data_ready = False # New state
    crm_data_ready = False       # New state
    has_category_column_main = False
    # -- Process Main Data File --
    if uploaded_main_file:
        current_main_file_id = uploaded_main_file.file_id
        # Check if it's the same file that previously caused an error
        if current_main_file_id == st.session_state.last_main_file_id and st.session_state.main_load_error:
            st.error(st.session_state.main_load_error) # Show the stored error
            main_analysis_ready = False
        # Otherwise, try to load (new file or previously successful one)
        elif current_main_file_id != st.session_state.last_main_file_id or not st.session_state.main_load_error:
            st.session_state.last_main_file_id = current_main_file_id # Store current file ID
            with st.spinner("â³ æ­£åœ¨åŠ è½½åˆ†ææ•°æ®..."):
                uploaded_main_content = uploaded_main_file.getvalue()
                # Call modified load_data which returns error message
                sales_df, stock_df, purchase_df, error_msg = load_data(uploaded_main_content, uploaded_main_file.name)

            if error_msg:
                st.error(error_msg) # Show error returned by loading function
                st.session_state.main_load_error = error_msg # Store the error
                main_analysis_ready = False
            elif isinstance(sales_df, pd.DataFrame) and isinstance(stock_df, pd.DataFrame):
                st.session_state.main_load_error = None # Clear any previous error
                main_sales_data, main_stock_data, main_purchase_data = sales_df, stock_df, purchase_df
                main_analysis_ready = True
                # Check for category column after successful load
                if not main_stock_data.empty:
                    has_category_column_main = ("äº§å“åˆ†ç±»" in main_stock_data.columns and not main_stock_data["äº§å“åˆ†ç±»"].isnull().all())
            else:
                 # Handle unexpected case where load_data returned no error but invalid data
                unknown_error = "åŠ è½½åˆ†ææ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é—®é¢˜ï¼Œæœªæ”¶åˆ°æœ‰æ•ˆæ•°æ®ã€‚"
                st.error(unknown_error)
                st.session_state.main_load_error = unknown_error
                main_analysis_ready = False

    # If user clears the file uploader, reset the state
    elif not uploaded_main_file and st.session_state.last_main_file_id is not None:
        st.session_state.last_main_file_id = None
        st.session_state.main_load_error = None
        main_analysis_ready = False # Ensure state is reset
    # --- NEW: Process Additional Data Files (Placeholder Logic) ---
    # Initialize session state for new file types if they don't exist
    if 'financial_load_error' not in st.session_state: st.session_state.financial_load_error = None
    if 'last_financial_file_id' not in st.session_state: st.session_state.last_financial_file_id = None
    if 'crm_load_error' not in st.session_state: st.session_state.crm_load_error = None
    if 'last_crm_file_id' not in st.session_state: st.session_state.last_crm_file_id = None
    if 'production_load_error' not in st.session_state: st.session_state.production_load_error = None
    if 'last_production_file_id' not in st.session_state: st.session_state.last_production_file_id = None
    if 'hr_load_error' not in st.session_state: st.session_state.hr_load_error = None
    if 'last_hr_file_id' not in st.session_state: st.session_state.last_hr_file_id = None

    # Placeholder processing logic for Financial Data
    financial_data_loaded = None
    if uploaded_financial_file:
        current_financial_file_id = uploaded_financial_file.file_id
        if current_financial_file_id == st.session_state.last_financial_file_id and st.session_state.financial_load_error:
            st.error(f"è´¢åŠ¡æ•°æ®åŠ è½½é”™è¯¯: {st.session_state.financial_load_error}")
            financial_data_ready = False
        elif current_financial_file_id != st.session_state.last_financial_file_id or not st.session_state.financial_load_error:
            st.session_state.last_financial_file_id = current_financial_file_id
            try:
                # Placeholder: In a real scenario, call a load_financial_data function here
                financial_data_loaded = pd.read_excel(io.BytesIO(uploaded_financial_file.getvalue())) # Basic load example
                st.success("è´¢åŠ¡æ•°æ®æ–‡ä»¶å·²åŠ è½½ (å ä½ç¬¦)ã€‚")
                st.session_state.financial_load_error = None
                financial_data_ready = True
            except Exception as e:
                error_msg = f"åŠ è½½è´¢åŠ¡æ•°æ®å¤±è´¥: {e}"
                st.error(error_msg)
                st.session_state.financial_load_error = error_msg
                financial_data_ready = False
    elif not uploaded_financial_file and st.session_state.last_financial_file_id is not None:
        st.session_state.last_financial_file_id = None
        st.session_state.financial_load_error = None
        financial_data_ready = False

    # Placeholder processing logic for CRM Data (similar structure)
    crm_data_loaded = None
    if uploaded_crm_file:
        current_crm_file_id = uploaded_crm_file.file_id
        if current_crm_file_id == st.session_state.last_crm_file_id and st.session_state.crm_load_error:
            st.error(f"CRMæ•°æ®åŠ è½½é”™è¯¯: {st.session_state.crm_load_error}")
            crm_data_ready = False
        elif current_crm_file_id != st.session_state.last_crm_file_id or not st.session_state.crm_load_error:
            st.session_state.last_crm_file_id = current_crm_file_id
            try:
                crm_data_loaded = pd.read_excel(io.BytesIO(uploaded_crm_file.getvalue()))
                st.success("CRMæ•°æ®æ–‡ä»¶å·²åŠ è½½ (å ä½ç¬¦)ã€‚")
                st.session_state.crm_load_error = None
                crm_data_ready = True
            except Exception as e:
                error_msg = f"åŠ è½½CRMæ•°æ®å¤±è´¥: {e}"
                st.error(error_msg)
                st.session_state.crm_load_error = error_msg
                crm_data_ready = False
    elif not uploaded_crm_file and st.session_state.last_crm_file_id is not None:
        st.session_state.last_crm_file_id = None
        st.session_state.crm_load_error = None
        crm_data_ready = False

    # --- END NEW: Process Additional Data Files ---

    # --- Analysis Parameters (Only show if main data loaded successfully) ---
    selected_category = "å…¨éƒ¨" # Default value
    try:
        default_end_date = datetime.now(APP_TIMEZONE).date()
    except Exception as e:
        st.warning(f"è·å–å½“å‰æ—¥æœŸæ—¶å‡ºé”™({APP_TIMEZONE_STR}): {e}ï¼Œä½¿ç”¨ UTCã€‚")
        default_end_date = datetime.utcnow().date()
    default_start_date = default_end_date - timedelta(days=89) # Default 90 day period
    date_range = (default_start_date, default_end_date) # Default range tuple
    target_days_input = 30 # Default target days
    safety_days_input = 7 # Default safety days

    if main_analysis_ready:
        with st.expander("âš™ï¸ åˆ†æå‚æ•°è®¾ç½®", expanded=True): # Default expanded

            # Category Filter (Indented)
            if has_category_column_main:
                try:
                    # Get unique categories, convert to string, sort, handle potential NaN/None
                    all_categories = sorted([str(cat) for cat in main_stock_data["äº§å“åˆ†ç±»"].dropna().unique()])
                    options = ["å…¨éƒ¨"] + [cat for cat in all_categories if cat != "å…¨éƒ¨"] # Ensure "å…¨éƒ¨" is first
                    selected_category = st.selectbox(
                        "ğŸ—‚ï¸ äº§å“åˆ†ç±»ç­›é€‰",
                        options=options,
                        index=0, # Default to "å…¨éƒ¨"
                        key="category_select_key"
                    )
                except Exception as cat_err:
                    st.warning(f"åŠ è½½äº§å“åˆ†ç±»é€‰é¡¹æ—¶å‡ºé”™: {cat_err}")
                    selected_category = "å…¨éƒ¨" # Fallback
                    has_category_column_main = False # Disable filtering if error occurs
            else:
                selected_category = "å…¨éƒ¨" # Set default if no category data

            # Date Range Selector (Indented)
            st.markdown("##### ğŸ—“ï¸ é”€å”®åˆ†æå‘¨æœŸ")
            min_date_allowed = None
            max_date_allowed = None
            if main_sales_data is not None and not main_sales_data.empty and 'è®¢å•æ—¥æœŸ' in main_sales_data.columns:
                 # Use already converted and cleaned dates
                 valid_dates = main_sales_data['è®¢å•æ—¥æœŸ'].dropna()
                 if not valid_dates.empty:
                     try:
                          min_date_allowed = valid_dates.min().date()
                          max_date_allowed = valid_dates.max().date()
                     except Exception as date_parse_err:
                          st.warning(f"æ— æ³•è§£æé”€å”®æ•°æ®ä¸­çš„æ—¥æœŸèŒƒå›´: {date_parse_err}")

            # Determine final min/max for the date picker, considering data range and default range
            final_min_date = min(min_date_allowed, default_start_date) if min_date_allowed else default_start_date
            final_max_date = max(max_date_allowed, default_end_date) if max_date_allowed else default_end_date
            # Ensure min is not after max
            if final_min_date > final_max_date: final_min_date = final_max_date

            # Adjust default start/end to be within the allowed range
            actual_default_start = max(final_min_date, default_start_date)
            actual_default_end = min(final_max_date, default_end_date)
            # Ensure start is not after end in default values
            if actual_default_start > actual_default_end: actual_default_start = actual_default_end

            try:
                date_range_input = st.date_input(
                    "é€‰æ‹©å‘¨æœŸ",
                    value=(actual_default_start, actual_default_end), # Use adjusted defaults
                    min_value=final_min_date,
                    max_value=final_max_date,
                    key="date_range_selector",
                    help="é€‰æ‹©ç”¨äºè®¡ç®—æœŸé—´æ—¥å‡é”€é‡ç­‰æŒ‡æ ‡çš„æ—¶é—´èŒƒå›´ã€‚"
                )
                # Validate the input tuple/list
                if isinstance(date_range_input, (tuple, list)) and len(date_range_input) == 2:
                    start_d, end_d = date_range_input
                    if start_d <= end_d:
                        date_range = (start_d, end_d) # Update date_range if valid
                    else:
                        st.warning("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸï¼Œä½¿ç”¨ä¸Šæ¬¡æœ‰æ•ˆæˆ–é»˜è®¤èŒƒå›´ã€‚")
                        # Keep previous date_range value
                else:
                    # Handle cases where date_input might return a single date if max_value = min_value
                     if isinstance(date_range_input, datetime.date):
                         date_range = (date_range_input, date_range_input)
                     else:
                         st.warning("æ—¥æœŸèŒƒå›´é€‰æ‹©æ— æ•ˆï¼Œå°†ä½¿ç”¨ä¸Šæ¬¡æœ‰æ•ˆæˆ–é»˜è®¤èŒƒå›´ã€‚")
                         # Keep previous date_range value
            except Exception as date_err:
                st.warning(f"æ—¥æœŸèŒƒå›´è®¾ç½®æ—¶å‡ºé”™: {date_err}ï¼Œå°†ä½¿ç”¨é»˜è®¤èŒƒå›´ã€‚")
                date_range = (actual_default_start, actual_default_end) # Fallback to defaults on error

            # Inventory/Purchase Parameters (Indented)
            st.markdown("##### âš™ï¸ åº“å­˜ä¸é‡‡è´­å‚æ•°")
            target_days_input = st.number_input(
                "ç›®æ ‡åº“å­˜å¤©æ•°",
                min_value=1, max_value=180, value=30, step=1,
                key="target_days_key",
                help="æœŸæœ›åº“å­˜èƒ½æ»¡è¶³å¤šå°‘å¤©çš„é”€å”®"
            )
            safety_days_input = st.number_input(
                "å®‰å…¨åº“å­˜å¤©æ•°",
                min_value=0, max_value=90, value=7, step=1,
                key="safety_days_key",
                help="é¢å¤–çš„ç¼“å†²å¤©æ•°"
            )

            # Prediction Method Selection
            st.markdown("##### ğŸ“Š é”€é‡é¢„æµ‹æ–¹æ³•")
            predict_method = st.selectbox(
                "é€‰æ‹©é¢„æµ‹ç®—æ³•",
                ["moving_average", "exponential_smoothing"],
                index=1,
                key="predict_method_key",
                help="ç§»åŠ¨å¹³å‡é€‚åˆç¨³å®šäº§å“ï¼ŒæŒ‡æ•°å¹³æ»‘é€‚åˆæœ‰å­£èŠ‚æ€§æˆ–è¶‹åŠ¿çš„äº§å“"
            )

# --- Main Area ---
st.markdown(f"""<div style='text-align: center; padding: 15px 0 10px 0;'><h1 style='margin-bottom: 5px; color: #262730;'>ğŸ“Š TP.STER æ™ºèƒ½æ•°æ®å¹³å° {APP_VERSION}</h1><p style='color: #5C5C5C; font-size: 18px; font-weight: 300; margin-top: 5px;'>æ´å¯Ÿæ•°æ®ä»·å€¼ Â· é©±åŠ¨æ™ºèƒ½å†³ç­– Â· ä¼˜åŒ–ä¾›åº”é“¾ç®¡ç†</p></div>""", unsafe_allow_html=True)
st.divider()

# --- Main Content ---
# Display Welcome Message if no files are uploaded at all (Check all potential uploaders now)
if not any([uploaded_main_file, uploaded_financial_file, uploaded_crm_file]): # Adjusted condition
    # Use the Centered and refined welcome message
    st.markdown(
        f"""
        <div style='text-align: center; max-width: 800px; margin: auto; padding-top: 20px; padding-bottom: 30px;'>

        æœ¬å¹³å°é€šè¿‡é›†æˆåŒ–åˆ†æï¼ŒåŠ©æ‚¨è½»æ¾ç®¡ç†ä¸šåŠ¡æ•°æ®ï¼Œæ ¸å¿ƒåŠŸèƒ½åŒ…æ‹¬ï¼š

        <div style='text-align: left; display: inline-block; margin-top: 15px; margin-bottom: 20px;'>
        <ul style="list-style-type: none; padding-left: 0; font-size: 16px;">
            <li style="margin-bottom: 10px;">ğŸ“Š &nbsp; <strong>é”€å”®åˆ†æ:</strong> è¿½è¸ªé”€å”®è¶‹åŠ¿ï¼Œè¯†åˆ«æ ¸å¿ƒäº§å“ï¼Œåˆ†æå­£èŠ‚æ€§æ³¢åŠ¨</li>
            <li style="margin-bottom: 10px;">ğŸ“¦ &nbsp; <strong>åº“å­˜åˆ†æ:</strong> è¯„ä¼°åº“å­˜å¥åº·åº¦ï¼Œè¯†åˆ«æ»é”€å•†å“ï¼Œä¼˜åŒ–å‘¨è½¬æ•ˆç‡</li>
            <li style="margin-bottom: 10px;">ğŸ›’ &nbsp; <strong>é‡‡è´­å»ºè®®:</strong> åŸºäºAIé¢„æµ‹ï¼Œæä¾›ç²¾å‡†è¡¥è´§å»ºè®®ï¼Œé¿å…æ–­è´§æˆ–ç§¯å‹</li>
            <li style="margin-bottom: 10px;">ğŸ’° &nbsp; <strong>è´¢åŠ¡æŒ‡æ ‡:</strong> è®¡ç®—æ¯›åˆ©ç‡ã€ROIç­‰å…³é”®è´¢åŠ¡æŒ‡æ ‡ï¼Œç›‘æ§ä¸šåŠ¡å¥åº·åº¦</li>
            <li style="margin-bottom: 10px;">ğŸ‘¥ &nbsp; <strong>CRMæ‘˜è¦:</strong> åˆ†æå®¢æˆ·è´­ä¹°è¡Œä¸ºï¼Œè¯†åˆ«é«˜ä»·å€¼å®¢æˆ·ï¼Œä¼˜åŒ–å®¢æˆ·å…³ç³»</li>
        </ul>
        </div>

        <hr style='margin-top: 15px; margin-bottom: 25px; border-top: 1px solid #eee;'>

        #### **å¼€å§‹ä½¿ç”¨**

        <p style="font-size: 16px; margin-bottom: 20px;">è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶ (æ”¯æŒ <code>.xlsx</code>, <code>.xls</code>, <code>.csv</code>):</p>

        <div style='text-align: left; max-width: 600px; margin: auto; background-color: #f8f9fa; padding: 15px; border-radius: 5px; border: 1px solid #e9ecef;'>
        <ol style="padding-left: 25px; margin-bottom: 0;">
            <li style="margin-bottom: 15px;">
                <strong>æ ¸å¿ƒæ•°æ® (ç”¨äºåˆ†æä¸å»ºè®®):</strong><br>
                ä¸Šä¼ åŒ…å« <code>è®¢å•æ•°æ®</code> å’Œ <code>åº“å­˜æ•°æ®</code> çš„æ–‡ä»¶ã€‚<br>
                <span style="font-size: 0.9em; color: #6c757d;"><em>(Excelæ–‡ä»¶éœ€åŒ…å«åä¸º "è®¢å•æ•°æ®" å’Œ "åº“å­˜æ•°æ®" çš„å·¥ä½œè¡¨)</em></span><br>
                <span style="font-size: 0.9em; color: #6c757d;"><em>(CSVæ–‡ä»¶éœ€åŒ…å« 'DataType' åˆ—åŒºåˆ†æ•°æ®ç±»å‹)</em></span>
            </li>
        </ol>
        </div>

        </div>
        """, unsafe_allow_html=True)

# Display content if at least one file was uploaded and processed (or attempted) - Check all potential uploaders
elif any([uploaded_main_file, uploaded_financial_file, uploaded_crm_file]): # Adjusted condition

    metrics = {}
    stock_analysis = pd.DataFrame()
    purchase_suggestions = pd.DataFrame()
    has_category_data = False # Will be set by calculate_metrics
    data_filtered_message = ""

    # Perform main analysis only if data is ready
    if main_analysis_ready:
        st.markdown(f"### ğŸ“ˆ **ä¸»æ•°æ®åˆ†æ** ({date_range[0].strftime('%Y-%m-%d')} åˆ° {date_range[1].strftime('%Y-%m-%d')})")

        # --- Apply Category Filtering if selected ---
        sales_calc = main_sales_data.copy()
        stock_calc = main_stock_data.copy()
        # Ensure purchase_calc is a DataFrame even if main_purchase_data was None initially
        purchase_calc = main_purchase_data.copy() if main_purchase_data is not None else pd.DataFrame()

        if selected_category != "å…¨éƒ¨" and has_category_column_main:
            data_filtered_message = f"åˆ†æå·²ç­›é€‰ï¼Œä»…åŒ…å«åˆ†ç±»ï¼š**{selected_category}**"
            st.info(data_filtered_message) # Show filter info early

            # Filter stock data first
            if "äº§å“åˆ†ç±»" in stock_calc.columns and not stock_calc.empty:
                 category_product_ids = stock_calc[stock_calc["äº§å“åˆ†ç±»"] == selected_category]["äº§å“ID"].unique()
                 stock_calc = stock_calc[stock_calc["äº§å“ID"].isin(category_product_ids)] # Filter stock

                 # Filter sales and purchase based on the product IDs from the filtered stock
                 if not sales_calc.empty and "äº§å“ID" in sales_calc.columns:
                     sales_calc = sales_calc[sales_calc["äº§å“ID"].isin(category_product_ids)]
                 if purchase_calc is not None and not purchase_calc.empty and "äº§å“ID" in purchase_calc.columns:
                     purchase_calc = purchase_calc[purchase_calc["äº§å“ID"].isin(category_product_ids)]
            else:
                 st.warning("åº“å­˜æ•°æ®ä¸­æ— 'äº§å“åˆ†ç±»'åˆ—æˆ–æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æŒ‰åˆ†ç±»ç­›é€‰ã€‚å°†åˆ†æå…¨éƒ¨æ•°æ®ã€‚")
                 data_filtered_message = "" # Clear filter message if filtering failed

        # --- Run Calculations ---
        start_date_dt, end_date_dt = date_range
        try:
            with st.spinner('â³ æ­£åœ¨åˆ†æä¸»æ•°æ®...'):
                 # Pass the potentially filtered dataframes to calculation functions
                 metrics, stock_analysis, has_category_data, ai_analysis = calculate_metrics(
                     sales_calc, stock_calc, purchase_calc, start_date_dt, end_date_dt,
                     ai_model_version="deepseek-v3"
                 )
                 # Ensure AI analysis is always generated
                 if not ai_analysis:
                     ai_analysis = {
                         "top_insights": [],
                         "risk_alerts": [],
                         "recommendations": ["âš ï¸ AIåˆ†ææœªç”Ÿæˆå…·ä½“ç»“æœï¼Œä½†æ•°æ®å·²å¤„ç†å®Œæˆ"]
                     }
                 # Calculate suggestions based on the result of calculate_metrics
                 purchase_suggestions = calculate_purchase_suggestions(
                     stock_analysis, target_days_input, safety_days_input
                 )
        except Exception as calc_error:
            st.error(f"âŒ åœ¨æ‰§è¡Œä¸»æ•°æ®è®¡ç®—æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {calc_error}")
            traceback.print_exc()
            # Reset results to prevent downstream errors
            metrics = {}
            stock_analysis = pd.DataFrame()
            purchase_suggestions = pd.DataFrame()
            # Keep main_analysis_ready as True if initial load was ok, but show error

        # --- Display KPIs (if calculation succeeded) ---
        if main_analysis_ready and metrics: # Check if metrics dictionary was populated
            if data_filtered_message:
                st.markdown(f"**ç­›é€‰åˆ†ç±»:** `{selected_category}`") # Show filter again if applied

            # æ‰©å±• KPI åˆ—æ•°ä»¥å®¹çº³æ¯›åˆ©æŒ‡æ ‡
            kpi_cols = st.columns(7)
            # Use .get() with default values for robustness
            kpi_cols[0].metric("æœŸé—´æ€»é”€é‡", f"{metrics.get('total_sales_period', 0):,} ä¸ª")
            kpi_cols[1].metric("æœŸé—´æ€»é”€å”®é¢", f"â‚¬ {metrics.get('total_revenue_period', 0):,.2f}") # æ–°å¢æ€»é”€å”®é¢
            kpi_cols[2].metric("æœŸé—´æ€»æ¯›åˆ©", f"â‚¬ {metrics.get('total_gross_profit_period', 0):,.2f}", help="æœŸé—´æ€»é”€å”®é¢ - æœŸé—´æ€»é”€å”®æˆæœ¬") # æ–°å¢æ€»æ¯›åˆ©
            kpi_cols[3].metric("æœŸé—´æ¯›åˆ©ç‡", f"{metrics.get('overall_gross_margin_period', 0):.1%}", help="æœŸé—´æ€»æ¯›åˆ© / æœŸé—´æ€»é”€å”®é¢") # æ–°å¢æ¯›åˆ©ç‡

            # Calculate SKU count from the final stock_analysis dataframe
            sku_count = stock_analysis['äº§å“ID'].nunique() if isinstance(stock_analysis, pd.DataFrame) and not stock_analysis.empty else 0
            kpi_cols[4].metric("åˆ†æäº§å“ SKU æ•°", f"{sku_count:,}")

            # Calculate total stock from the final stock_analysis dataframe
            total_stock_kpi = 0
            if isinstance(stock_analysis, pd.DataFrame) and 'å½“å‰åº“å­˜' in stock_analysis.columns:
                 total_stock_kpi = int(pd.to_numeric(stock_analysis['å½“å‰åº“å­˜'], errors='coerce').fillna(0).sum())
            kpi_cols[5].metric("å½“å‰æ€»åº“å­˜", f"{total_stock_kpi:,} ä¸ª")

            kpi_cols[6].metric("æœŸé—´çƒ­é”€äº§å“", metrics.get('top_product_period', 'æ— '))
            st.divider()
        elif main_analysis_ready and not metrics:
             # This case might happen if calculate_metrics itself failed internally but didn't raise exception caught above
             st.warning("ä¸»æ•°æ®åˆ†æè®¡ç®—å®Œæˆï¼Œä½†æœªèƒ½ç”Ÿæˆå…³é”®æŒ‡æ ‡ã€‚è¯·æ£€æŸ¥æ•°æ®æˆ–è®¡ç®—é€»è¾‘ã€‚")
             st.divider()
        # If main_analysis_ready is False (loading failed), KPIs won't show. Error is shown in sidebar.

    # --- Display Tabs ---
    # --- Define Tabs (Including New Modules) ---
    tab_list = [
        "ğŸ“Š é”€å”®åˆ†æ", "ğŸ“¦ åº“å­˜åˆ†æ", "ğŸ›’ é‡‡è´­å»ºè®®",
        "ğŸ¤– AIåˆ†æ", # æ–°å¢AIåˆ†ææ ‡ç­¾
        "ğŸ’° è´¢åŠ¡æŒ‡æ ‡", "ğŸ‘¥ CRMæ‘˜è¦",
        "ğŸ”” å¾…åŠæé†’", "ğŸ“ˆ è‡ªå®šä¹‰åˆ†æ"
    ]
    tabs = st.tabs(tab_list)

    # Assign tabs to variables for clarity
    tab_sales = tabs[0]
    tab_inventory = tabs[1]
    tab_purchase = tabs[2]
    tab_ai = tabs[3]
    tab_financial = tabs[4]
    tab_crm = tabs[5]
    tab_alerts = tabs[6]
    tab_custom_analysis = tabs[7]
    # --- End Define Tabs ---

    # --- AI Analysis Tab ---
    with tab_ai:
        # APIé€‰æ‹©å™¨
        col1, col2 = st.columns([1, 2])
        with col1:
            st.selectbox(
                "é€‰æ‹©AIæœåŠ¡æä¾›å•†",
                ["deepseek", "openai"],
                index=0,
                key="api_provider",
                help="é€‰æ‹©è¦ä½¿ç”¨çš„AI APIæœåŠ¡"
            )
        
        with col2:
            if st.session_state.get("api_provider", "deepseek") == "deepseek":
                st.text_input(
                    "DeepSeek APIå¯†é’¥ (å¯é€‰)",
                    type="password",
                    help="ç•™ç©ºå°†ä½¿ç”¨å†…ç½®å¯†é’¥",
                    key="deepseek_api_key"
                )
            else:
                st.text_input(
                    "OpenAI APIå¯†é’¥",
                    type="password",
                    key="openai_api_key"
                )
        if main_analysis_ready:
            # Initialize ai_analysis if not exists
            if 'ai_analysis' not in st.session_state:
                st.session_state.ai_analysis = None
            
            # AIåˆ†æç»“æœå±•ç¤º
            st.subheader("AIæ•°æ®åˆ†ææ‘˜è¦")
            if ai_analysis:
                # ç›´æ¥å±•ç¤ºå…³é”®æ•°æ®å’Œä¼˜åŒ–å»ºè®®
                st.markdown("### ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡")
                if ai_analysis["top_insights"]:
                    for insight in ai_analysis["top_insights"]:
                        if insight["type"] == "sales_trend":
                            trend_icon = "ğŸ“ˆ" if insight["trend"] == "ä¸Šå‡" else "ğŸ“‰"
                            st.metric(
                                f"{trend_icon} {insight['current_month']}æœˆé”€å”®è¶‹åŠ¿",
                                f"{insight['trend']} {abs(insight['growth_rate'])}%"
                            )
                
                st.markdown("### âš ï¸ é£é™©æ¦‚è§ˆ")
                if ai_analysis["risk_alerts"]:
                    for alert in ai_analysis["risk_alerts"]:
                        if alert["type"] == "dead_stock":
                            st.error(f"æ»é”€å•†å“: {alert['count']}ç§")
                        elif alert["type"] == "low_stock_fast_moving":
                            st.warning(f"ç¼ºè´§é£é™©: {alert['count']}ç§")
                
                # ç›´æ¥å±•ç¤ºä¼˜åŒ–å»ºè®®
                if ai_analysis["recommendations"]:
                    st.markdown("### ğŸ’¡ ä¼˜åŒ–å»ºè®®")
                    for rec in ai_analysis["recommendations"]:
                        st.success(f"âœ… {rec}")
            else:
                st.info("AIåˆ†ææ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®ã€‚")
            
            # ç®€æ´AIå¯¹è¯åŠŸèƒ½
            st.divider()
            st.subheader("AIæ•°æ®åˆ†æåŠ©æ‰‹")
            
            # åˆ›å»ºå®¹å™¨ç”¨äºèŠå¤©è®°å½•
            chat_container = st.container()
            
            # ç”¨æˆ·è¾“å…¥æ¡†å›ºå®šåœ¨é¡¶éƒ¨
            if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨å…³äºæ•°æ®åˆ†æçš„é—®é¢˜...", key="chat_input"):
                st.session_state.ai_messages = st.session_state.get("ai_messages", [])
                st.session_state.ai_messages.append({"role": "user", "content": prompt})
                
                # AIå›å¤
                response = generate_ai_response(prompt, ai_analysis)
                st.session_state.ai_messages.append({"role": "assistant", "content": response})
                
                # é‡æ–°æ¸²æŸ“æ•´ä¸ªèŠå¤©è®°å½•
                st.rerun()
            
            # åœ¨å®¹å™¨ä¸­æ˜¾ç¤ºèŠå¤©è®°å½•
            with chat_container:
                for msg in st.session_state.get("ai_messages", []):
                    with st.chat_message(msg["role"]):
                        st.markdown(msg["content"])
        elif main_analysis_ready and not ai_analysis:
            st.info("AIåˆ†ææ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®ã€‚")
    # --- End Define Tabs ---

    # --- Sales Analysis Tab ---
    with tab_sales:
        if main_analysis_ready and metrics: # Only show content if main analysis ran successfully
             st.subheader("é”€å”®è¶‹åŠ¿ä¸å æ¯”åˆ†æ")
             chart_cols = st.columns([2, 1])

             # --- Monthly Sales Trend (Line Chart) ---
             with chart_cols[0]:
                 st.markdown("###### æœˆåº¦é”€å”®è¶‹åŠ¿ (æŒ‰é”€å”®é‡)")
                 monthly_data = metrics.get('monthly_trend_chart_data')
                 # Check if data is valid Series and contains meaningful data
                 if isinstance(monthly_data, pd.Series) and not monthly_data.empty and not (monthly_data.isnull().all() or (monthly_data == 0).all()):
                     fig_line = None # Initialize figure variable
                     try:
                          # Convert PeriodIndex to Timestamp for plotting
                          plot_index = monthly_data.index.to_timestamp()
                          fig_line, ax_line = plt.subplots(figsize=(8, 3))
                          ax_line.plot(plot_index, monthly_data.values, marker='o', linestyle='-', linewidth=1.5, markersize=4, color='#1f77b4')

                          xlabel, ylabel = "æœˆä»½", "é”€å”®æ•°é‡ (ä¸ª)"
                          # Apply Chinese font if available
                          if FONT_AVAILABLE and chinese_font:
                              ax_line.set_xlabel(xlabel, fontproperties=chinese_font, fontsize=9)
                              ax_line.set_ylabel(ylabel, fontproperties=chinese_font, fontsize=9)
                              # Set font for tick labels
                              for label in ax_line.get_xticklabels() + ax_line.get_yticklabels():
                                   label.set_fontproperties(chinese_font)
                                   label.set_fontsize(8)
                          else:
                              ax_line.set_xlabel(xlabel, fontsize=9)
                              ax_line.set_ylabel(ylabel, fontsize=9)
                              for label in ax_line.get_xticklabels() + ax_line.get_yticklabels():
                                   label.set_fontsize(8)

                          # Format x-axis dates
                          ax_line.xaxis.set_major_formatter(mdates.DateFormatter("%Yå¹´%mæœˆ"))

                          # --- Dynamic Date Locator Interval ---
                          # Calculate the number of months in the selected range
                          start_date_dt, end_date_dt = date_range # Get dates from sidebar selection
                          num_months_in_range = (end_date_dt.year - start_date_dt.year) * 12 + end_date_dt.month - start_date_dt.month + 1

                          # Determine interval based on the number of months in the selected range
                          if num_months_in_range <= 1: # Very short range (e.g., within a month)
                              locator_interval = 1 # Show the month(s) involved
                          elif num_months_in_range <= 12: # Up to a year
                              locator_interval = 1 # Show every month
                          elif num_months_in_range <= 24: # Up to two years
                              locator_interval = 2 # Show every 2 months
                          elif num_months_in_range <= 48: # Up to four years
                              locator_interval = 3 # Show every 3 months
                          else: # Very long range
                              locator_interval = 6 # Show every 6 months

                          # Apply the determined locator interval
                          ax_line.xaxis.set_major_locator(mdates.MonthLocator(interval=locator_interval))
                          # --- End Dynamic Date Locator Interval ---

                          ax_line.grid(axis='y', linestyle=':', alpha=0.7)
                          fig_line.autofmt_xdate(rotation=30, ha='right') # Auto format date labels
                          plt.tight_layout() # Adjust layout
                          st.pyplot(fig_line, clear_figure=True) # Display plot

                     except AttributeError as attr_err:
                          st.warning(f"æœˆåº¦è¶‹åŠ¿å›¾çš„æ—¥æœŸæ ¼å¼æ— æ³•è¢«æ­£ç¡®å¤„ç†ä»¥è¿›è¡Œç»˜å›¾: {attr_err}")
                     except Exception as e:
                          st.error(f"ç»˜åˆ¶æœˆåº¦è¶‹åŠ¿å›¾æ—¶å‡ºé”™: {e}")
                          if fig_line is not None: plt.close(fig_line) # Close plot if error occurred during processing
                 else:
                     st.caption("æ— è¶³å¤Ÿæ•°æ®ç»˜åˆ¶æœˆåº¦é”€å”®è¶‹åŠ¿ã€‚")

             # --- Sales Share (Pie Chart) ---
             with chart_cols[1]:
                 st.markdown("###### æœŸé—´é”€é‡å æ¯” Top 10")
                 top_data = metrics.get('top_selling_period_chart_data')
                 # Check if data is valid Series and has a positive sum
                 if isinstance(top_data, pd.Series) and not top_data.empty and top_data.sum() > 0:
                     fig_pie = None # Initialize figure variable
                     try:
                          pie_labels = [str(label) for label in top_data.index] # Ensure labels are strings
                          pie_values = top_data.values
                          fig_pie, ax_pie = plt.subplots(figsize=(5, 3)) # Adjust figure size if needed

                          # Define text properties, considering font availability
                          text_props = {'fontproperties': chinese_font, 'size': 8} if FONT_AVAILABLE and chinese_font else {'size': 8}
                          legend_props = {'prop': chinese_font} if FONT_AVAILABLE and chinese_font else {}
                          title_fontprop = chinese_font if FONT_AVAILABLE and chinese_font else None

                          # Use a suitable colormap
                          colors = plt.get_cmap('Pastel1').colors

                          wedges, texts, autotexts = ax_pie.pie(
                              pie_values,
                              autopct='%1.1f%%', # Format percentage
                              startangle=90,
                              pctdistance=0.85, # Distance of percentage text from center
                              colors=colors,
                              textprops=text_props # Apply font props to percentage labels
                          )

                          # Set font for autotexts (percentages) explicitly if font is available
                          if FONT_AVAILABLE and chinese_font:
                             plt.setp(autotexts, fontproperties=chinese_font)

                          ax_pie.axis('equal') # Equal aspect ratio ensures pie is drawn as a circle.

                          # Add legend
                          legend_title = "äº§å“åç§°"
                          # Position legend outside the pie
                          ax_pie.legend(wedges, pie_labels,
                                        title=legend_title,
                                        loc="center left",
                                        bbox_to_anchor=(1.05, 0, 0.5, 1), # Adjust anchor to position legend
                                        fontsize=8,
                                        prop=legend_props.get('prop'), # Font for legend items
                                        title_fontproperties=title_fontprop) # Font for legend title

                          plt.subplots_adjust(left=0.1, right=0.65) # Adjust subplot to make space for legend
                          st.pyplot(fig_pie, clear_figure=True) # Display plot

                     except Exception as e:
                          st.error(f"ç»˜åˆ¶é”€é‡å æ¯”å›¾æ—¶å‡ºé”™: {e}")
                          if fig_pie is not None: plt.close(fig_pie) # Close plot on error
                 else:
                     st.caption("æ— è¶³å¤Ÿæ•°æ®ç»˜åˆ¶é”€é‡å æ¯”å›¾ã€‚")
        elif not main_analysis_ready:
            st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ æœ‰æ•ˆçš„ **ç™¾è´§åŸæ•°æ®** æ–‡ä»¶ä»¥æŸ¥çœ‹é”€å”®åˆ†æã€‚")
            if st.session_state.main_load_error:
                st.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {st.session_state.main_load_error}")


    # --- Inventory Analysis Tab ---
    with tab_inventory:
        if main_analysis_ready and isinstance(stock_analysis, pd.DataFrame) and not stock_analysis.empty:
             st.subheader("åº“å­˜å¥åº·åº¦ä¸è´¦é¾„åˆ†æ")
             
             # æ»é”€ç­›é€‰å¼€å…³
             col1, col2 = st.columns(2)
             with col1:
                 show_dead_stock = st.checkbox("ä»…æ˜¾ç¤ºæ»é”€äº§å“", value=False)
             with col2:
                 dead_stock_level = st.selectbox(
                     "æ»é”€ç­‰çº§ç­›é€‰",
                     ["å…¨éƒ¨", "é«˜", "ä¸­", "ä½"],
                     index=0
                 )
             
             # åº”ç”¨ç­›é€‰
             if show_dead_stock or dead_stock_level != "å…¨éƒ¨":
                 stock_analysis = stock_analysis.copy()  # Avoid modifying original
                 if show_dead_stock:
                     stock_analysis = stock_analysis[stock_analysis["æ»é”€ç­‰çº§"] != "ä½"]
                 if dead_stock_level != "å…¨éƒ¨":
                     stock_analysis = stock_analysis[stock_analysis["æ»é”€ç­‰çº§"] == dead_stock_level]
             # --- å¤šç»´åº¦ç­›é€‰å™¨ ---
             st.markdown("### ğŸ” å¤šç»´åº¦ç­›é€‰")
             filter_col1, filter_col2, filter_col3 = st.columns(3)
             
             with filter_col1:
                 # äº§å“åˆ†ç±»ç­›é€‰
                 if "äº§å“åˆ†ç±»" in stock_analysis.columns:
                     categories = ["å…¨éƒ¨"] + sorted(stock_analysis["äº§å“åˆ†ç±»"].dropna().unique().tolist())
                     selected_category = st.selectbox("äº§å“åˆ†ç±»", categories, index=0)
                     if selected_category != "å…¨éƒ¨":
                         stock_analysis = stock_analysis[stock_analysis["äº§å“åˆ†ç±»"] == selected_category]
             
             with filter_col2:
                 # å‹è´§æ—¶é—´æ®µç­›é€‰
                 age_ranges = ["å…¨éƒ¨", "0-30å¤©", "31-60å¤©", "61-90å¤©", "91-180å¤©", "181+å¤©"]
                 selected_age = st.selectbox("å‹è´§æ—¶é—´æ®µ", age_ranges, index=0)
                 if selected_age != "å…¨éƒ¨":
                     if "+" in selected_age:
                         min_days = int(selected_age.split("+")[0])
                         stock_analysis = stock_analysis[stock_analysis["å‹è´§æ—¶é—´_å¤©"] >= min_days]
                     else:
                         min_days, max_days = map(int, selected_age.split("å¤©")[0].split("-"))
                         stock_analysis = stock_analysis[
                             (stock_analysis["å‹è´§æ—¶é—´_å¤©"] >= min_days) &
                             (stock_analysis["å‹è´§æ—¶é—´_å¤©"] <= max_days)
                     ]
             
             with filter_col3:
                 # åº“å­˜å¤©æ•°åŒºé—´ç­›é€‰
                 stock_days_ranges = ["å…¨éƒ¨", "0-7å¤©", "7-30å¤©", "30+å¤©"]
                 selected_stock_days = st.selectbox("åº“å­˜å¤©æ•°åŒºé—´", stock_days_ranges, index=0)
                 if selected_stock_days != "å…¨éƒ¨":
                     if "+" in selected_stock_days:
                         min_days = int(selected_stock_days.split("+")[0])
                         stock_analysis = stock_analysis[stock_analysis["å½“å‰åº“å­˜"] >= min_days]
                     else:
                         min_days, max_days = map(int, selected_stock_days.split("-")[0].split("å¤©")[0].split("-"))
                         stock_analysis = stock_analysis[
                             (stock_analysis["å½“å‰åº“å­˜"] >= min_days) &
                             (stock_analysis["å½“å‰åº“å­˜"] <= max_days)
                         ]
             
             st.markdown("---")
             # --- Stock Aging Distribution (Bar Chart) ---
             st.markdown("###### åº“å­˜è´¦é¾„åˆ†å¸ƒ (æŒ‰ SKU æ•°)")
             # Helper function for bucketing (can be defined inside or outside)
             def get_age_bucket(days):
                 try:
                      if pd.isna(days): return "æœªçŸ¥"
                      days_int = int(float(days)) # Ensure it's treated as number
                 except (ValueError, TypeError, OverflowError):
                      return "æœªçŸ¥" # Handle non-numeric gracefully

                 if days_int == 9999: return "ä»æœªå”®å‡º" # Special code for never sold / no last sale date
                 elif days_int <= 30: return "0-30 å¤©"
                 elif days_int <= 60: return "31-60 å¤©"
                 elif days_int <= 90: return "61-90 å¤©"
                 elif days_int <= 180: return "91-180 å¤©"
                 else: return "181+ å¤©"

             try:
                 if 'å‹è´§æ—¶é—´_å¤©' in stock_analysis.columns:
                     # Apply the bucketing function
                     stock_analysis['åº“å­˜è´¦é¾„åˆ†ç»„'] = stock_analysis['å‹è´§æ—¶é—´_å¤©'].apply(get_age_bucket)
                     # Define the desired order for the categories (ä»å°åˆ°å¤§)
                     age_order = ["0-30 å¤©", "31-60 å¤©", "61-90 å¤©", "91-180 å¤©", "181+ å¤©", "ä»æœªå”®å‡º", "æœªçŸ¥"]
                     # Sort the data before plotting
                     aging_data_sku = stock_analysis.groupby('åº“å­˜è´¦é¾„åˆ†ç»„', observed=False).size()
                     aging_data_sku = aging_data_sku.reindex(age_order).fillna(0)
                     aging_data_sku = aging_data_sku[aging_data_sku > 0]
                     # Group by the new category, count SKUs, reindex to enforce order, fill missing groups with 0
                     aging_data_sku = stock_analysis.groupby('åº“å­˜è´¦é¾„åˆ†ç»„', observed=False).size().reindex(age_order).fillna(0)
                     # Filter out categories with zero count for cleaner chart
                     aging_data_sku = aging_data_sku[aging_data_sku > 0]

                     if not aging_data_sku.empty:
                          # Use Streamlit's built-in bar chart
                          st.bar_chart(aging_data_sku, use_container_width=True)
                          st.caption("åº“å­˜è´¦é¾„æ ¹æ®äº§å“æœ€åé”€å”®æ—¥æœŸè®¡ç®—ã€‚'ä»æœªå”®å‡º'è¡¨ç¤ºæ— é”€å”®è®°å½•æˆ–æ— æ³•ç¡®å®šæœ€åé”€å”®æ—¥æœŸã€‚")
                     else:
                          st.caption("æ— æœ‰æ•ˆçš„åº“å­˜è´¦é¾„æ•°æ®å¯ä¾›ç»˜åˆ¶ã€‚")
                 else:
                     st.warning("æ— æ³•è®¡ç®—åº“å­˜è´¦é¾„åˆ†å¸ƒï¼Œç¼ºå°‘ 'å‹è´§æ—¶é—´_å¤©' æ•°æ®ã€‚")
             except Exception as e:
                 st.error(f"ç”Ÿæˆåº“å­˜è´¦é¾„å›¾è¡¨æ—¶å‡ºé”™: {e}")
                 traceback.print_exc()

             # --- Inventory Details Table ---
             st.markdown("---")
             st.markdown("###### åº“å­˜æ˜ç»†ä¸çŠ¶æ€")
             # Sort the full dataframe (e.g., by stock age descending)
             stock_analysis_sorted = stock_analysis.sort_values("å‹è´§æ—¶é—´_å¤©", ascending=False) if 'å‹è´§æ—¶é—´_å¤©' in stock_analysis.columns else stock_analysis
             # Limit display rows
             stock_analysis_display_limited = stock_analysis_sorted.head(TOP_N_DISPLAY)
             st.info(f"ğŸ’¡ ä¸‹è¡¨æ˜¾ç¤ºåº“å­˜è¯¦ç»†ä¿¡æ¯ï¼ˆæœ€å¤šæ˜¾ç¤ºå‰ {TOP_N_DISPLAY} æ¡è®°å½•ï¼ŒæŒ‰å‹è´§å¤©æ•°é™åºæ’åˆ—ï¼‰ã€‚å®Œæ•´æ•°æ®å¯ä¸‹è½½ã€‚")

             # Configure columns for st.data_editor
             dynamic_stock_config = {}
             base_stock_configs = {
                 "äº§å“åç§°": st.column_config.TextColumn("äº§å“åç§°", width="medium", help="äº§å“åç§°"),
                 "äº§å“åˆ†ç±»": st.column_config.TextColumn("åˆ†ç±»", width="small", help="äº§å“æ‰€å±åˆ†ç±»"),
                 "å½“å‰åº“å­˜": st.column_config.NumberColumn("å½“å‰åº“å­˜", format="%d ä¸ª", help="å½“å‰å®é™…åº“å­˜æ•°é‡"),
                 "æœŸé—´é”€å”®é‡": st.column_config.NumberColumn("æœŸé—´é”€å”®é‡", format="%d ä¸ª", help="æ‰€é€‰åˆ†æå‘¨æœŸå†…çš„æ€»é”€å”®æ•°é‡"),
                 "æœŸé—´é”€å”®é¢": st.column_config.NumberColumn("æœŸé—´é”€å”®é¢ (â‚¬)", format="%.2f", help="æ‰€é€‰åˆ†æå‘¨æœŸå†…çš„æ€»é”€å”®é¢"), # æ–°å¢
                 "æœŸé—´é”€å”®æˆæœ¬": st.column_config.NumberColumn("æœŸé—´é”€å”®æˆæœ¬ (â‚¬)", format="%.2f", help="æœŸé—´é”€å”®é‡ * é‡‡è´­ä»·"), # æ–°å¢
                 "æœŸé—´æ¯›åˆ©": st.column_config.NumberColumn("æœŸé—´æ¯›åˆ© (â‚¬)", format="%.2f", help="æœŸé—´é”€å”®é¢ - æœŸé—´é”€å”®æˆæœ¬"), # æ–°å¢
                 "æ¯›åˆ©ç‡": st.column_config.NumberColumn("æ¯›åˆ©ç‡", format="%.1f%%", help="(æœŸé—´æ¯›åˆ© / æœŸé—´é”€å”®é¢) * 100"), # æ–°å¢
                 "æœŸé—´æ—¥å‡é”€é‡": st.column_config.NumberColumn("æœŸé—´æ—¥å‡é”€å”®", format="%.2f ä¸ª/å¤©", help="æ‰€é€‰åˆ†æå‘¨æœŸå†…çš„å¹³å‡æ¯æ—¥é”€å”®æ•°é‡"),
                 "é¢„è®¡å¯ç”¨å¤©æ•°": st.column_config.NumberColumn("é¢„è®¡å¯ç”¨å¤©æ•°", help="å½“å‰åº“å­˜é¢„è®¡å¯ç»´æŒå¤©æ•° (9999ä»£è¡¨>9999å¤©æˆ–æ— è¿‘æœŸé”€é‡)", format="%d å¤©"),
                 "å‹è´§æ—¶é—´_å¤©": st.column_config.NumberColumn("å‹è´§å¤©æ•°", help="è‡ªä¸Šæ¬¡å”®å‡ºè‡³ä»Šçš„å¤©æ•° (9999ä»£è¡¨ä»æœªå”®å‡ºæˆ–æ— è®°å½•)", format="%d å¤©"),
                 "æœ€åé”€å”®æ—¥æœŸ": st.column_config.DateColumn("æœ€åé”€å”®æ—¥æœŸ", format="YYYY-MM-DD", help="è¯¥äº§å“æœ€åä¸€æ¬¡æœ‰é”€å”®è®°å½•çš„æ—¥æœŸ"),
                 "å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­": st.column_config.NumberColumn("è·ä¸Šæ¬¡é‡‡è´­", help="è‡ªä¸Šæ¬¡é‡‡è´­è‡³ä»Šçš„å¤©æ•° (9999ä»£è¡¨æ— é‡‡è´­è®°å½•)", format="%d å¤©"),
                 "æœ€åé‡‡è´­æ—¥æœŸ": st.column_config.DateColumn("æœ€åé‡‡è´­æ—¥æœŸ", format="YYYY-MM-DD", help="è¯¥äº§å“æœ€åä¸€æ¬¡æœ‰é‡‡è´­è®°å½•çš„æ—¥æœŸ"),
                 "æœ€åé‡‡è´­æ•°é‡": st.column_config.NumberColumn("æœ€åé‡‡è´­æ•°é‡", format="%d ä¸ª", help="æœ€åä¸€æ¬¡é‡‡è´­çš„æ•°é‡"),
                 "é‡‡è´­ä»·": st.column_config.NumberColumn("é‡‡è´­ä»· (â‚¬)", format="%.2f", help="åº“å­˜æ•°æ®ä¸­è®°å½•çš„é‡‡è´­å•ä»·"),
             }

             # Define which columns to show and in what order
             stock_cols_to_show_final = ["äº§å“åç§°"]
             if has_category_data: stock_cols_to_show_final.append("äº§å“åˆ†ç±»")
             # æ·»åŠ æ¯›åˆ©ç›¸å…³åˆ—åˆ°æ˜¾ç¤ºåˆ—è¡¨
             stock_cols_to_show_final.extend([
                 "å½“å‰åº“å­˜", "æœŸé—´é”€å”®é‡", "æœŸé—´é”€å”®é¢", "æœŸé—´é”€å”®æˆæœ¬", "æœŸé—´æ¯›åˆ©", "æ¯›åˆ©ç‡",
                 "æœŸé—´æ—¥å‡é”€é‡", "é¢„è®¡å¯ç”¨å¤©æ•°", "å‹è´§æ—¶é—´_å¤©",
                 "æœ€åé”€å”®æ—¥æœŸ", "å¤©æ•°è‡ªä¸Šæ¬¡é‡‡è´­", "æœ€åé‡‡è´­æ—¥æœŸ", "æœ€åé‡‡è´­æ•°é‡", "é‡‡è´­ä»·"
             ])

             # Filter columns to only those present in the dataframe and configure them
             final_stock_cols = []
             for col_name in stock_cols_to_show_final:
                  if col_name in stock_analysis_display_limited.columns:
                       final_stock_cols.append(col_name)
                       if col_name in base_stock_configs:
                           dynamic_stock_config[col_name] = base_stock_configs[col_name]
                       #else: # Add default config for columns not explicitly defined? Optional.
                           #dynamic_stock_config[col_name] = st.column_config.TextColumn(col_name)


             # Display the data editor table
             st.data_editor(
                 stock_analysis_display_limited[final_stock_cols], # Use filtered columns
                 use_container_width=True,
                 num_rows="fixed", # Fixed height based on TOP_N_DISPLAY
                 disabled=True, # Make table read-only
                 key="stock_data_editor",
                 # æ ¼å¼åŒ–æ¯›åˆ©ç‡ä¸ºç™¾åˆ†æ¯”
                 column_config={**dynamic_stock_config, "æ¯›åˆ©ç‡": st.column_config.NumberColumn("æ¯›åˆ©ç‡", format="%.1f%%")},
                 hide_index=True
             )
             st.caption(f"æ³¨ï¼šè¡¨æ ¼ä»…æ˜¾ç¤ºæ’åºåçš„å‰ {len(stock_analysis_display_limited)} æ¡è®°å½• (å…± {len(stock_analysis_sorted)} æ¡)ã€‚")

             # --- Download Button for Full Inventory Analysis ---
             try:
                 # Select columns for download (exclude temporary/grouping cols)
                 stock_cols_download = [col for col in stock_analysis_sorted.columns if col not in ['åº“å­˜è´¦é¾„åˆ†ç»„']]
                 # Ensure columns exist
                 stock_cols_download = [col for col in stock_cols_download if col in stock_analysis_sorted.columns]

                 df_to_download_stock = stock_analysis_sorted[stock_cols_download]
                 # Prepare Excel file in memory
                 excel_buffer_stock = io.BytesIO()
                 with pd.ExcelWriter(excel_buffer_stock, engine='openpyxl') as writer:
                     df_to_download_stock.to_excel(writer, index=False, sheet_name='åº“å­˜åˆ†æè¯¦ç»†æ•°æ®')
                 excel_buffer_stock.seek(0) # Rewind buffer

                 # Generate filename
                 download_filename_stock = f"åº“å­˜åˆ†æ_{selected_category}_{start_date_dt.strftime('%Y%m%d')}-{end_date_dt.strftime('%Y%m%d')}.xlsx"

                 # Create download button
                 st.download_button(
                     label=f"ğŸ“¥ ä¸‹è½½å®Œæ•´åº“å­˜åˆ†æè¡¨ ({len(stock_analysis_sorted)}æ¡)",
                     data=excel_buffer_stock,
                     file_name=download_filename_stock,
                     mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                     key="download_stock_analysis"
                 )
             except Exception as e:
                 st.error(f"ç”Ÿæˆåº“å­˜åˆ†æ Excel ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                 traceback.print_exc()

        elif not main_analysis_ready:
             st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼ æœ‰æ•ˆçš„ **ç™¾è´§åŸæ•°æ®** æ–‡ä»¶ä»¥æŸ¥çœ‹åº“å­˜åˆ†æã€‚")
             if st.session_state.main_load_error:
                  st.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {st.session_state.main_load_error}")
        else: # main_analysis_ready is True, but stock_analysis is empty or invalid
             st.info("æ— è¯¦ç»†åº“å­˜æ•°æ®å¯ä¾›åˆ†æã€‚è¯·æ£€æŸ¥ä¸Šä¼ çš„â€œç™¾è´§åŸæ•°æ®â€æ–‡ä»¶ä¸­çš„ 'åº“å­˜æ•°æ®' æˆ–ç­›é€‰æ¡ä»¶ã€‚")


    # --- Purchase Suggestions Tab ---
    with tab_purchase:
         st.subheader("æ™ºèƒ½é‡‡è´­å»ºè®®")
         if main_analysis_ready and isinstance(purchase_suggestions, pd.DataFrame) and not purchase_suggestions.empty:
             # Get selected prediction method from session state
             predict_method = st.session_state.get("predict_method_key", "exponential_smoothing")
             # Data is ready and suggestions exist
             purchase_suggestions_full = purchase_suggestions # Keep the full dataframe
             purchase_suggestions_display_limited = purchase_suggestions_full.head(TOP_N_DISPLAY) # Limit for display

             st.info(f"ğŸ’¡ ä¸‹è¡¨æ˜¾ç¤ºå»ºè®®é‡‡è´­çš„äº§å“ï¼ˆæœ€å¤šæ˜¾ç¤ºå‰ {TOP_N_DISPLAY} æ¡ï¼ŒæŒ‰å»ºè®®é‡‡è´­é‡é™åºæ’åˆ—ï¼‰ã€‚å®Œæ•´å»ºè®®å¯ä¸‹è½½ã€‚")

             # Configure columns for display
             dynamic_purchase_config = {}
             base_purchase_configs = {
                  "äº§å“åç§°": st.column_config.TextColumn("äº§å“åç§°", width="medium"),
                  "äº§å“åˆ†ç±»": st.column_config.TextColumn("åˆ†ç±»", width="small"),
                  "å½“å‰åº“å­˜": st.column_config.NumberColumn("å½“å‰åº“å­˜", format="%d ä¸ª"),
                  "æœŸé—´æ—¥å‡é”€é‡": st.column_config.NumberColumn("æœŸé—´æ—¥å‡é”€å”®", format="%.2f ä¸ª/å¤©"),
                  "é¢„è®¡å¯ç”¨å¤©æ•°": st.column_config.NumberColumn("å½“å‰å¯ç”¨å¤©æ•°", format="%d å¤©", help="åŸºäºå½“å‰åº“å­˜å’ŒæœŸé—´æ—¥å‡é”€é‡çš„ä¼°ç®—"),
                  "ç›®æ ‡åº“å­˜æ°´å¹³": st.column_config.NumberColumn("ç›®æ ‡åº“å­˜", help=f"ç›®æ ‡({target_days_input}å¤©)+å®‰å…¨({safety_days_input}å¤©)æ‰€éœ€åº“å­˜é‡", format="%.0f ä¸ª"),
                  "å»ºè®®é‡‡è´­é‡": st.column_config.NumberColumn("å»ºè®®é‡‡è´­é‡", format="%d ä¸ª", width="large", help="å»ºè®®è¡¥å……çš„æ•°é‡ (å·²å‘ä¸Šå–æ•´)")
             }

             # Determine columns to show based on what's available in purchase_suggestions_display_limited
             purchase_cols_to_show = []
             for col_name in purchase_suggestions_display_limited.columns:
                  if col_name in base_purchase_configs:
                       purchase_cols_to_show.append(col_name)
                       dynamic_purchase_config[col_name] = base_purchase_configs[col_name]
                  # Add handling for unexpected columns if necessary

             # Display the data editor table
             st.data_editor(
                 purchase_suggestions_display_limited[purchase_cols_to_show],
                 use_container_width=True,
                 num_rows="fixed",
                 disabled=True, # Read-only
                 key="purchase_data_editor",
                 column_config=dynamic_purchase_config,
                 hide_index=True
             )
             st.caption(f"æ³¨ï¼šè¡¨æ ¼ä»…æ˜¾ç¤ºå»ºè®®é‡‡è´­é‡æœ€å¤šçš„å‰ {len(purchase_suggestions_display_limited)} æ¡å»ºè®® (å…± {len(purchase_suggestions_full)} æ¡)ã€‚")
             
             # --- Download Button for Full Purchase Suggestions ---
             try:
                 df_to_download_purchase = purchase_suggestions_full # Use the full dataframe
                 excel_buffer_purchase = io.BytesIO()
                 with pd.ExcelWriter(excel_buffer_purchase, engine='openpyxl') as writer:
                     df_to_download_purchase.to_excel(writer, index=False, sheet_name='é‡‡è´­å»ºè®®æ¸…å•')
                 excel_buffer_purchase.seek(0)

                 # Generate filename including parameters
                 download_filename_purchase = f"é‡‡è´­å»ºè®®_{selected_category}_{start_date_dt.strftime('%Y%m%d')}-{end_date_dt.strftime('%Y%m%d')}_T{target_days_input}S{safety_days_input}.xlsx"

                 st.download_button(
                     label=f"ğŸ“¤ ä¸‹è½½å®Œæ•´é‡‡è´­å»ºè®® ({len(purchase_suggestions_full)}æ¡)",
                     data=excel_buffer_purchase,
                     file_name=download_filename_purchase,
                     mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                     key="download_purchase_suggestions"
                 )
             except Exception as e:
                 st.error(f"ç”Ÿæˆé‡‡è´­å»ºè®® Excel ä¸‹è½½æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                 traceback.print_exc()

         elif main_analysis_ready and isinstance(purchase_suggestions, pd.DataFrame) and purchase_suggestions.empty:
             # Analysis ran, but no suggestions were generated
             st.success("âœ… æ ¹æ®å½“å‰åº“å­˜å’Œæ‰€é€‰å‚æ•°ï¼Œæš‚æ— äº§å“éœ€è¦ç«‹å³é‡‡è´­ã€‚")
         elif not main_analysis_ready:
             # Main data wasn't loaded successfully
             pass

             # --- è´¢åŠ¡å¯¹è´¦ Tab ---
             if 'tab_reconciliation' in locals():
                 with tab_reconciliation:
                     st.subheader("è´¢åŠ¡å¯¹è´¦")
                
                     if 'order_df' in locals() and not order_df.empty:
                         # æ£€æŸ¥è´¢åŠ¡æ•°æ®æ˜¯å¦ä¸Šä¼ 
                         if 'finance_df' not in locals() or finance_df.empty:
                             st.warning("è¯·å…ˆä¸Šä¼ è´¢åŠ¡æ•°æ®æ–‡ä»¶")
                         else:
                             # æ ¸å¿ƒå¯¹è´¦é€»è¾‘
                             try:
                                 # 1. æŒ‰è®¢å•ç¼–å·ç²¾ç¡®åŒ¹é…
                                 reconciled = order_df.merge(
                                     finance_df,
                                     on='è®¢å•ç¼–å·',
                                     how='left',
                                     suffixes=('_é”€å”®', '_è´¢åŠ¡')
                                 )
                                 
                                 # 2. å¯¹æœªåŒ¹é…çš„è®°å½•å°è¯•å®¢æˆ·+é‡‘é¢è¿‘ä¼¼åŒ¹é…
                                 unmatched = reconciled[reconciled['å®æ”¶é‡‘é¢'].isna()]
                                 if not unmatched.empty:
                                     for idx, row in unmatched.iterrows():
                                         match = finance_df[
                                             (finance_df['å®¢æˆ·åç§°'] == row['å®¢æˆ·']) &
                                             (abs(finance_df['é‡‘é¢'] - row['é”€å”®é¢']) < 0.01)
                                         ].head(1)
                                         if not match.empty:
                                             reconciled.loc[idx, 'å®æ”¶é‡‘é¢'] = match.iloc[0]['é‡‘é¢']
                                             
                                 # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                                 total_sales = reconciled['é”€å”®é¢'].sum()
                                 total_received = reconciled['å®æ”¶é‡‘é¢'].sum()
                                 payment_rate = (total_received / total_sales) * 100 if total_sales > 0 else 0
                                 
                                 # æ˜¾ç¤ºç»“æœ
                                 col1, col2, col3 = st.columns(3)
                                 col1.metric("æ€»é”€å”®é¢", f"{total_sales:,.2f}å…ƒ")
                                 col2.metric("å·²å›æ¬¾", f"{total_received:,.2f}å…ƒ")
                                 col3.metric("å›æ¬¾ç‡", f"{payment_rate:.1f}%")
                                 
                                 # åº”æ”¶è´¦æ¬¾æ˜ç»†
                                 with st.expander("åº”æ”¶è´¦æ¬¾æ˜ç»†"):
                                     overdue = reconciled[reconciled['å®æ”¶é‡‘é¢'].isna()]
                                     st.dataframe(overdue)
                                     
                             except Exception as e:
                                 st.error(f"å¯¹è´¦è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
             st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æœ‰æ•ˆçš„ **ç™¾è´§åŸæ•°æ®** æ–‡ä»¶ä»¥ç”Ÿæˆé‡‡è´­å»ºè®®ã€‚")
             if st.session_state.main_load_error:
                  st.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {st.session_state.main_load_error}")
         else:
             # Catchall for other states (e.g., purchase suggestion calculation failed)
             st.warning("æ— æ³•ç”Ÿæˆé‡‡è´­å»ºè®®ã€‚è¯·æ£€æŸ¥åº“å­˜åˆ†ææ•°æ®å’Œå‚æ•°ã€‚")


    # --- NEW: Financial Metrics Tab ---
    with tab_financial:
        st.subheader("ğŸ’° è´¢åŠ¡æŒ‡æ ‡æ¦‚è§ˆ")
        if financial_data_ready and isinstance(financial_data_loaded, pd.DataFrame):
            st.success("è´¢åŠ¡æ•°æ®å·²åŠ è½½ã€‚")
            st.markdown("_(æ­¤å¤„å°†æ˜¾ç¤ºå…³é”®è´¢åŠ¡æŒ‡æ ‡ï¼Œä¾‹å¦‚æ€»æ”¶å…¥ã€æ€»æ”¯å‡ºã€å‡€åˆ©æ¶¦ã€åº”æ”¶/åº”ä»˜è´¦æ¬¾ç­‰)_")
            st.dataframe(financial_data_loaded.head(), use_container_width=True) # Display sample data
            # TODO: Implement actual financial metric calculations and display
        elif uploaded_financial_file and not financial_data_ready:
            st.warning("è´¢åŠ¡æ•°æ®æ–‡ä»¶å·²ä¸Šä¼ ï¼Œä½†åŠ è½½æˆ–å¤„ç†å¤±è´¥ã€‚è¯·æ£€æŸ¥ä¾§è¾¹æ é”™è¯¯ä¿¡æ¯ã€‚")
        else:
            st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼  **è´¢åŠ¡æ•°æ®** æ–‡ä»¶ä»¥æŸ¥çœ‹æ­¤æ¨¡å—ã€‚")

    # --- NEW: CRM Summary Tab ---
    with tab_crm:
        st.subheader("ğŸ‘¥ CRM æ‘˜è¦")
        if crm_data_ready and isinstance(crm_data_loaded, pd.DataFrame):
            st.success("CRM æ•°æ®å·²åŠ è½½ã€‚")
            st.markdown("_(æ­¤å¤„å°†æ˜¾ç¤º CRM ç›¸å…³æ‘˜è¦ï¼Œä¾‹å¦‚æ–°å¢æ½œåœ¨å®¢æˆ·ã€å®¢æˆ·æ´»åŠ¨æ¦‚è§ˆç­‰)_")
            st.dataframe(crm_data_loaded.head(), use_container_width=True) # Display sample data
            # TODO: Implement actual CRM metric calculations and display
        elif uploaded_crm_file and not crm_data_ready:
            st.warning("CRM æ•°æ®æ–‡ä»¶å·²ä¸Šä¼ ï¼Œä½†åŠ è½½æˆ–å¤„ç†å¤±è´¥ã€‚è¯·æ£€æŸ¥ä¾§è¾¹æ é”™è¯¯ä¿¡æ¯ã€‚")
        else:
            st.info("è¯·åœ¨å·¦ä¾§ä¸Šä¼  **CRM æ•°æ®** æ–‡ä»¶ä»¥æŸ¥çœ‹æ­¤æ¨¡å—ã€‚")


    # --- NEW: Alerts & Tasks Tab ---
    with tab_alerts:
        st.subheader("ğŸ”” å¾…åŠäº‹é¡¹ä¸æé†’")
        st.markdown("_(æ­¤åŒºåŸŸå°†æ•´åˆå…³é”®æé†’ï¼Œä¾‹å¦‚ä½åº“å­˜é¢„è­¦ã€å»ºè®®é‡‡è´­é¡¹ç›®ç­‰)_")
        # Example: Display low stock items (requires modification in calculate_metrics or here)
        if main_analysis_ready and isinstance(stock_analysis, pd.DataFrame) and not stock_analysis.empty and 'é¢„è®¡å¯ç”¨å¤©æ•°' in stock_analysis.columns:
            low_stock_threshold_days = 7 # Example threshold
            low_stock_items = stock_analysis[stock_analysis['é¢„è®¡å¯ç”¨å¤©æ•°'] <= low_stock_threshold_days]
            if not low_stock_items.empty:
                st.warning(f"âš ï¸ **ä½åº“å­˜é¢„è­¦** (é¢„è®¡å¯ç”¨å¤©æ•° <= {low_stock_threshold_days} å¤©):")
                st.dataframe(low_stock_items[['äº§å“åç§°', 'å½“å‰åº“å­˜', 'é¢„è®¡å¯ç”¨å¤©æ•°']].head(10), use_container_width=True, hide_index=True)
            else:
                st.success("âœ… å½“å‰æ— æ˜æ˜¾ä½åº“å­˜é£é™©ã€‚")
        else:
            st.info("éœ€è¦åŠ è½½æœ‰æ•ˆçš„ **ç™¾è´§åŸæ•°æ®** ä»¥ç”Ÿæˆåº“å­˜é¢„è­¦ã€‚")

        # Example: Display purchase suggestions summary
        if main_analysis_ready and isinstance(purchase_suggestions, pd.DataFrame) and not purchase_suggestions.empty:
             st.info(f"ğŸ›’ **é‡‡è´­å»ºè®®æé†’**: {len(purchase_suggestions)} ä¸ªäº§å“å»ºè®®é‡‡è´­ã€‚è¯¦æƒ…è¯·è§ 'é‡‡è´­å»ºè®®' æ ‡ç­¾é¡µã€‚")
        # TODO: Add other potential alerts (e.g., overdue tasks if CRM data is available)

    # --- NEW: Custom Analysis Tab ---
    with tab_custom_analysis:
        st.subheader("ğŸ“ˆ è‡ªå®šä¹‰åˆ†æ (å ä½ç¬¦)")
        st.info("æ­¤åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ã€‚æœªæ¥å°†å…è®¸æ‚¨åŸºäºå·²åŠ è½½çš„æ•°æ®è¿›è¡Œæ›´çµæ´»çš„æ¢ç´¢å’ŒæŠ¥è¡¨ç”Ÿæˆã€‚")
        # Placeholder for future features like:
        # - Selecting data source (Sales, Stock, Finance, etc.)
        # - Choosing columns for grouping/aggregation
        # - Selecting chart types
        # - Saving custom views


# Fallback message if file upload was attempted but processing failed for *both* types (or only one was attempted and failed)
# This might be redundant now with errors shown in sidebar/tabs, but can be a final catch-all. Check all potential uploads.
elif any([(uploaded_main_file and not main_analysis_ready),
          (uploaded_financial_file and not financial_data_ready), # Check new states
          (uploaded_crm_file and not crm_data_ready)]):             # Check new states
    st.error("âŒ éƒ¨åˆ†ä¸Šä¼ çš„æ–‡ä»¶å¤„ç†å¤±è´¥æˆ–åŒ…å«æ— æ•ˆæ•°æ®ã€‚è¯·æ£€æŸ¥ä¾§è¾¹æ çš„é”™è¯¯ä¿¡æ¯ï¼Œå¹¶æ ¹æ®æç¤ºæ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå†…å®¹ã€‚")


# Footer (appears only if any file was uploaded or attempted)
if any([uploaded_main_file, uploaded_financial_file, uploaded_crm_file]): # Ensure footer shows if any file is uploaded
    st.markdown("---")
    try:
        current_year = datetime.now(APP_TIMEZONE).year
    except NameError: # Fallback if APP_TIMEZONE wasn't defined early
        current_year = datetime.utcnow().year
    st.markdown( f"""<div style='text-align: center; font-size: 14px; color: gray;'> TP.STER æ™ºèƒ½æ•°æ®å¹³å° {APP_VERSION} @ {current_year}</div>""", unsafe_allow_html=True)


# --- é”€é‡é¢„æµ‹åŠŸèƒ½ ---
def predict_sales(sales_df, forecast_days=30, method="exponential_smoothing"):
    """é¢„æµ‹æœªæ¥é”€é‡
    
    å‚æ•°:
        sales_df (DataFrame): å†å²é”€å”®æ•°æ®
        forecast_days (int): é¢„æµ‹å¤©æ•°
        method (str): é¢„æµ‹æ–¹æ³• ('moving_average' æˆ– 'exponential_smoothing')
    
    è¿”å›:
        DataFrame: åŒ…å«é¢„æµ‹ç»“æœçš„DataFrame
    """
    try:
        # æ ¹æ®é€‰æ‹©çš„æ–¹æ³•è°ƒç”¨ä¸åŒçš„é¢„æµ‹é€»è¾‘
        if method == "moving_average":
            # ç®€å•ç§»åŠ¨å¹³å‡æ³•
            window_size = 7  # 7å¤©ç§»åŠ¨å¹³å‡
            sales_df['é¢„æµ‹é”€é‡'] = sales_df['é”€é‡'].rolling(window=window_size).mean()
            sales_df['30å¤©é¢„æµ‹æ€»é”€é‡'] = sales_df['é¢„æµ‹é”€é‡'] * forecast_days
        else:
            # é»˜è®¤ä½¿ç”¨æŒ‡æ•°å¹³æ»‘æ³•
            from statsmodels.tsa.holtwinters import ExponentialSmoothing
        from statsmodels.tsa.holtwinters import ExponentialSmoothing
        
        # æŒ‰äº§å“å’Œæ—¥æœŸèšåˆé”€é‡
        daily_sales = sales_df.groupby(['äº§å“ID', 'è®¢å•æ—¥æœŸ'])['é”€å”®æ•°é‡'].sum().reset_index()
        
        predictions = []
        for product_id, group in daily_sales.groupby('äº§å“ID'):
            # å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®
            ts_data = group.set_index('è®¢å•æ—¥æœŸ')['é”€å”®æ•°é‡'].sort_index()
            
            # å¡«å……ç¼ºå¤±æ—¥æœŸä¸º0
            idx = pd.date_range(ts_data.index.min(), ts_data.index.max())
            ts_data = ts_data.reindex(idx, fill_value=0)
            
            # è®­ç»ƒæ¨¡å‹
            model = ExponentialSmoothing(ts_data, trend='add', seasonal='add', seasonal_periods=7).fit()
            
            # é¢„æµ‹æœªæ¥é”€é‡
            forecast = model.forecast(forecast_days)
            
            predictions.append({
                'äº§å“ID': product_id,
                'äº§å“åç§°': group['äº§å“åç§°'].iloc[0],
                'æ—¥å‡é¢„æµ‹é”€é‡': round(forecast.mean(), 2),
                '30å¤©é¢„æµ‹æ€»é”€é‡': round(forecast.sum(), 2)
            })
            
        return pd.DataFrame(predictions)
        
    except Exception as e:
        st.error(f"é”€é‡é¢„æµ‹å¤±è´¥: {str(e)}")
        return None

# --- END OF SCRIPT ---